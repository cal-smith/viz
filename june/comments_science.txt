The only industry where you don't know how much the service costs until after they bill you for it.

Can I at least ask for an estimate? Back in November I was nearly killed by a drunk driver while riding my motorcycle. I was in the hospital for a month and I had 3 surgeries to save my leg in that time, with one more so far sense I was discharged. I live in California and have fairly good insurance. Regardless , I get a letter after I was home from my insurer saying I had exceeded my limit by $200,000 and that they where entitled to any money I received from the responsible party. Plus there are several medicines and doctors that apparently were not in my "network" therefor are not covered. I'm just finding out about this now. My layers are cutting a deal with my insurer but they're still getting a 3rd. (The person who hit me was minimally insured and quite poor). Having to deal with this is totally overwhelming and it makes me so mad I don't like to think about it. The system is so broken and I really feel sorry for anyone who has to go through it.

Sorry for venting on your comment. This whole thread got me worked up &gt; Plus there are several medicines and doctors that apparently were not in my "network" therefor are not covered.

This is what pisses me off the most. I went to the ER after an accident. The hospital was in my network; they accepted my insurance. I had some x-rays done and was given 1 pill and spoke to a doctor for all of 2 minutes.

A few weeks later I get a bill and all of the x-ray stuff was out of network and not covered by insurance. The hospital claims they "rent" the equipment from another vender and the technicians aren't part of the network. It's infuriating that they can do this and get away with it. They also billed $60 for a single vicodin, at least that was covered.  Then don't pay it, keep sending back the bill and challenging it. My insurance has been giving me the run around about a post-natal check up for my daughter from 5 months ago which should have been covered and they say she wasn't covered, and each time I tell them to run it again. I'll get a bill a month later and repeat the process. I'm not paying $300 for their mistake, so we'll keep doing this until they get it right.  And just imagine how much money this bureaucracy and "existence friction" costs the country. Some peoples' jobs in insurance agencies are just to find loopholes in their own policies so they don't have to pay. This was a couple years ago. I ended up paying about half of the revised charges. The lower ones that the insurance usually negotiates vs the ones the uninsured pay. I didn't pay at first trying to get a clear answer from the hospital and my insurance and during that time the bill got sent to collections.  &gt; The hospital claims they "rent" the equipment from another vender and the technicians aren't part of the network.

Perhaps it's time to coin a new term: "financial malpractice." I think that's called fraud... We need single payer. Expand medicare.  Do you mind briefly explaining how single payer works, how it is beneficial?  Well currently we have a multiple payer system. So like you have insurance through your workplace (one payer) and you pay the rest (2 payer). Which is silly. The single payer should be the government and we should get money taken out in our taxes to pay for it. So you never actually cut a check to pay a hospital bill. 

Also if the feds are footing the bill I'd imagine they would constantly be only paying for the cheapest supplies. So if a hospital buys saline for $5 they can't charge $500 for it. The feds wouldn't pay it. They would mandate all saline to be sold to patients for $10... Yes it's a little socialist, but better a little socialist then ALOT Capitalist.

I'm no expert but that is sort of how it works. People forget that insurance at its core is a socialist concept to begin with. It's literally a group of people pooling their resources together to help each other, or at least it's supposed to be.

In my opinion, every insurance company should be operated to break even. If an insurer is making a net profit, it means that either people are overpaying for their services or they aren't fulfilling enough claims. The idea of insurance as a moneymaking endeavor goes directly against what insurance is supposed to do. Germany does that.   A guy over in /r/Economics described there system as basically the best parts of a market based and a government run system. [Here is the post](http://www.reddit.com/r/Economics/comments/3054ex/hospitals_are_robbing_us_blind/cppqe2b)  Nice. Seems like the Germans have it down. I especially like the part about insurance not being considered an employment benefit, as I never really got how those two were related (other than insurance companies giving themselves guaranteed easy money). There was a fed mandated wage freeze at some point in the 20th century. Benefits via insurance were devised as a way around this. Also it's a good way to keep employees terrified to quit or strike or otherwise cause trouble, especially if they have sick family members. I don't understand why people in the US (I am Canadian) are so vehemently against universal healthcare. It's the same principle as private insurance, except that the government doesn't make a profit, and you can't opt out. But who voluntarily doesn't want health insurance in the US?

  Here in Canada, it costs less in taxes than what we would pay in insurance in the US, it's a lot less stressful when you need healthcare, and if you're poor or making a low income, you pay very little tax and don't get financially ruined by going to the hospital. So yes the rich are paying for the poor, but they're still paying as much or less than they would in a private insurer system. Isn't it what matters?

  In the end, the mere fact of not being stressed by financial worries when going to the hospital and already being stressed about being sick or injured is worth having universal healthcare. I'd push things further to have universal federal drug insurance (currently, it's a mix of insurance with your employer if you're eligible else you can get on a provincial drug insurance plan). Because a bunch of politicians and private interests who benefit from the system control large portions of the media, and have them convince everyone they can that single payer healthcare is somehow really bad because socialism and reasons. Essentially, they're so successful at conning voters into voting against their own interests that they can pretty much keep it up indefinitely. It's so obscenely corrupt that it's almost comical. *almost*
 The main argument they'll give is that you pay higher taxes and you have long waiting lists.  From what I observed, I believe it is because a lot of Americans don't know any better. I lived in a bunch of countries before and currently in US, There are broken things in this country that work wonderful in others, yet a lot of Americans I've talked to think their system is the best and there is no way it can be better. When I tell them how it works in other countries they are surprised, all the propaganda made them believe otherwise.  I worked for a very large insurance company HQ in a large city.  With the profits they made, they built a huge state of the art building on one of the most expensive lake front properties.  They could of returned overpaid premiums or reinvested to keep future expenses down.  Nope, we want our big new building overlooking the water and parks. [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] &gt; Can I at least ask for an estimate?

Actually, in the ED you probably can't.

1) Very few if anyone, actually knows what it will cost.

2) Under EMTALA, the issue of cost cannot be discussed until after appropriate medical screening and stabilization, as it could be used as tool to discourage people from seeking care. There is at least case where it was declared an EMTALA violation to give the patient a ball park of cost, when the patient asked, because it caused him to refuse care and leave the ED. Yes, that is bass-akwards.  Instead, it's better for people to refuse to go to the hospital for what should be emergency situations, purely because they don't know what it will cost and with charges like they are, they can assume that it will be far outside their price range.

Too bad there's not a good way to get a statistic of people who have died because they were afraid to seek medical help due to costs. My mother was having severe chest pain about 6 years ago. I wanted to call an ambulance but she knew it would be $500+ and our power was already shut off so she knew heat and food were the next to go. She refused an ambulance and downplayed her pain to protect me and my little brother. I got her into the car and drove as fast as I could to the hospital. It turns out she had multiple blood clots in the lungs and she to lucky to survive. No one should have to make a decision like that. To feed your children or get appropriate medical care. Now we're 100s of thousands of dollars in debt because my brother has seizures and I have an immune system problem. My brother is 22 and considering declaring bankruptcy to save his future. Unfortunately even that won't help his $40,000 student loan debt...and he's still years from graduating because of his seizures. My family is seriously considering leaving the US permanently at this point.
        
Edit: lower=&gt;power What is ED? Emergency Department [deleted] [deleted] [deleted] Price transparency could actually happen soon.  While it would be very hard for a complex procedure, this is a huge talking point in the industry for more routine screenings and procedures.  Baby steps. Some people are doing it right.  Hopefully it takes off.

http://www.surgerycenterok.com/pricing/ You can, so long as you go to the billing department, or if the doctor communicates with billing. They can give you an estimate for a procedure. They probably can't account for "extra" fees, but at least you'll be able to get a baseline.

Of course, this is hard to do if you're in an emergency, but for scheduling a procedure, it's pretty cut and dry. My parents did that once and did manage to get a price. The price quoted was $2000. The bill was $4000. This was for a surgery for my father. The surgeon said everything went perfectly with no surprises. No one could explain the difference between what they were quoted and what they were charged. [deleted] And somehow it always seems to be a higher figure...  The dartboard acts as a multiplier. His father obviously got the 2x multiplier. This is my favorite explanation.  I had a sebaceous cyst removed from my face.

"What will the approximate cost for this procedure be?"

"I can't answer that. It depends on your insurance."

"Ok, assume I have no insurance, how much then?"

"Roughly $500-600, maybe a little bit more. You have insurance right?"

"Yes I do, thanks."

The bill was $2400, $1500 after in-network discounts, but closer to $2000 after they charged me for "surgery fees" when I came in to get my stitches removed and she asked me if I wanted an injection to help with scarring.

I had asked repeatedly to make sure the follow up visit was included in the original price (it was), but when I accepted the injection, suddenly it was a separate office visit charge, surgery charge, medication charge, etc etc.

They are vultures. [deleted] The timeliness of this in my personal life is insane. What I am dealing with right now is hospitals that use contracted doctors. The hospital is in network, but the doctor is not and you are captive so you have no choice. 

There is a huge problem where doctors are tired of getting the short end of the insurance stick so they are in no insurance company networks. That way the insurance company pays the agreed out of network amount, 80% of the "expected cost" in my case, and the patient is on the hook for the difference...the entire difference.

In my example, a family member went into the ER at a local hospital and was admitted and released a day later. A month after that the bills started coming in. The hospital was in network so most of that was covered and my portion was quite small. Then I get a bill for the ER doctor who was a contractor, but in network my portion was $100 against an $800 bill, Lastly, I get a bill from the medical group which were the doctors on contract to work in the hospital and my portion was $850 against a $1000 bill because that medical group was out of network.

When I contacted the hospital they said I was notified and signed a paper agreeing to using contracted doctors, signed at admission during a very intense ER visit. When I contacted the medical group they said they were out of network and the insurance company should be paying more and I should appeal the claim. When I contacted the insurance company they said because the medical group was out of network they paid what was customary. When I asked about an appeal they said all appeals have to be submitted in writing with all supporting documentation.

It is insane that in this enlightened age we do not have single payer healthcare. Profit is the only motivator. I can't believe I have to consider if I should get treatment for myself or my family based on financial criteria. It sickens me, but I can't go to the doctor because it will cost too much. Hospital billing is out of control, I've had "issues" on several occasions with a particular hospital's billing being outright fraudulent.  

On the first occasion was my dying grandmother in law, who was unable to eat for days preceding her death.    The hospital tried to charge something like 20 or 30  per meal for "ensure type" drinks.  Now, her room had family in it constantly, saying goodbye, and not a one of them recalls a single drink being delivered to her room.

On the second occasion I had gone to the emergency room for stiches, and months later received a bill for some thousand dollars.  The odd thing that made me notice, and not simply right a check, is that under my insurance plan (which is an incredibly generous one) covers 100% of ER visits.   When I complained, I had to speak to three separate people and the last one simply told me that "my insurance didn't pay ENOUGH". The tone was aggressive from them, and at not point reflected that the hospital has an agreed upon amount that they will charge for a service, and that from that agreed total I cover a portion (0%) and my insurance company covers a portion (100%).   I had to actually get my insurance company to send over a copy of their negotiated contracts (not my coverage) with the hospital, or at least that's the story the billing office pushed on me.  

TLDR:  Billing office charged ME more than my insurance had negotiated them to charge, and then tried to bully me into paying more. I had a similar experience a couple years ago. I went in to the ER, when I left I paid the copay and applied my work related insurance. I specifically asked if there were any other fees and was assured everything was covered. I had a print out showing I was good to go.

A few months later I had a collector hounding me for something ridiculously small like $140. I was confused, but it turned out the hospital had charged me a 'physician fee' or some such. Since it was so small and already in collections I just paid it off, but learned to not ignore follow up mail from the hospital. Had I been paying attention I might have worked it out early in the process. Fortunately it was not in the thousands of dollars or I would have been in big trouble. If it was only 140, I WOULD HAVE paid that bill without a thought, and they would have gotten away with it.

On the other end of the spectrum, they made a mistake with my wife, nearly killed her... and then never sent us a bill. Guess they wrote it off so we would forget and not sue.   Turns out they were right, but all of this made me think very very deeply on my opinions of healthcare, and the need for more regulation.

Glad that you at least only lost out on 140, and didn't take a credit hit or anything nasty like that. [deleted] My dads friend had a heart attack and died on a golf course in Florida.  The ambulance ride and being pronounced dead ended up with a bill over $100,000.  His wife brought his body home (Canada) and just never went back into the US, refuses to pay the bill (as she should IMO). [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] Even under US Law she doesn't have to but people will often try to convince people they will. At best it will be taken from any estate that is left but those were his bills and debt is not inherited  She was his wife - doesn't that make it a joint estate unless there was some kind of a prenup? So having worked at a hospital business office and dealing with suing people, we learned that if the woman dies you can go after the man in all the states we had hospitals in, but if the man died, you couldn't go after the woman in west virginia  Good ole equality.  equality^(Some restrictions may apply.) Yea it does. If he doesn't have a will then the money goes to her. So she is paying it out of the estate which is hers anyways I don't understand how ambulance rides cost money. Don't we pay taxes for emergency services? Ambulances are private enterprises. It's one of the things that makes me question the economic points of libertarianism. In DC, they say to take an Uber. It costs between $5-20 in most parts of the city and and response time is usually a few minutes compared to a half hour for an ambulance.

EDIT: Yes. We get it. Don't call an Uber if you need medical attention DURING the trip. That really depends on how much of an emergency it is.  That's true for everywhere.  People complain about the wait times and 99/100 it's for something that,  might be an emergency to them,  but not as urgent medically ie.  possible death.   Not in OH they aren't!  They're supported by our taxpayers.  There are SOME independent ambulances but if you call 911 they aren't the responders, the county ones are.  Thank god. [deleted] That's enough to buy the damn ambulance!  Many older people retire and move to Florida. Those hospitals are just cashing in on older people having serious medical issues, and needing to seek treatment more often. That doesn't make much sense to me.  If it was just old people, they wouldn't be counted among the uninsured due to Medicare.

Oh wait... actually this does make sense.  If you have a ton of Medicare patients (or worse, Medicaid), you're gonna have a shortfall, hence the overcharging.  Yeah... that actually could be it. That is precisely why many medical facilities flat out do not accept Medicare or Medicaid. In addition to receiving crummy reimbursements, both require superfluous amounts of paperwork prior to any treatment and a lot of follow up to actually receive money. Essentially, you have to work harder to get paid less. &gt;many medical facilities do not accept Medicare or Medicaid

This really only applies to physician-owned hospitals or specialty-specific facilities (think an orthopedic surgery center) that don't need to run emergency rooms.  That and physician practices that don't opt to see Medicare/Medicaid patients.  Virtually all not-for-profit and for-profit hospitals take Medicare/Medicaid.  
 It looks like almost all are owned by two companies HCA and CHS that are fairly widespread in Florida.  The governor of Florida was the former CEO of HCA and outspoken opponent of Obamacare?   A mere coincidence, I assure you.   Halliburton won those no bid contracts fair and square. HCA isn't even based out of Florida, which is what's surprising to me. Also, that company is broken up into so many different parts, I don't even know how they find their own asses. Sadly, most of the self-insured hospitals are falling into the larger conglomerates' hands with all the malpractice suits that happen down here. :-/ Florida is the state with most fraud, in any category, than any other state, Tax fraud, insurance fraud, billing frauds, you name it. Including the governor, he won't reveal to us that he is really voldemort. Rearrange his name and you get:

Trick Cost


(Or maybe Crock Tits) Our second term governor ~~is a former hospital admin once convicted~~ was CEO of a company that owned hospitals,  one of which was convicted of medicare fraud... Enough said.

Edit: Thank you for those correcting the details of my hastily written mobile reply. And I agree with those of you who still lay responsibility on him as a CEO even if he himself wasnt convicted. Corporations are legally set up to distance its  execs from liability. This doesnt equate to innocence IMO. Where to begin.... Florida's got pretty loose regulations on basically everything, and we have a very large population. I don't know that we're per capita any worse than very rural states, but the large pop means we're always gonna be high on a list like this. [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] I used to work for a hospital in Madison. I was talking to a higher up and he was telling me a large reason we overcharge people is because of how the hospital loses money treating people with Wisconsin state aid. It's even worse for people with Illinois State aid, who usually runs out of money by March every year. Meaning you receive 0 money for treating somebody with it. 

And it's only going to worse. He was saying the new Hep C treatment is so expensive it'll likely bankrupt Illinois. 

A big issue is the current battle between obscene drug costs and insurances refusing to pay it. The new oral cancer meds cost $15,000 to $25,000 a month. And the insurance doesn't agree with it. So often when the patient leaves with the medication your pharmacy has made about 50 dollars.  But you've spent far more than that in man hours getting the medication covered by insurance. 

Basically, fuck the whole system.  they charge 100 dollars for 5 dollars in cost, get paid ten dollars and claim a 90 dollar loss.   and note the Affordable Care Act does little to bring prices down in general because we are subsidizing already too expensive insurance plans We don't need insurance.

We need health care. As digitaldavis pointed out, it's not insurance we need.   We need health care, and with a nationalized health care, these insurance and hospital relationships need serious regulation.   The top comment in this thread is a perfect example.   Someone owing a quarter million dollars because they got hit by a drunk driver is unacceptable.  This thread has so. much. confusion.

Source: Was an EDI programmer for a health insurance company, specializing in ANSI 835 claim payments.

Your insurance company is Acme Insurance. Your hospital system is Seattle Grace. These two parties negotiate rates for individual procedures. Every medical procedure is coded as a numeric **procedure code**. There are thousands of procedure codes. The collection of procedure codes and prices are a fee schedule.

For example, code 47.01 represents a laparoscopic appendectomy. Acme Insurance agrees to pay Seattle Grace up to $5,000 for this procedure if a patient sees them.

Your appendix hurts like a little bitch. You are covered by Acme Insurance and you go to Seattle Grace for a laparoscopic appendectomy. You are saved from mortal danger and you have minimal scarring.

Time to settle up. **As a courtesy** and to avoid issuing a refund later, Seattle Grace bills Acme Insurance before you. Seattle Grace can bill for any amount on this procedure, but if it is over the contract rate, it will be discounted. Since you are covered by Acme Insurance, the amount eligible for payment is **the lesser of the billed amount and the contract rate**. The system just does this:

    eligible amount = min(billed amount, contract rate)
The "discount" is the difference of the billed amount and the eligible amount:

    discount = eligible amount - billed amount
The "discount" is **not** a percentage of the billed amount.

For example, Seattle Grace bills $30,000.

    Eligible amount = min($30,000, $5,000) = $5,000
    Discount = $30,000 - $5,000 = $25,000

Another example, Seattle Grace bills $30.

    Eligible amount = min($30, $5,000) = $30
    Discount = $30 - $30 = $0

Seattle Grace wants the most money it can possibly get. The easiest way to do this is to bill for an amount that is so high that it will be well above each insurance company rate for the foreseeable future. In the second example, Seattle Grace could have received an additional $4,970, but they did not bill that much.

Once the eligible amount is determined, then Acme Insurance runs this through your benefits to see how much they will pay Seattle Grace. The difference goes to you.

If you don't have insurance, there is no negotiated rate in place, so you receive the outrageous price. Sometimes Seattle Grace will be benevolent and adjust for this with a cash price.

Both Acme Insurance and Seattle Grace have access to their fee schedule. Theoretically, if you give a procedure code to either of them, they could tell you the price. Your insurance company probably has a feature on their website where you can estimate the costs by selecting a procedure and provider.

Personally, I hate this system. The federal government should set the rates for all procedures. So basically we need someone to hack into all the major insurance companies and publish their fee schedules to WikiLeaks. [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] I live in a country where, i still have to pay for health care, but the government makes sure if you're ill you can get treatment regardless of the fee and helps to foot a decent chunk of the bill.

Every time i hear something about the American healthcare system i get more and more disgusted totally and absolutely. Why do American's stand for this? As in, i remember during the whole Obamacare thing there was alot of arguments against it being all "hurr socialist medicare".

Can somebody explain to me why there are people actively against changing the US healthcare system? It's the people who make the money from it that don't want to have it changed and they have the money to make sure it doesn't. Misleading headline.  These are the *top 50*, not the only 50.  And in all honesty, pretty much every hospital overcharges the uninsured.  Being uninsured is playing Russian Roulette. So we set a system that makes sure they get paid no matter what they charge and no regulation on the charges. What could go wrong? Damn, these guys cant afford health insurance, better charge them 1000% more. I bet they can afford that. [deleted] [deleted] [deleted] [deleted] I don't get why healthcare is so expensive.  I blame it on the politics, the insurance companies, and of course money.  We live in an age where we have advanced medical technologies.  Many surgeries have become so efficient that the patient can go home that very same day.  My question is, since we have developed many new technologies, why has cost gone up instead of down?  Computers for example, back in the late 70's and 80's, a personal computer costs anywhere from $5000 on up.  Now we have computers and even small tablets that cost a couple hundred bucks, and are thousand times faster and more effecient.  Why has many things gone down in price, but health care system is at an all time high?  It's like the same idiots banking on healthcare are the same idiots banking on student loans.  Why as a society, did we decide that we MUST profit from healthcare and education?  Greed. [deleted] The hospital where I live charges $25 for whatever you need and staying in the hospital costs $12 each day, everything included. 

The total cost of both my children being born is $60 total.

Sweden rules! The hospital where I live charges $0 for whatever you need and staying in the hospital costs $0 each day (unless you upgrade to a private room), everything included. 

The total cost of my child being born was $0.

Canada rules! [deleted] Lawyer here.

This is less about "overcharging" and more about transferring costs.  Two factors are in play here: (1) the arms race between hospitals/providers and insurers/Medicare/Medicaid and (2) lawsuits.

Say you're an indigent wage laborer with no insurance and no assets.  You're on the way to your minimum wage job and you get hit on the highway.  You're going to need to go to the E/R just like someone who has insurance.  At the E/R you are just like any other patient in one important respect: you can sue a hospital or doctor who fails to provide you with adequate care *even if you have no assets and no insurance.*

The hospital, fearful of lawsuits, is going to go ahead and give you an MRI, a series of x-rays, maybe a CT scan, to rule out spinal cord injury or broken bones.  If you happen to have a herniated disk in your back or actual broken bones, the hospital will offer you care, possibly including fixation surgeries, in order to avoid any malpractice claims. 

Now in the scenario above, you can't pay the hospital back, but there's a good chance you'll have a lawsuit against the person who hit you.  The hospital knows that if you ever get paid on that suit, it would have to pursue collections actions against you for your outstanding bills owed to them.  In any collections action they are going to incur costs and likely settle the claim for less than the outstanding bill.  What actually ends up happening most of the time is that the plaintiff's lawyer for the injured person clears medical bills and liens by settling out with the providers, *for a discount* out of the settlement funds paid by the at-fault party's insurance company.

Hospitals thus know that they are going to get whacked on their bill no matter what: either the indigent person stiffs them outright, or the bill gets paid at a discount settlement via a collections action, or the bill gets paid again at a discount out of the proceeds of any potential settlement.  For every ten indigents to whom the hospitals provide care, the hospital might recover payment (usually via the third option above) one time.  They are typically not allowed (by law) to deny care to people who walk in, although they might try to ship them off somewhere else.  Ultimately someone is going to provide that care.

This is simple math.  If you only get paid one out of ten times, functionally, then you need to charge 10X the normal rate in order to break even.  You need to charge ALL TEN people 10X the normal rate, because you're only going to get paid once, and you need that one payment to make up for all the other non-payors.

All of this adds up to a great argument for some kind of blanket health insurance.  I don't care if it's government supplied, mandated through private insurers (which is Obamacare), or some kind of hybrid.  As long as every person who walks through the doors of the E/R has some financial backing and means to pay, then the hospitals can stop playing this shell game and begin to charge each individual 100% of what they owe instead of 1000%.

Now that we are making baby steps towards universal insurance coverage, the next step is to begin putting pressure on the hospitals to change their billing systems to reflect their actual payment rates.  Over time, in theory, we should see charges begin to drop as the rate of recovery for outstanding bills improves as more and more people are covered by some form of insurance.  There are a lot of ways to bring that pressure (including, in my profession, allowing defense lawyers to discuss insurance coverage and fair market value with juries, which is currently outlawed by something called the collateral source rule in many jurisdictions, thereby bringing down overall lawsuit values and relieving runaway verdict pressures on the insurance system).

Tl;dr: 1000% charges are a symptom of hospitals getting paid 1 time out of 10 and the solution to this shell game is some form of universal insurance coverage. Maryland solved this problem very well through their hospital rate regulation system.

All hospital services have their prices set by the State, and the State controls how much profit each hospital is allowed to earn. In order to even the costs out to stop hospitals in poor areas from having to gouge their customers because of the low collection rates, the State Health Services Cost Review Commission calculates the annual cost of providing "uncompensated care" for each hospital then calculates a statewide average. Any hospitals with less than the average have to contribute an equivalent amount of their revenues into an uncompensated care fund to bring them up to the average cost, and any hospitals with more than average uncompensated care costs receive money from the fund to being them down to the average cost.

Add to that the fact that the same Commission closely monitors hospital billing to ensure that hospitals don't discriminate based on who is paying for care (Medicare, private insurance, no insurance all MUST be billed the exact same amount or hospitals have profits seized) and it means that you don't get one or two high-risk groups having to bear the total burden of healthcare. The risk gets evenly distributed across all patients statewide.

The other nice thing that Maryland does is when they set the allowable profits for each hospital, they build in incentives like reducing readmission and offering higher levels of charity care (forgiving the fees of low-income patients), so that hospitals are permitted to be more profitable if they meet certain performance goals. This does sound like a smart solution. [deleted] [deleted] [deleted] Healthcare as a commodity is the only thing you need to know about the morality in the United States. Of all the things the ACA didn't give us, it was the transparency we were promised. IMHO one of the things we could do that would have the most impact is for hospitals and doctors to be forced to publish their pricing and eliminate preferential pricing for insurance companies. We should all pay the same rate for the same service and we should know what that rate is.   Hospital billing is ridiculous. When my sister was having her first kid. A translator came in and ask if she needed translating. My sister answer no in English and they left. 2 months later, she got billed for needing a translator. I live in Canada and it doesn't cost me anything for doctor or hospital visits, should I need one, or routine blood work. I think an ambulance ride is $60.  Threads like this make me feel so incredibly grateful for the services we take for granted here every day. Sure, the wait times are sometimes criminal. Sure, there have been cases where a person has died in the ER before anyone even assessed them... But on the whole I feel like this system works. Hearing these stories, and reading the cost breakdown associated with health care in the US just makes me feel awful. Ughhh. I feel for you guys.   Who's the nazi that deleted the whole top of this thread?  [deleted] Is this the actual price or the advertised price. A lot of hospitals jack their rates up super high and then "adjust" them for insurance groups and if you're uninsured you have to ask them to reduce it to the uninsured rate.  I like to put it this way when speaking to healthcare colleagues from the US. It is not that the rest of the world has a better system or systems, it is that the USA has a bad one. [deleted] Because the optimum time to shop around for the best prices on medical care is when you *need* that medical care.

"Hello, yes?  I'm currently having a heart attack.  How much do you charge for cardiac servicing?" "WHAT?? That's simply too much! Look, buddy, if you can't make me a reasonable deal, you'll be losing yourself a customer!" Listen I understand there were complications in surgery that required extra care, but you should have stopped right there, and come to ask me if I wanted those extra services that prevented his death in this risky surgery.
 [deleted] Life saving services cannot be subjected to market forces. You *might* be able to subject things like outpatient surgery and clinics to market forces, but that would require everyone in the system to actively promote that. 

Medicine likely can't be an efficient market anyway because of all the occult information involved. You are likely not a doctor or a specialist capable of accurately assessing the care you are likely to receive. 

Not to mention that the gouging here is being done to the people most exposed to the 'market forces', and they're charging what they can. Partly because if you can't afford insurance, you're probably going to have trouble getting to a hospital much farther away, and partly because there's no one to explain the things you need to know to make informed market decisions.  Perfectly free markets require completely accessible prefect information. Medicine is excellent example of an Industry that breaks this kind of model.  Market forces don't really apply to medicine anyway. If you're dying, you'll pay pretty much whatever is asked, regardless of the actual cost. If you're having a heart attack, you're not going to shop around for an ambulance that's cheapest. Nobody has an annual appendectomy sale. to be fair, that is the point of insurance. You make decisions when you are healthy so you don't have to make those decisions when you are sick. There have been many studies regarding the effects of general anesthesia and it's neuroapoptotic effects. The general consensus is that there is a linear relationship with neuroapoptosis increasing as number of general anesthetics and cumulative time under general anesthesia increases. This is of course only done in animal models such as rats and monkeys, but more recent retrospective multicenter analyses are being done to assess its true effect on human children and their cognitive development. These effects are less profound as children stop their effective cognitive development and age. A recent breakthrough though is that use of drugs in the class of alpha 2 agonists, specifically Dexmedetomidine (Precedex) have been shown to be neuroprotective and directly decrease neuroapoptosis in animal models. My opinion is that general anesthesia for elective procedures should be postponed until the brain is more developed and you should have a serious conversation with your surgeon and anesthesologist about your options on the type of anesthetic and the possibility of postponing an elective procedure if at all possible. :) 

Source: I do anesthesia for a living. 

Edit: a few links if you're curious 

http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3096156/

http://webcache.googleusercontent.com/search?q=cache:wSotXJr-H2wJ:anesthesiology.pubs.asahq.org/data/Journals/JASA/931085/0000542-200905000-00023.pdf%3FresultClick%3D1+&amp;cd=4&amp;hl=en&amp;ct=clnk&amp;gl=us

Edit: Wow, this is my first gilded post. Thank you so much kind stranger. :)  So are there some anesthetics that  are shown to be better and are currently in use?

I tried to have a conversation with my three year old son's surgeon on this topic and felt blown off. He needs a second surgery - a prior hydrocele repair at 3 months old somehow created a trapped testicle that needs resolved. When I brought up concern due to him needing general a second time (he had language delay and an autism diagnosis) I was told all evidence, including twin studies, showed no real worry. I guess I don't really have a point to this paragraph, I'm just a frustrated parent who hates dancing around stuff with doctors.  I am sorry to hear that and he sounds like quite the trooper! Many surgeons brush off the questions because a lot of anesthesia is very poorly understood, even by anesthesia professionals. Perhaps he felt ill-equipped to answer your questions about it. My honest opinion is that anesthetics have been shown to cause neuronal cell death to some degree. That being said, children's brains display an incredible degree of neuroplasiticity- that is to say that when neurons die they make connections to new ones much quicker and in a fashion that adults cannot. As far as anesthetic agents go, all of the inhalational agents exhibit the same effect. They all depress neural activity to allow surgery to happen without you being aware and without the perception of pain. My only suggestion would be that you ask about drugs like precedex and that they be included in the plan if at all possible or just delay the surgery until he is no longer a child. Based on what you've told me, the trapped testicle is already effectively dead in the sense it will never produce sperm again so what is the hurry? I hope that answered some of your questions and if not, please keep them coming Oh my, I was told it will probably be fine as long as they remove the thermoregulation issue, though they wouldn't know 100% until until he reached maturity. Now I feel all kinds of uninformed. The surgeon actually waited (knew about this issue since the first surgery) until he was three 'just in case' but felt it should be gotten over with once he turned three.  By that point I was 8 months pregnant with my second, so they postponed it again. I'm supposed to be calling scheduling to set up the surgery any day now., but now I feel I should have a serious chat with his doctor over realistic outcomes and risks. 

Anyhow, thank you for your insight (I sincerely mean that). You've given me a lot to think about.   Thermoregulation can be an issue sometimes, but now we have things like convection warming devices, IV fluid warmers, and just warming the room up! The convection warming devices are called BairHuggers and they're awesome :) 
http://solutions.3m.com/wps/portal/3M/en_US/IPD-NA/3M-Infection-Prevention/products/catalog/~?N=7570550&amp;rt=c3
Good on you for postponing. :) anesthesia with pregnancy can be a sticky issue because we have two patients now. I would just say that the vast majority of people, especially adults, do not have any permanent cognitive delay from general anesthesia and those that do are very often visiting for 5-10+ procedures that are hours at a time. Anything that depresses brain activity enough to keep you from being aware of or responding to surgery is a strong drug and should be respected as such. :) you will be just fine, no worries  By thermoregulation, they probably meant with regard to the testicle. By "trapped", I assume the surgeon meant undescended. There are very real consequences to cryptorchidism (undescended testicle) that most would agree necessitate early intervention. Oh then I may have misunderstood the question. I have no idea in that regard and I refer to your expertise, sir! It seems OP was referring to thermoregulation in regards to the testicle, not the temperature of the OR. Wait, if children are able to bounce back from injury better than adults due to a higher neural plasticity, why is it best to wait until the brain is more developed to have the procedure involving anesthetic?? I suppose I would make the analogy that cutting a branch off a large tree is not as detrimental as cutting the stump of a small one. My point was that children can regrow quite well, but if you have the choice, why make a risk? I took care of a child that had refractory epileptic seizures a while ago and we cut the connection between his two hemispheres of his brain because there was no other choice. If I did that to an adult, there would be almost no chance of him moving one whole side of his body. Two days post-op the child was moving his other side of his body. I guess I'm trying to say that the brain is strange and I can't pretend to understand it all, but children's brains are very resilient nonetheless. What I saw was a miracle to me, and all due to the fact that the child didn't have the proper scaffolding yet to say this neuron does this and this neuron does that. Their neuroplasticity is simply amazing.  &gt; we cut the connection between his two hemispheres of his brain because there was no other choice. If I did that to an adult, there would be almost no chance of him moving one whole side of his body.

Huh? Are you talking about a callosotomy? Split brain patients have some limitations in processing but paralysis isn't one of them. If you were cutting at a level where there were efferent motor tracts decussating, BOTH sides would be affected. Did the surgeons do anything else during the surgery, such as a lobectomy or cortical excision? Ayup, doesn't damage the function of each hemisphere, but does limit communication between the two. Each hemisphere keeps on functioning largely normally, except for the somewhat spooky fact that depending upon which hemisphere you ask the question of, you will get different responses sometimes.

I.E There are some people with a theist hemisphere and an atheist hemisphere. Google Ramachandran corpus callosum if you're curious.

T;DR Iunno, this guy seems to not know some very basic things, see issues with temperature and undescended testes, not actually knowing what a corpus callostomy is. See also: ketamine is not an anesthetic. He said last year that he was an "anesthesia student" so I presume he is an AA or CRNA, not anesthesiologist.

Wow, that's really interesting about theist vs atheist. I wonder if I have a theist hemisphere and just don't know it. And if you're a fully grown adult? You may have some neuroapoptosis (neuronal cell death), but it will not effect long term cognition. It will simply leave you feeling "hazy" or having thoughts "on the tip of your tongue" for a few days. The brain is very plastic and remakes the connections quite well. The older you are, the harder it is to make the connections again. The geriatric population has a prolonged depression of cognitive function to general anesthesia as well mainly because of the design of the brain as it ages. Anesthetics all work on something called a GABA channel that is inhibitory in nature. The older you are, the more GABA activity you already have, and thus the more sensitive you are to anesthetics :)  Why are you never warned about this when you have a surgery under general anaesthetic?

Is this a recent discovery?

I've had two surgeries under general anaesthetic, one when I was 14, and another when I was 19. I may need another at some stage in the near future (mid twenties).

What do you think the repercussions would've been for myself?

And should I have another surgery, will there be further harm done given the cumulative effect?

Thanks. Since your brain is no longer in the critical stage of development, effects of general anesthesia are overall very minimal and the least of the concerns of most surgery. I am sorry to hear you've had some surgery recently :( not to worry though, your brain is safe. The reason people are not warned about this is multifactorial:

1. In adults the effect is often negligible and unavoidable unless you'd rather be aware during your surgery if regional or neuraxial anesthesia are not possible. Regional and neuraxial are great alternatives that often do not involve a full general anesthetic.
2. Many surgeries are not elective, so discussing the risks/benefits of anesthesia is pointless when you will need it regardless. 
3. An overall lack of communication is also to blame to some degree because quite honestly, the anesthetic risks are usually much less than the surgical risks of a procedure; exceptions being obstetrics, trauma, and cardiac surgery usually. 
4. People really are afraid to ask these questions. I understand completely. Anesthesia is a mysterious, unknown, and misunderstood thing by many people. It's not sleep and many people think we just turn a knob and they go to sleep when thats the easiest part of the job. Its both the practitioner's fault and the fault of the system. Patients deserve to know more and should inquire more about how anesthesia actually works

I hope that helps you some. You will be completely fine, no worries :)  I'm just a vet student but my understanding is that there is sometimes a choice in human med between GA and neuroaxial or regional anesthesia for the same patient/same procedure. Do/should anesthesiologists take these cognitive effects into consideration when you're discussing the pros and cons of each approach? I see what you're saying but I think the risks of sticking a needle somewhere are more obvious to a layperson than the nebulous effects of these drugs.

Also, you mention that alpha-2 agonists are neuroprotective and that volatile anesthetics all have similar deleterious effects. Is there any difference between volatile agents and TIVA? Well that's a good question. The easy answer and the most often true is that we do regional and nueraxial anesthetics for many other reasons:

1. They allow us to avoid dealing with securing an airway in many cases. Securing the airway is a major cause of morbidity and mortality especially in obstetric surgery. Less invasive is better!
2. They cut down on use of narcotics which have side effects like constipation, urinary retention, nausea, vomiting, addiction, and withdrawal
3. They block many components of the systemic surgical inflammatory response that causes things like delayed wound healing, massive inflammation, and immunocompromise. 
4. They improve perfusion to the blocked area, reducing risk of blood clots, blood loss, and reperfusion injuries. 
5. Overall they are safer for patients and improve patient satisfaction. 

The overall effects of anesthesia on the brain are still being researched and not greatly understood but less is often more and safer overall! Thanks again for the response, I'm not trying to illicit a response of reassurance by the way, nor scaremongering for that matter. I'd just like to know what effects several such anaesthetics could have, particularly given I was 14 when I had the first - surely an age where I'd be at greater risk?

Please be honest, I'd sooner know and learn a bit more than read something I want to hear.

Thanks again. Actually many studies indicate the pathways for neural development often end by the onset of adolescence, so I would say you are safe. To be honest you sound quite intelligent and your concern is a good thing because these are questions everyone should have. I am very confident that your brain was minimally affected and for a short enough time to not warrant concern. :)  One of the key points he mentioned was about how there are areas of anaesthesia that are poorly understood.  There were plenty of things physicians were doing because we knew they worked. Just not how. I've been working on a literature review of Long Term postoperative cognitive dysfunction. One of the reasons was that I found it interesting and also because I am young like you and the research tends to focus almost exclusively on the elderly and at best those over 50. There is very little research or even a belief that need for research in our age group is necessary because it's not considered to be a long term issue.  Maybe future work will change that. It's also possible that a lot of the degenerative effects we see that are long lasting are not down to the anaesthesia but what happens s afterwards. I'm talking mainly about social issues. Such as lack of human interaction for extended periods of time. Loss of a job and so on. These can lead to a drop on cognitive ability and the elderly are most at risk of these also.
I'm ttyping from my phone on my way to work so excuse any typos.  You can check Google scholar for I think Rasmussen and vizkaichipi. I can't remember other current researchers off the top of my head. Also reviews on the ICPOD studies 1 and 2. Short summary though is that you probably have not had any drop in cognitive ability. But to be honest we couldn't possibly know without having tracked you and others against a sizeable sample size. The very young and the very old are the areas of concern. I think worst case scenario you may have lost an extra game of poker once or twice.  Bear in mind that I'm quite junior at all this. My experience in anaesthesia is about three years and very little of that time has been on this topic. It's only been down to my interest. &gt; Anesthetics all work on something called a GABA channel that is inhibitory in nature.

That's not universally true. There are a few, like ketamine, that don't, and we don't know how most of the inhaled anesthetics work at all. My first reaction when reading the title was, "children who need surgery requiring general anesthesia probably have other conditions affecting brain development." Glad you provided science that backs the claim. Thank you very much for saying that and I hope I helped a little! For sure it can be hard to distinguish between children that have things like multi factorial syndromes that affect overall cognitive development and the fact that they have multiple surgeries and anesthetics for said congenital abnormalities. The most recent studies even try and remove this from the equation and quite honestly it can be very had. That being said, sometimes anesthesia is unavoidable and knowing that there are inherent risks with anesthesia as with surgery can help set expectations and hopefully pave the way for safer anesthetics. :)  Other posters have praised you, but thank you for knowing what the hell you're talking about. This made me genuinely feel like I understood the import better than before. Thank you for your kind words. It's high praise to be sure. I think that the more information you can have going into something, the better off you are and a big part of my job is to provide people with that information :) I am more than happy to help As a parent of 2 little ones, I'm starting to worry. Needlessly or not, I'm not sure. My 3.5 year old is scheduled to be under general anesthesia for dental surgery one month shy of his 4th birthday for 1.5 hours. Could be a 2 hour procedure, could be a 1 hour procedure. 

This is an elective surgery. He has 10 cavities, some of which are 2 years old and the dentist has said it could be genetic as I also had terrible baby teeth. The dentist told us that while the teeth were clean, many are really bad inside.

Should I be postponing this surgery until after he is 4 years old? Should I not have one at all? I'm trying to figure out what the best way to go about this:

Either surgery, or 5+ visits to the dentist (which could lead him to hating dentists for long, long time). Any insight from anyone is greatly appreciated. Nomoreheroes: I understand your concern. I work at a pediatric hospital that does anesthesia for dental caries very frequently. Judging by the state of his teeth, I would say this is more than likely not elective. That being said, this procedure is often relatively short. "Longer" anesthetics are those that last for 4 hours or more and longer is often a factor. Ultimately, as a parent this is your decision. I will just say that your son is at minimal risk for any long term cognitive development. Anesthetics are much safer now than they once were and most pediatric hosptials use precedex regularly to make children more sedated and less combative when they wake up from anesthesia. The fact that they are neuroprotective is an added bonus and something I see being a big deal later on. I would simply ask about these kinds of drugs and their role in his anesthesia. I would caution against saying use as little anesthesia as possible though, as you want to not remember it. It's a fine balance and we do our best every day to make sure that the patient, family and surgeon are happy. I hope that helps you some and please do not worry. :) Thank you wsides. Your words did make me feel better. And to be fair, he will be going to CHEO (http://www.cheo.on.ca - I'm from Canada) which is a hospital specialized for children. So I can't ask for any better care. 

It's astonishing how I can emotionally get when dealing with my own children, when I'm supposed to be a rational individual! 

Thanks again for replying :) Of course I am happy to assuage any concerns you have about anesthesia and the whole childrens healthcare conglomerate is very well-renowned. I myself work for one in the USA. :) Children are precious and it's only natural that you love your children so much. I would feel the same about mine. You're a great mother for wanting to know more about your son's healthcare and taking a proactive approach. He will do great, I am sure!  I had two reconstruction surgeries on both feet (one while 6 months, another while in pre-k) and I turned out just fine. I graduated valedictorian and have a year of college under my belt before graduating high school.  Just think what you might have been! wtfusob with club foot portals in from another dimension.

"I would have invented INTER-DIMENSIONAL TRAVEL!"

Portals out. &gt;...neuroapoptotic effects on neurons.  
&gt;The general consensus is...

Paging Nurse Redundant to the operating room... 
^^^(sorry)

 Is general anesthesia to blame, or just the problems the come with suffering from conditions severe enough to warrant surgery? I believe there was a similar study published in late 2011, early 2012. I know because I had read it after my 3 month old daughter had just had anesthesia for the first time and we were a week away from her next surgery. It gave me pause, but I couldn't imagine explaining to her years later about why we let her go blind when there was a procedure which had an excellent chance of preserving her vision available. I totally did an existential quality of life measurement. 
Sorry, may not be relevant to the sub, but I am certain there is another work which is older

Edit: thanks for the messages of support. She is doing well and it looks the surgery on her eyes has been a success. She does 6 month follow-ups as opposed to the monthly before. She has a genetic mutation which is x chromosomal dominant called, incontinentia pigmenti. 

Also, mad props to all the folks at Children's Hospital of Philadelphia, we've had to use many of the departments and they are all phenomenal, and the special specialists at Penn. If it helps, you absolutely made the right choice (and it would have been the right choice even if surgery were guaranteed to cause her some mild cognitive delays).  I hope your daughter is doing well. I hope your daughter recovered and you don't need to have that conversation If you don't mind my asking, what disease does your daughter have? This post caught my eye (pun intended) because I have a rare disease called coats that would have left me blind if I had not been periodically treated under anesthesia from a very young age. Speaking as an offspring, you and my parents made a great decision to preserve our vision. Thank you for being a good parent! Well

&gt; Loepke and his research colleagues have published previous studies showing widespread cell death, permanent deletion of neurons and neurocognitive impairment in laboratory rats and mice after exposure to general anesthesia. It's still a perfectly valid question since humans are not rodents.  Still, it gives some theoretical basis.  Totally, wasn't implying it doesn't, I even defended it for those reasons down below.

*Edited for redundancy. [deleted] Still a valid question but the whole "humans are not rodents" thing is stupid to say. Rodent receptors, cellular mechanisms, cytoplasm, extracellular matrices, etc... they are the same as humans. They are the same in essentially all mammals. Rats are just easy to raise and make tons of. Rats and mice help guide us to more human specific discoveries. Saying "humans are not rodents" is not useful criticism of a paper or research.  As a counter point, there is a a good difference between rats and people.  I can cite the case of saccharin, the artificial sweetener that, during testing, caused bladder cancer in rats. it was subsequently banned in many countries. Only later did they find out that the bladder cancer does not occur in humans because the pH of the urine is different. So saccharin is safe in humans, but not rats.  Safe for humans with a urine pH within normal human boundaries.  We're talking about neurological stuff here. And in terms of protein coding difference there's almost none between humans and rats, it's mostly expression level differences.  &gt; My name is Azra Raza. I am an oncologist, professor of medicine and director of the MDS Center at Columbia University in New York. The scientific idea that I believe is ready for retirement is mouse models must be retired from use in drug development for cancer therapy because what you see in a mouse is not necessarily what you are going to see in humans. 

&gt; From the mid-90s to now, about 20% of drugs are actually entering clinical trials and FDA approved. But 90% of the drugs still fail because of either unacceptable toxicity, or once we give them to humans we find that theyre not working the way they were supposed to. So why are these facts so grim? Because we have used a mouse model that is misleading. They do not mimic human disease well and theyre essentially worthless for drug development.

http://freakonomics.com/2015/03/05/this-idea-must-die-full-transcript/ Mouse models are not perfect but they do work and a TON of great science has come out of it. One doctor saying one thing is not enough to overhaul biological research practices, there needs to be a consensus. Drug design is very hard and takes forever. The fact that most drugs fail is not because of the mice or rats but because of our own designs. The mice and rats are tested *after* they design the drug. That's how they first test the drug and most drugs fail the mouse test. It's not like most drugs pass the mouse test and then fail the chimp or human trials, most things fail far earlier than that. Freakonomics is not a good source of biological research critique. He is using very broad models of incentives and economics and applying them too broadly to very specific fields. All life on Earth has the same origin as far as we know. Mammals especially share the same transmitters, cell signaling mechanisms, and so on. 

Edit: I am definitely speaking too broadly myself (ironically) but I still stand by my statements since it's at least less wrong than what Freakonomics is implying.  I agree with you, but I wouldn't say Azra is completely wrong either. However he's not being very helpful, of course rat models aren't perfect predictors of human models because...they're rats. 

What rat are is cheap. Obviously the best way to test a drug would be to test it on humans, or non-human primates. However it is unethical to test experimental drugs on humans without EXTENSIVE knowledge, and even then MILLIONS of dollars need to be spent to gather subjects, garner legal support, analyze samples, etc. And primates? Well they cost about $20,000 for a lab raised sterile individual (each), and a Sprague-Dawley rat costs about $5...

While I agree the rat model might not be ideal for human-based drugs, ITS THE BEST WE HAVE. I doubt Azra is ignorant of that either, my opinion is that he wanted his name more out there and stated something obvious to the scientific community but shocking to the general public. 

Plus, rats are becoming better and better as models go. Check out the Jackson Laboratory, their entire deal is to genetically modify rats to fit into these human disease characteristics (schizophrenia rat, Parkinson's rat, diabetic-prone, etc). Fantastic organization 

http://www.jax.org


 Just to get nit-picky - Sprague rats are like $20-50 (+$3/day) While reliance on particular mouse models is a problem in the biosciences, it's much more nuanced than that. For starters, the vast majority of mouse models are built on a *single strain* - C57Bl/6, or the Black-6. Further, within particular disease research areas, entire bodies of literature are built on one or two genetic disease models that might not even have etiologies identical to the typical human forms of diseases. We lean toward genetic modifications that result in disease development in the vast majority of the animals rather than what is often the more typical case in humans where many diseases are the result of the accumulation of decades of environmental insults or other accidents of development or aging through the life history. For example, we tend to use Alzheimer's models in mice that rely on gene insertions of familial AD associated genes, even though the vast majority of Alzheimer's dementia patients don't have these gene  variants.

However, that doesn't suggest we should do away with mice entirely, or worse yet that they're worthless - in that regard, Dr. Raza overplays his hand. We just need to diversify our models a bit, use different strains if not different rodents or mammals more generally, and find more ways to get our disease models than relying on genetic models that aren't typical of human disease etiology.

It's also worth adding, in this case, that what we observed in our mouse models corresponds to our observations with humans. That strongly suggests that, at least in this case, the model got it right. You'd be surprised how similar we are.  There's a reason they're used for research so often. Yup, but not all pharmacodynamics or kinetics scale exactly. 

That said, stuff that causes necrosis and apoptosis usually does so across mammalian species. But dosage equivalence can be tricky, especially when a cocktail of drugs is used like with anaesthesia.  Especially given the complex metabolism of anasthetic drugs. [deleted] 
it's not a valid question because it implies,  to those who don't know better,  that this could realistically be studied in humans.    it can't,  because it would be unethical to give general anesthesia to healthy children,  especially if you think it might cause brain damage.   its unlikely there are many surgeries where a local anesthetic would suffice that could be used as a control.   observed in rats and humans.  that's enough. [deleted] Even seemingly routine surgeries like tonsillectomy and hernia repair (as below) are done at higher rates in individuals with other developmental problems due to conditions with [multiple congenital anomalies.](http://accesspediatrics.mhmedical.com/content.aspx?bookid=457&amp;sectionid=40092893) it's very important that we ask the above question. [deleted] [deleted] When I was a kid they would not give me a tonsillectomy until after I was 4.  Yeah they made me wait until I was six for tonsil and adnoid removal. I had mine removed before 4. I also couldn't breath anytime my allergies were aggravated which was all the time.To be frank, I did very well in primary/secondary school and college, but I can't help but wonder... Well Frank, I hate to break it to you, but you could've been a super genius. Too bad. If it helps, unrestricted airflow increases IQ. Oxygen is super important. Or umbilical hernia repair  Yeah I had that before I was 4. I mean I'm PRETTY sure I'm alright...  or tubes being put in toddlers ears for drainage Ear tubes are done under general. Both my kids had them. :( Don't fret - studies have shown that recurrent AOM can have effects similar to the effects highlighted in this study, so your children are probably none the worse for wear from the ~15 minutes of GA, and their quality of life was (I imagine) greatly improved from not having extraordinary pressure in their middle ear all the time. My son (almost 2) is getting tubes in his ears in 2 weeks. I was getting anxious reading this thread till I saw this. Thanks for the bit of reassurment. Our daughter had two sets, one at 1.5 years and one at 2.5 years and we have no regrets. The ear infections stop immediately. She was sick non stop without tubes. Now at 4.5 years, the second set fell out months ago and so far so good. Cave babies must have been constant screamers. But cave mom and cave dad were probably deaf from chronic ear infection so it worked out ok My son had surgery when he was 8 months old. He was under for 30 minutes. Today he's 2 years old and talks nonstop. 

A friend's son had tubed put in because of recurring infections. He had trouble listening because of the infections and is delayed on his talking because of it. He's now doing great and he has never had an ear infection afterwards. 

What I mean is that the benefits outweigh the cons. Your son will be just fine, actually, better after the surgery.  A friend of mine had a son with behavioral problems. He was mean, did poorly in pre-school, was difficult at home, etc. His doctor noticed his ears were inflamed and recommended he have tubes put in.

Within a week of getting the tubes, my friend said his son was a new kid. He was happy, played well with others, and started listening to his parents.

GA may have some risk, but it's not like doctors give kids anesthesia for fun. There's risks from whatever pathology requires GA. Ear problems can damage hearing, affect balance, and cause pain. Tubes can be a huge relief to a kid that's probably unable to fully describe how much pain they're in. Thank you. I wouldn't worry too much! I had multiple grommets/ ear tube insertions, and ended up having a radical mastoidectomy and multiple surgeries on my ears when I was 2-3 years old. And this was in the 90's! I'm now doing my Master's in genetic counseling and am trilingual. So I wouldn't stress... Every statistic has it's outlier :) Me too. I read the title and thought we were all good but then I remembered the ear tubes :/ [deleted] [deleted] [deleted] [deleted] From the article:

&gt;Children included in the study did not have a history of neurologic or psychological illness, head trauma or any other associated conditions.

It's my understanding that they only took kids without any conditions that would directly or indirectly affect the outcome of an IQ test to ensure that any correlation could be attributed to the surgery.

As far as anesthesia being the culprit, dunno.  There's a lot that goes into surgery.  It could be particularly emotionally traumatic to a toddler.  It could be the anesthesia.  It could be the opiates afterwords (fentanyl injected in recovery and pills when released).  I'm not a doctor.  There might be more studies showing general anesthesia being particularly bad for the brain (some complications like pneumocephalus etc I know are bad).  It could be lots of things, but they only took individuals without any conditions that would change the outcome of the IQ test, so they at least narrowed it down a bit. Either way I'd take a lower IQ over death any day. For  children this age general is used much more frequently for non-life threatening things.  That's why this is important. If you can wait you should.  I can give a testimony here. I had nine surgeries under general anesthesia when I was one-year-old. It was a lawnmower accident, not a birth defect. In my early 20s I had a real deal IQ test with just me and a psychologist in a room together exhausting my brain for two or three hours. I don't know whether all that anesthesia had anything to do with it, but my intelligence is very lopsided. With language and reading comprehension, memory and abstract thinking areas I scored in the 120 range, but when it came to matrix reasoning and other puzzle and numerical type tests I was on par with Forrest Gump.  [deleted] Aren't most people naturally more adept at certain areas? Or was your result considered "abnormal." Psychologist in training here. A spread of about 10-15 points across different abilities is common. More than that and it is an unusual degree of variation (though the exact amount depends on which parts you're comparing). So to go from 120 on some subtests to "Forrest gump" (I'm guessing... below 80 because 80 is where average range ends) would be roughly 40+ points. That's a big difference, more than just everyday variability. [deleted] [deleted] yes this is the key question. The researchers controlled for neurocognitive disorders but I think it's important to consider non neuro related anesthetic exposures as well. Ear tubes for recurrent ottitis media may be associated with some degree of hearing loss. Adenoid and tonsillectomy for sleep apnea may effect children's concentration in school Because of daytime sleepiness.

I would like to see the study comparing outcomes between largely elective/cosmetic surgery related anesthetic exposures versus those that are less elective like PE tubes, T&amp;A etc That's a great question.  Further, could the healing process from the surgery itself (healing of the incision, rebuilding of other tissue, etc.) take away from the body's ability to develop the brain during that time period? You raise an extremely pertinent question but I do want to point out that anesthesia is required for less serious matters as-well. My daughter fell off her bike, chipped a tooth and had to have it removed. For the procedure they used an anesthetic and put her out. So basically a study that indicates early aged (untreated) trauma is linked to reduced IQ? How would treating the trauma save the IQ? I don't see what being treated or not has to do with this. The point the top level parent was making is that healthy children don't need to receive surgery to begin with. This is just generic surgery so whether the complication itself caused the IQ decrease, the treatment of the complication, or something else is irrelevant in so far that it's outside the scope of the study and would require further, more refined, analysis.  We need a comparison group which has surgery without anesthesia to see the effects on brain development. A flippant remark for sure, but there are so many factors that impact brain development in babies (including maternal IQ, socioeconomic factors, etc) that it is nearly impossible to account for all the unknown variables when looking at studies like this. Many kids have pre-existing brain abnormalities which are only picked up by extensive testing or brain MRIs, particularly in children with congenital heart disease. It's only a few decades ago that there was the mainstream belief that babies don't feel ANY pain, and therefore, were given no anesthesia or pain relief for surgery. This has subsequently been proven to be completely wrong. All general anesthetics affect the brain to some extent,but I'm a bit skeptical about the implication that anesthesia is ENTIRELY to blame for these adverse effects on brain development.  Anesthesiologist here. Not to beat a dead horse on a lot of the criticisms that have already been pointed out (especially given the fact that many sick kids requiring surgery are far from normal!), but there's certainly something more at play here than simply general anesthesia.  And I want to admit up front that I have yet to read this particular article as I don't have journal access from home at the moment.

There have been multiple studies done on the incidence of post-operative cognitive dysfunction in older patients undergoing surgery who received different types of anesthetics, ie general (what you classically think of in terms of "going to sleep" for the surgery) vs regional (epidural, spinal, or nerve blocks, where you may receive varying levels of sedation or none at all).  Interestingly, many of these studies found *no significant difference* between the incidence/persistence of post-operative cognitive dysfunction over a MONTH after surgery despite the type of anesthesia received!  Now, without getting too technical, this opens our eyes to another question: is it possible that general anesthesia may not be the real culprit here?  Why would patients who had entire surgeries done under spinal anesthesia, who were completely awake and communicative throughout the entire operation and received none of the medications (NO halogenated, volatile anesthetic gases) associated with general anesthesia, exhibit post-operative cognitive dysfunction?  Could it be that the neurohormonal, inflammatory, and physical stressors of surgery themselves are causing a cascade of events that lead to these neurological problems down the road?

Similarly, in these pediatric patients, could that same neurohormonal milieu, *a direct result of surgery*, be partly to blame for the problems being observed in the developing, admittedly fragile brain?

I don't know.  But I think it's irresponsible to pin this entirely on general anesthesia as a panacea explanation for these observations, especially when many surgeries that we perform on children may be life-saving, or at the very least quality-of-life improving procedures.

EDIT: because words are hard PhD candidate in neuroscience/psychology here. I'd like to salute you for what you do - a close friend of mine is in veterinary science and anaesthetics are a *precise field of work*; I can't imagine myself doing what you do. 

It is clear to me that there are issues with how the cohorts are being examined - a GA, surgery-dependent population being compared with a typically developing one. Just the fact that cognitive impairment of some sort is more likely to be present in the former population than the latter constitutes a confound - adding on the fact that psychometric testing of IQ is not likely to have taken place before the age of 4, and you've got a potentially surgery-traumatised, at-risk clinical population compared to a typically developing one, with no measurement of pre-surgery functioning that varies as a covariate. This can be problematic.

Small wonder why we find that we find diminished cognitive function and inter-group differences then, isn't it? Studies like these always leave me wondering "chicken or egg". Healthy kids under 4 aren't getting tons of surgeries. Also, it says that they only had children without previous neuro or psych problems, but at 4 years old those things are hard to diagnose. I would also like to see what kind of surgeries the kids had. It is always possible that the need for surgery was the first sign of an undiagnosed problem. This study should be redone with just children that had surgery for things like fractures of the arm, just to rule out the possibility of comorbidity. That is the only way to see if the anesthesia is truly causing the IQ to drop, or if something was just wrong with these kids anyway.  I think you along with many people in this thread are confusing this study as some sort of definitive statement.  It's a follow up on previous studies that showed rats and mice exposed to anesthesia also exhibited neurological deficiencies.  The purpose of the study is to see if more resources should be applied towards the effects of anesthesia on young children.

An edit since people seem to be making a counterpoint of "Article title is misleading/So what? we're just going to give up on anesthesia, I'm sure anesthesia is more beneficial for the patient during the surgery than not having surgery" and are still misunderstanding the article.  The point of this study is whether or not research/effort should be used to see if there are preventative measures or precautions that can be taken to reduce the effect of anesthesia on the damage it may have on the brain/nervous system.

 Every /r/science thread seems to have top comments with people picking apart the studies in ways that are totally ridiculous. They expect the studies to have answers for things that were *never* in the scope of the study to begin with.

They've gotta start *somewhere*, people. One of the main irritants here. ALL studies have limitations. Not only that, but people seem to think that these PhDs are totally oblivious to the fact that correlation doesn't equal causation. I'm pretty sure they know that.

Correlation, itself, is pretty fucking interesting, and worthy of further study. As long as they don't make any causation claims, it's not an issue. Absolutely they need to start somewhere. The issue is that these findings get taken out of context, and people will start decrying anaesthesia when there isn't enough evidence to do this. Whole threads of "Reviewer #2" [deleted] What he is reacting to is the headline. There's a reason these threads consistently have someone picking apart the study. It's because the study is introduced with a declarative headline, so people feel the need to qualify the claim.

Here's ask science in a nutshell:

**1. Title makes broad authoritative claim based on study**

**2. Top comment examines study and questions headline's conclusion**

**3. Secondmost comment explains how /r/science doesn't understand/appreciate the scientific process** You forgot the bit where peoples personal anecdotes trump the findings. My daughter had to have dental surgery under general anesthesia when she was two. She was under for 2 hours. 

My son had his adenoids removed under general anesthesia when he was four. 

Neither procedure was a matter of chronic ill health, nor was either absolutely essential to life. 

Edit: spelling Maybe not essential to life, but certainly medically necessary to quality of life. Children with extensive decay can have pain, infections (that can become life threatening), nutritional deficiencies, poorer school performance, etc. Children with large tonsils/adenoids suffer from sleep disorder breathing patterns that can lead to behavioral and emotional problems and a host of other conditions.  &gt; Children included in the study did not have a history of neurologic or psychological illness, head trauma or any other associated conditions. 
 Do they do general anesthesia when they put tubes in ears? I'm curious about the length of the anesthetics, the drugs and techniques used, and whether other variables were controlled (premature birth, age at the time of surgery). The article doesn't say, and I can't access the actual study.  There's a huge difference between a 10 minute anesthetic for myringotomy tubes and a 4 hour anesthetic for heart surgery.

We don't anesthetize kids unnecessarily, and we don't really have a choice among the techniques we can use for adults.  Kids don't take kindly to needles or the whole OR experience, so being awake with a spinal or nerve block isn't really an option.

One thing that may come of this is delaying surgical procedures until a child is older, if the risks of GA turn out to be bigger than the risks of delaying surgery. 
Now, that is an awful lot of variables to control for in a fairly small sample with apparently small group mean differences. Especially with the elephant in the room: Children below age 4 who received anesthesia did so for a reason, and sickness severe enough to require toddlerhood surgery seems very plausibly to have a greater impact on neural density and other brain development traits that the anesthesia per se. And if the disease was not enough in itself, the time spent in low-stimulus environments during both sickness and recovery might also be the underlying factors. So unless there is some more data I cannot see referred to in this article this seems a clear case of jumping to conclusions through possibly spurious correlation.

*Background:* I am a biologist doing a lot of statistics. Somehow I cannot find the original article so I cannot access the methods chapter directly. But, according to Science Daily 53 healthy participants were compared to 53 children who had undergone surgery before the age of 4. Average test scores for all 106 children in the study were within population norms, regardless of surgical history, controlling for age, gender, handedness and socioeconomic status.  I took countless 3 yr olds to the OR for severe dental decay. The other option is to strap them down and have them cry their lungs out for me to eliminate dental decay and infection from their oral cavities.  Brush your baby's god damn teeth before it comes to that. [This NEJM article](http://www.nejm.org/doi/full/10.1056/NEJMp1102155) concerns the safe use of anesthesia in children. It's a good read. My son was born with a unilateral cleft lip/palate in 2013.  He had 3 surgeries to repair it in his first year.  The last two were fairly long procedures (2-4) hours. He also had eartubes in a separate procedure that was about 15 minutes.  He is now coming up on 2 years old and while he has experienced some speech development delays he is doing great in all other milestones.  As a first time parent I was very concerned about him having GA that early, but the benefits in our situation greatly outweighed the risk.  Just wanted to write to let other parents know that as someone who has been through it, if a procedure will improve your child's quality of life you shouldn't let the possibility of delays from GA keep you from doing what is best for your child.  Children develop at different speeds even without exposure to GA or surgery.  
 Do they use GA when they circumcise baby boys? Nope. No, but that's even worse. There are schools of thought that tie the dramatic and traumatic experience of that during/right after childbirth to other complications later on in life.

They do when they're older though. I had mine done at 3 and they knocked me out for it. nah i don't think when they're babies , but later in life they do. 

Source: somebody on the internet that starts answers with "nah", you should probably go ahead and google it. Didn't they used to not anesthetize babies? They used to not anaesthetize anyone! But, yes. It turns out that there are cognitive issues caused by *not* using anaesthesia too.  All I can think about is my cousin's daughter, who was born with a major heart defect.  She had a few procedures and surgeries as an infant, and then a larger, open heart one right before her second birthday.  She still has side effects, like the need for oxygen when she gets a cold.  She also has a pacemaker.  All in all though, there is no way she could wait until she was four for the surgery.  She is at the best pediatric cardiology hospital in the country and she would not have these surgeries unless the doctors thought it was absolutely, positively necessary.  So, right now, I don't really care about a slight risk of a lower IQ.  She is alive, and that's all I care about.

PS - She turns four in August and is currently healthy.  She starts preschool in the fall! Waaalll -- not 100%.

My son was 18 months old when he had surgery for a strangulated hernia - with general anesthetic - and he went on to test in the upper 10% of all kids in the country, made the Dean's List 4 years of college and has a master's degree in Political Science.  He is now operations director for a mid-size newspaper and on several community boards.  

So, no, he never had any cognitive problems and still doesn't.  

Perhaps it was the type of anesthesia used or perhaps he was just lucky!  [deleted] We have to look more at the reasons though. For example you can die just from your body going into shock or too much pain. Would you rather have a dead baby or a kid that can't talk at 100% his level? There is no doubt that one has to apply anesthesia, if necessary, and the article explicitely says that many surgeries cannot be avoided and improve the quality of life significantly. At the same time the application of anesthesia can have certain negative effects.

However, the more we know about negative consequences of the application of anesthesia, the better we can develop strategies on how to cope with the dilemma in the best possible way. That is a really nice response. If I had enough money I'd gold you but you can have some [silver](/r/loungesilver) I got you bruh.  Also, here. Or develop alternatives too. Why would we try to if there is nothing wrong with what we have? I'll take option C: invest money in research to develop some newer anesthesia's that don't make the kids dumber Anesthesiologist here:  I'm all for option C!  There is precious little research and development of new anesthetics.  The last real developments came in the 1990's with propofol and the newer inhalation agents, sevoflurane and desflurane, 

We don't make a lot of money for drug companies, so we are largely ignored. This was always the craziest part of medical school for me. We really don't understand why these drugs work. We know they do, but we really have no idea why. I worry we're going to look back in 20-30 years and say, "We used to do THAT to KIDS?!?!" Not just kids, everybody.  But I'll be surprised if it's only 20-30 yrs.  There isn't much innovation in anesthesia.

At least we actually anesthetize babies for surgery now.  There was a time when some people thought babies didn't need anesthesia.  Thankfully, that was before I became an anesthesiologist. What about circumcisions? They only do local anesthetic for that, right? And isn't the process of strapping down the baby very traumatic? I don't know that I'd be a proponent of general anesthesia for this but maybe some sort of light sedation. Thoughts? I'm an anesthesiologist.  For a circumcision in a baby, a penile nerve block (ie. local anesthetic) is sufficient to prevent pain, with a little sugar water by mouth for distraction during the block and to keep baby occupied during the procedure.  At this age, babies are routinely swaddled and no one seems to think that this immobilization is traumatic for them. I'd really like to see an AMA with an Anesthesiologist. The little I know about the field (not a medical industry person myself) indicates there are a lot of hit/miss results when it comes to over/underdosing.

Given the long training period to become qualified I'm guessing the adoption rate of newer methods/agents is slow. There are several on Reddit already. As for what you've heard, it's not quite like that. (I, too, am an anesthesiologist.)

There are several aspects to anesthetics - pain relief, blocking consciousness, muscle relaxation, relief of anxiety. Every patient needs a different combination, but the major drugs used to block consciousness (the major issue for most) are all gases that are metered out and are actually very hard to under/overdose. It's actually pretty simple to do this on a straightforward surgery. We don't get paid what we do for doing the stuff that a computer can do; people talk up the "anesthesia robot" all the time, but that's like saying that the important thing an airplane pilot does is adjust the throttle. I'm not there to adjust the throttle; I'm there for when a flock of geese fly into the engines. And to recognize that taking off at peak-goose-time directly over peak-goose-habitat with goose-attracting-noisemakers is a bad idea.

The next time that someone tells you that they had too much or too little, ask whether they actually met an anesthesiologist (which is to say, a physician (MD/DO) with a residency training in anesthesia certified by the American Board of Anesthesiology or your country's equivalent). A great many procedures are conducted under "conscious sedation" or some equivalent with a nurse administering medications under the direction of the surgeon, gastroenterologist, cardiologist, etc., who is performing the procedure - NOT under anesthesia. Whole different ball game. Curious, would you feel safe sending your own child into general anaesthesia if it was necessary? What if it wasn't life or death necessary? I received general anesthsia when I was two. I'm only an average medical student now. I'm suing because my scores are not good enough to become a child anesthesiologist.    General anesthesia could apply to numerous non-life threatening procedures. My 2 y.o. daughter was diagnosed with sleep apnea because she was snoring. They wanted to remove her adenoids but said that'd require general anesthesia.  

I asked the doctor how harmful her sleep apnea condition was, and ultimately concluded I'd rather not risk the general anesthesia. OK, sure.

But keep in mind that toddlers aren't being put under general anesthesia at random.

What were the effects for kids who didn't get anesthetics, or the procedure that required using them?  Oh yeah: DEATH.  Or AT BEST physical issues that make developmental delays look like a blessing. My eldest daughter was knocked out 3 times for ear canal tubes (albeit for 5 - 10 mins each time). She's a bright kid, so glad to see this isn't ALWAYS the case Uhm... I don't have numbers in front of me but I'd bet the farm that the #1 reason kids under 4 get general anesthesia is for insertion of tympanostomy tubes. Consider that the indications for tubes are typically to correct language comprehension delay (due to plugged ears) and that makes sense. IQ tests have verbal comprehension components. Decreased posterior brain stimulation in formative years (again, due to decreased stimulation from the plugged ears) could lead to the decreased cell mass.

Chicken or egg? study made on 106 kids? Not quite reliable, I bet if you redo the same experiment right now with 106 different kids you'll get completely different results... this would be heart breaking if its true...


My 2 year old just had surgery about 6 weeks ago for a cyst in his throat. The only treatment was surgery to cut it out...



man i hate coming across stuff like this on the internet. =/ Decreased IQ is better than death. I'd assume anytime a very young child undergoes surgery it's not something trivial.  I know this is only anecdotal, so not the best reassurance, but I had multiple surgeries under general anesthesia before I was 4 and nothing would indicate that I have low IQ.

As everyone else has said, this isn't the best study so don't worry. Your child will be fine! Me too! General anesthetic is not usually optional or for kicks. Usually children are only given anesthetic if they really need it so it isn't something you can opt out of to avoid a potential side effect. The benefits far outweigh the negatives. I do think surgery and hospital stays and invasive procedures and pain can be traumatic for some kids and that may also be impacting their cognition. My son had surgery at 4 months old last year, and at 10 months old this year. He is meeting and exceeding almost all his developmental goals. I think, and I hope, that our kids will be just fine :)
 I would agree with most criticisms here.  I would not say that this is contributing much to the stock of scientific knowledge.

- The authors are working within a literature in which negative effects of anesthesia have been found on, e.g., rats.  This is not coming out of nowhere.   That is in their favor.  The rest of what follows is not.

-  They correctly worry that, conditional on needing surgery as an infant or young child, "getting anesthesia" may in this case just be picking up general poor health, including potentially poor cognitive function.

- They attempt to control for this by matching to compare control and treated subjects.   However, they match on gender, handedness, socioeconomic status and age.  That's it.  That is, I would be surprised if there are not other, very important dimensions of unobserved heterogeneity distinguishing the treated and control groups here.  The particular covariates they are matching on do not seem likely to pick all confounding heterogeneity.  Actually - this may be the worst aspect of the study, if you ask me.  

- With respect to the multiple hypothesis testing issue, they say that use a "false discovery rate error correction for multiple comparisons with SAS version 9.3."  To me this suggests that they are aware of the issue, which is good, but don't have the deeper statistical understanding necessary to explain what this actually means (though perhaps this is all the editor allowed them to say).  They are plugging their results into a canned package.  That can be fine, but there are several such corrections (in theory - I don't use SAS so I can't say if it includes only one) and they don't even say which one they use.

- The headline-generating figure (their p. 8) in the conclusion is (5-6 IQ point reduction per person) * ($18000 less income per lost IQ point) * (6 million children per year getting anesthesia through surgery) = $540 billion dollars.  This is a wild overestimate of the likely effects in the very worst case scenario.  As they do note, no one performs surgery on one of these kids without reason.  They aren't getting anesthesia because they are getting nose jobs or other plastic surgery.  No correction is made for the likely *tremendous* benefits reaped by performing these surgeries vs. not doing so.  That figure should be completely ignored.

tl;dr - I don't think this is that well designed or executed, particularly in their approach to matching, so I don't think it adds much.  Truly a wild overestimate of the welfare consequences too, to their discredit. &gt; They attempt to control for this by matching to compare control and treated subjects. However, they match on gender, handedness, socioeconomic status and age. That's it. That is, I would be surprised if there are not other, very important dimensions of unobserved heterogeneity distinguishing the treated and control groups here. The particular covariates they are matching on do not seem likely to pick all confounding heterogeneity. Actually - this may be the worst aspect of the study, if you ask me. 

An excellent group to use for future studies would be children undergoing treatment for port wine stains.  It's a simple condition in which capillaries near the skin don't close, so the skin appears very red.  In my state, one doctor handles all these cases.  The recommended treatment starts at 12 months old (or even less), as many as 10 treatments are used, and so by four years old almost all children have completed all treatments.  Each time the child is placed under anesthesia for an hour or two.  Then a pulsed dye laser basically zaps those capillaries.  

These children are generally healthy in every other way.  Port wine stains aren't associated with any kind of nefarious problems, and that would make for a much better apples to apples comparison.   And since a single doctor has done hundreds, if not thousands of these, it would be relatively easy to find a much larger study  population. This doesn't sound like an ideal group for several reasons, the main one being that port wine stains *are* associated with other abnormalities such as Sturge Weber and  Klippel-Trenaunay syndromes.  It is by definition a vascular malformation and is not always only confined to the skin.

In the UK that would not constitute recommended treatment and seems to me to be unnecessarily invasive at a young age for a cosmetic condition.  This is exactly the kind of use of anaesthesia that should be re-evaluated if a significant impairment in IQ is demonstrated from anaesthetics - perhaps this makes it a good group to study, but it might not be in the interest of the doctor performing the laser treatment! I hed sugary at six monthes n im finne duddes.

No, I really did.  Hernia surgery.  Grew up fine, smart and well-spoken. Lots of kids get dental work under general anesthesia in that range.  I wonder if there was a way to just separate out those kids. who is the control group for this study...  not like they can put healthy kids under.  Perhaps the need for anesthesia under age 4 correlates to other medical problems? Probably better off with diminished IQ than whatever else was so bad that they needed surgery before 4 years.  As someone who had to undergo 20+ surgeries through my childhood, this study is definitely disconcerting, especially because I consider myself to be a sharp individual mentally. Never had an issue with such things like AP exams, but on standardized tests I always performed more poorly when statistically compared to my level of intelligence/IQ. The two don't really match. I am studying Computer Science in college currently, and I do derp out quite a lot. I wonder how much this is to blame... Does the study take into account the event that was serious enough to have required being administered general anesthesia?  Be very careful with this OP. You have not accounted for what surgery was needed, was it life saving or not. This argument as a student is bordering on fear rather than convern They also had a reason to have surgery before age 4, so obviously something else was wrong too.  Association does not equal causation. Ha....I hereby anecdotally dispute this study - I had had general anesthesia 3 times by age 4 (ear tubes)...I'm now 31, and have a master's degree in mechanical engineering and my IQ is 138.

SUCK IT! &gt; I'm now 31, and have a master's degree in mechanical engineering and my IQ is 138.

I know that statistics is misunderstood by many, but a mechanical engineer with a master's degree and an IQ of 138 should know enough about statistics to not dispute this kind of study because of an individual case. I want to hear a study detailing how many of these children would have even survived if they had not gotten surgery and had not been given anesthesia. These kids weren't getting a nose job, they don't just give kids unnecessary surgery. I'm sure the outcome of the surgery, maybe saving the child's life, was worth the chance of a small decrease in gray matter. Study's like this are annoying, like the vaccination argument, the positives of using the drug or immunization greatly outweigh the negative affects.  [deleted] Thats kind of sad to see such a lack of statistical knowledge being a top voted comment in /r/science

15 IQ points is one standard deviation. Thus 5 points is an effect size of about .33 (according to Cohens standards) thats a small to medium effect, and most definitely significant from a statistical point of view.

In psychology we have whole 'phenomenons' (in social psych mostly) dedicated to effects that are only about .20 &gt;  That's exactly a third of a standard deviation. Not exactly statically significant, especially with a sample size of just 53 people


That's a third of a stdev for one person.  Multiply by sqrt(53) to get the population significance, and you get 2.18 sigma.  So this looks like it satisfies the usual p&lt;0.05 criterion (or it wouldn't have been published).

However, it *is* important in light of the fact that

&gt; Loepke and his research colleagues have published previous studies showing widespread cell death, permanent deletion of neurons and neurocognitive impairment in laboratory rats and mice after exposure to general anesthesia. 

&gt; I hate bad science.

I hate people who shout 'bad science' without justification, and with probably a *worse* knowledge of the underlying facts than the original scientists and reviewers.   If you're gonna criticize peer reviewed science you need to be up at their level. Please don't comment about statistics if you don't understand how sample size affects standard deviation. /r/science is a place where people should only talk about stuff they know very, very well. This is taught in just about every introductory statistics course. &gt; That's exactly a third of a standard deviation. Not exactly statically significant, especially with a sample size of just 53 people

Unfuckingbelievable that this is the top post.

&gt; I hate bad science.

The irony. The population standard deviation for IQ is about 15. We can be a bout 90% sure that the data supports the hypothesis given in the title of the post.

Should I spend time posting details? This is very misleading, because it's not a comparison of people who had surgery *with* anesthesia versus people who had surgery *without* anesthesia.

The real comparison done here is between people who had health complications that needed *surgery* versus healthier people who had no surgery.

 I was responding to the comment's criticism of the sample size. What you bring up is even more important I better read the paper. Sure, but the paper is not claiming that anesthesia causes a loss of IQ. The researchers are looking into whether it does based on previous studies with rats, this is a followup study. Obviously now they need to do more work to find out one way or the other. It could entirely be that the anesthesia is unrelated here, but the authors do not care whether children who have surgery have a drop in IQ, just if there's a correlation with anesthetic use. If the title said 'Anesthesia *causes* a drop in IQ in children under 4' then I'd have an issue, but as it is I think people are reading it and making false assumptions that were not being given to them. Guess he forgot the 1/sqrt(n) factor for the std err.  vs std dev.  normcdf(5,0,15 / sqrt(53))) = 0.9938  =&gt; p=0.0062. A 1 IQ point difference would be statistically significant if the sample size was big enough.  "Statistical significance" does not mean "bigness." If a child has a surgery before the age of four, you can probably bet that it's an important one that affects their life greatly.  Surgery that young can be risky, and usually a doctor wouldn't perform it unless it's absolutely necessary.  I think we can say that the benefits outweigh the risks. They put toddlers under for cavities too.  All the time like it's no big deal.  We found a dentist that would do conscious sedation instead for this reason.  Save the GA for something actually life threatening. Children not receiving anesthesia or surgery when needed before age four die. I'd rather have a lower IQ and be alive, then be dead. Any day. A no brainer.  Can anyone find the primary literature article? The link here is a dead DOI. 

Also this article gives zero of the results. And the sample size is ~100 kids. 50 in each group. Does not talk about methods. Does not talk about criteria, or confidence intervals.  Couldn't this be a fallacy under the **identification problem**? It sounds more likely that kids that have anesthesia before the age of 4 are probably having major medical issues that may hinder their school attendance or their ability to learn as well as other kids the same age. Perhaps it is simply because many children who go to surgery at that age suffer brain injuries? It really depends on the surgery here. A simple hernia surgery is not going to lead to these effects. Is general anaesthesia alone to blame? Doubtful Well, this is awkward. I was put under to have tubes put in my hears. Previous to that surgery, I was deaf. I do have terrible language skills. I always thought it was due to the years of deafness. Does anyone know if children exposed during the birth (through their mothers) were included in the study? I don't mean to be/sound overreactive, but are there ways to test/gauge whether this is true in your own children?

My son has been under GA 3 times since birth. Twice for craniofacial surgery to fix bilateral cleft lip and palate, once to fix an inguinal hernia.

So far, he seems to show no signs of delayed development (he's 3 yrs 4 months old) but some of the speech/linguistic stuff is harder to detect or determine since his mouth is still not properly formed.

This worries somewhat since he has more surgeries ahead. 

Once his baby teeth fall out, he may have to get the front of his upper jaw realigned so his teeth can grow in normally (which they currently aren't). 

He'll also have to get surgery to create/implant a hard palate around age 9 (or something like that) where they will use a bone graft from his leg. My newborn daughter is in the same boat.  She has a cleft palate and needs surgery and tubes.  I'm not thrilled to read this, but it's not like I can just opt to leave her palate the way it is.  Her speech will definitely be impaired that way. So if I was under anesthesia for surgery as a child, is there a way to prove this decreased grey matter density is a direct result of this anesthesia?

Am I totally just going to have to deal with the fact that I could be handicapped, and am left to my own devices? I had surgery for tubes in my ears 2 separate times when I was very young. I am 23 now and actually just last year I graduated college with an engineering degree so I wouldn't say the surgeries negatively affected my ability to learn or anything like that.

I do on the other hand remember having trouble pronouncing certain words as a kid  and I took speech lessons around kindergarten or first grade to help. All i remember is it had to do with words starting with the letter S. It was a short term problem and with the speech lessons I was able to correct the issue pretty quickly. I have no memories of it continuing past first grade or anything. I also have no idea if any of this was actually related to the anesthesia but i figured I share.  

I also remember how miserable those ear infections were... When I was three I went under anesthesia to remove a birthmark. It was showing signs of becoming cancerous. I'm 33 now and now I will blame every shortcoming in my life on this.

I also had a really creepy dream while asleep. I still remember it in detail.

Edit: Grammar (Anesthesia's fault) The cohort of those that need surgery would trend towards those defects. There may not be causation. This comment will probably get buried, but I'm kind of concerned now about myself. I was in a traumatic accident at the age of four and suffered an epidural hematoma, which left in a medically induced a coma for roughly 14 days. I wonder if it'll have any affect on me later in life. Personally, I doubt that you should be worried about the anesthesia being applied to you. I am no physician, but the medical treatment you've got seems to be what had to be done and I guess you would have much more reason for being concerned, if the physicians did not do what they did (you even may have died in that case). Our youngest had to have his tonsils and adenoids removed and tubes in his ears when he was two. He had severe allergies and major ear/throat infections almost every month :(

Now I can feel the guilt as his Mom for having him receive general anesthesia *cries* Interesting read. I've had nearly 30 surgeries in my life, most of them were before the age of 6 or so. I definitely didn't excel in school like my siblings, but made up for it in other areas. I've never had great long term memory, and my mother chalked it up to that. She might have had something going. It seems to go in line with the study here. I didn't do horrible in school, but was always a little under the norm of my classmates. It makes me sick that everything is linked back to monetary value, "Estimated Social Cost", is this really the only way we can relate to these studies?
  My brother had a cyst on his back and was put under anasthesia to surgicaly remove it. A month or so later he developed parkinsons. He was only 29 at the time. He believes, and so do I, that the anasthesia is the cause of his disease.  Ah man, I broke my arm in two places when I was three, requiring surgery for proper recovery. Was trying to join my brother jumping on bed and he bumped me off. Not sure it's related to what this study suggest but I had to be in special speech classes until second grade. Was slower in reading too but eventually caught up to average. Just wonder if i'd struggled less in college simply by not having anesthesia at a young age.  Maybe the ones with those symptoms were more likely to need surgery before the age of 4 I speak 4 languages, have been a C level executive, have a master degree and had general anesthesia before I was 4. Just saying.  Does this include anesthetic delivered to the mother/baby during a c-section?  The discovery that worms from different parts of the world move in specific directions based on the magnetic field is fascinating by itself imo. Agreed! On longer timescales, I wonder what happens when the magnetic pole reverses. Do all the worms get lost for a few generations until they figure it out? It's amazing that there is some kind of hereditary "knowledge" about which way is down. Yeah, this is something I've never understood, how much of behaviour is based on genetic coding, how much 'choice' does a worm have over which direction ot moves?

Scaling up to more complex organisms such as spiders, how does web building pass down the generations despite no 'teaching' mechanism being in place? The behaviour must be hard wired into the spider's genetic code. 

Scaling up again to birds and nest building? 

Scaling up again to mammals, can complex behaviour be genetically imprinted? Wouldn't genetics just be the blueprint for creating a spider brain that is wired to build the web? I'm guessing so, but the coding for the neural structures needs to be as complex as the structures themselves, right?

How much actual data would it take to explain a spider web? Is it an algorithm (put a dot of webbing just so far from your last dot, and keep it this taut) or is it an actual blueprint (you want a web that is fifty strides to either side and that you can see all the edges of)

I feel like it's been someone's job to study this. I want to pick their brain. &gt; I'm guessing so, but the coding for the neural structures needs to be as complex as the structures themselves, right?

Well, I mean, bird flocking has turned out to be governed by fairly simple rules despite appearing complex, so just because the emergent structure is complex doesn't necessarily mean its creation is. Right.

I'm curious (as I'm sure many are) as to how a ruleset in the genome can end up controlling imagination and motor neurons.

I can see now why we study worms and spiders for this... And I know it's beyond my ability to imagine the data held in 2b or 3b nucleotide pairs.

Maybe we could get a computer to figure this out. Generate the absolute simplest ruleset, or database, that makes a standard spider web, based only on the actions needed to be taken to create it. (The spider doesn't know a damn thing about its silk except that food can't get unstuck, and it comes out of its butt -- the spider only cares about when to apply a dot and when to rebuild a section) There's a book called "a new kind of science" by Stephen wolfram. It uses cellular automata to try to explain how many kinds of complexity can arise from simple rules. I understand that it is rather controversial, but someone more knowledgeable would need to explain why. I got this. The idea is that we study experimentally how computation works. He suggests we steady simple programs. That's something that can be written in a few lines of code, explained in a couple of sentences, and illustrated. The thing is that even these basic programs often do things we don't expect, so based on just a simple set of rules that can be explained in a couple sentences the result is a completely unexpected amount of complexity. Another crazy thing is that adding more code usually doesn't change the amount of complexity. 

Oh hugely important clarification here: by "complexity" used in this sense we mean the amount and diversity of possible outcomes. The idea is that all those millions of lines of extra code you find in basically every program we use are there for stability - to cause the program to get to the output that you want. So a game wouldn't necessarily be any more complex than something with 20 lines of code, just much more stable.

Now the thing about simple programs is that we've randomly discovered that they can model things such as basic thermodynamics, ecology, etc. So there's a bit of a debate as to whether we happened to create something capable of modeling these systems or if we have figured out how nature does it. He also makes some huge leaps to get his theories working - for example, he assumes that every program that can't be reduced to a simple program has about the same level of complexity, but we really don't have any idea about that AFAIK.

He also flat out said that the beauty of his theory is that it proves that the human mind is nothing special, just the result of complex interactions between rules, which is obviously going to spark controversy among different groups.

The takeaway, anyway, as far as the idea of modelling a spider's web goes, is that we don't even understand on the level you're talking about how basic computer programs that we write work, let alone how a brain comes about from 4 nucleotides. &gt; human mind is nothing special, just the result of complex interactions between rules, which is obviously going to spark controversy among different groups.

Is that not obvious at this point?  I don't understand why it's still controversial.   someone please explain why! And to really torque your noodle, how do they know to put the web in a good spot? Near a light, or in a open path a flying insect may come across? How do they know to build vertical and not in any other orientation? So many questions.... I read somewhere that most spiders dont travel far from their place of birth because it ist too energy expensive. They just kinda start building near the spot where they're born. If this place happens to suck they are in bad luck and eventually starve. But keep in mind that a spider can live for a very long time before starving, so their chances of survival aren't that bad.

If a generation of spiders is in a lucrative spot i imagine they have enough energy to give birth to more generations of spiders and might lure males more often. Can someone maybe comment on that?

This may be the reason there are more spiders in your shed or near illuminated areas than, lets say, the top of a tree 


Edit: I recall one type of spider which lets itself carry away with the wind, while hanging at a silk thread. Sometimes they get even picked up by strong winds and get sucked up by thunderclouds into the stratosphere.  When they land after their long and far travel, they wake up and start building their web. This way they [invade isolated islands and mountaintops](https://en.wikipedia.org/wiki/Ballooning_(spider)) I would think that would lead to large groups of localized spiders.  They certainly travel throughout a house, readily moving from room to room, so that "home" territory would have to be rather large compared to their size.  And I'll suddenly have a large spider web near the porch light where I've never seen a web in many years.  I think there's more seeking behavior there.   In my opinion the answer to all these question is pretty simple- Survival of the Fittest. No animal starts with "basic knowledge pack". That's why some animals are born in much greater numbers than others- to balance the further existence of a species. Animals who have better ways to "transfer" their experience to their children give birth to only one child (like humans). The others lay up to 1500 eggs (like spiders). [Of course there is also the "descendants protection factor" or whatever the scientific term for it is. A lot of these eggs will be eaten, smashed or just won't be hatched.]. All of them have no idea how to weave a web or preserve food for later use, the ones who discover it with tries and mistakes will advance in the next survival step. But in the end even if 1 male and 1 female from 1500 get enough experience to survive by themselves then the species will continue. The only build-in genetic knowledge in most individuals seems to be the basic instincts for survival and reprodusing. I've seen plenty of disused, spiderless spiderwebs in bad locations. Presumably they don't all get it right.  I'm more puzzled by the web building itself than by the location.  Plenty of animals select specific spots, whether it be dark caves or ground of a certain texture or temperature.  The complex structure building is a little less common. They took a spider to space one time. First try at building a web in 0 G, miserable failure. So the spider took a second crack. It built something somewhat resembling a web. So then it tried again, and the third try was essentially perfect. Saw this on display at the Udvar Hazy Centre in Virginia. So it may not just be a simple instruction set judging from this; the spider rapidly seems to "learn", or at the very least adjust its methods to compensate. Well keep in mind spiders can have a lot of babies which make webs in bad spots and then die. We just notice the good spots because the spider lives to maintain them.  Maybe we can alter the environment in controlled ways to see if and how a spider adapts.  For example, spiders in space: http://www.wired.com/2011/06/space-spiders-action/ You'd probably be interested in reading about cellular automata. Go on... &gt;I'm guessing so, but the coding for the neural structures needs to be as complex as the structures themselves, right?

Nope, complexity arises from a set of *simple* rules. DNA itself is not a blueprint. Let me say that again.  DNA is *not* a blueprint.  It's a *recipe*. There is no symbolic representation of the final structure of the organism. In other words: it's not an animal in miniature. A recipe for a cake can be written in a handful of sentences. Now imagine trying to describe and recreate a cake from a diagram, crumb by crumb- or even molecule by molecule. This is orders of magnitude more complicated than a list of ingredients and directions. 

A spiders web, and really all instinctive behavior,  is similar.  There's no blueprint of a web in a spider's head. It has a set of rules it follows which are, in a sense,  more simple than the final structure itself. (And when I say "rules" that's even a bit of an overstatement)

If this seems unintuitive at first its because symbolic representation comes so easily to us, we can't *not* see the world in symbols. That's why I had to use an analogy of a cake.  But if that simple analogy did it's job, it should lead to a much more complex shift in your behavior and how you see the world (; This makes me think that genetic manipulation... i.e curing disease... might be easier than expected. Since it wouldn't do much good to go snipping pieces of genetic code (except for in obvious genetic disorders), the more productive route would be through epigenetics and finding pathways to control stress, inflammation, endocrine function, neurotransmitters, etc. that makes it harder than expected, not easier. gene editing is easy, epigenetic editing is VERY difficult because nongenomic variants arise from combinatorial signal cascade networks, often transiently Do spider webs get better with practice? That is a really good question. I would love to know if anyone has studied this.  Yes but can a spider practice something in our sense of the word? Wouldn't that require something like foresight, planning, memory recall, etc. Come to think of it, do spiders even have memories?  [Some spiders do have all of those things](https://en.wikipedia.org/wiki/Portia_(genus\)). It think you can find a youtube video of them (portia fimbriata, I think) seeing their pray, deciding which of two routes are the best, going down one and *while they cannot see their prey*, discover that it is the wrong one, go back and take the other. This would take foresight, planning and memory recall to pull of. The coding does not have to be as complex as the structure, no. Fractals are a simple example of something that is very complex (in fact infinitely so) but can be created using a very simple set of instructions.  No, see the Mandelbrot set as an example.  Relatively simple algorithm, but the structure is infinitely complex. This is kind of a question best answered using the concept of Kolmogerov complexity. Some computationalist somewhere has probably looked at this problem or a similar one I'd like to see a programmer make a simple robot that can thread yarn into a web, using the absolute least data and power possible. I'd say it's the same as how babies know to be infuriatingly annoying to survive (i.e. crying) or how people know that running away from something means you're likely to avoid it. Yes but I think they must have some intelligence to navigate through the world, to send up parachutes to target anchor points, to hunt and and attack prey  My guess is that there is a lot more intelligence in most creatures than what we humans have ever realized. Re: mammals - a dog knows what to do when she has puppies, even if she's never been taught the birthing/mothering process. Blind humans smile without being taught. There's plenty of hard coded genetic info! I don't really like thinking this way, because it removes free will, but I feel everything is predetermined.

Everything in existence is a reaction from stimulus created by another reaction.

If you know every variable and how they interact, you could predict how many children someone would have 3000 years from now. It's obviously so many variables that we could never know all the values and hence never accurately predict the future, but it shows free will is just an illusion, a result of one massive chemical reaction.

If all variables remain constant, every time a beam of light hits your eye from the same angle, the exact same result will occur. It's all predetermined I wouldn't say it's predetermined since the universe is not strictly deterministic - but probabilistic (from quantum mechanics), so if you knew the state of every particle at one instant (even though the uncertainty principle forbids this), then the possible futures from that point would be infinite.
This still doesn't give any more leeway for ultimate free will however - since we have no control over quantum fluctuations. 
That said, I wouldn't call free will an illusion (though it depends on your definition of an illusion), just as I wouldn't call colours an illusion - it's just as real as it feels it is. You could look into 'compatibilism' for further explanation  I think there was a machine in the Pendragon series of young adult books that looked at the world kind of like that. It contained an absurd amount of data and as a result could simulate a changed event in history and see the domino effect it would cause. I'm not sure if it also could tell the future or if i actually read that in those books at all...  That's correct. It was the library computer somewhere in New York on 3rd Earth. I always thought it was kind of unrealistic to assume that by the year 5100ish (to the best of my memory) we'd have a machine that could calculate every variable and predict the past future. Eh, young adult fiction, right? Still love that series though. Dr. Robert Sapolsky would be the first to tell you that we don't really have choice.
Here is his talk: https://youtu.be/Cx8xEUYrb74 He also has some lectures on youtube and some published by The Teaching Company IIRC.  Partially. But imagine genetics as computer hardware that can rewire itself as necessary. So yes there are physical limitations, but there is also adaptation. Brains are pretty plastic! Genomic expression is pretty varied (epigenetics)! And, as I think you were maybe alluding to, to what extent does genetically imprinted behaviour actually affect humans &amp; our anthropocentric view of free will? Would it possibly result in feeling pleasure moving in the "best" direction?  &gt;Most reversals are estimated to take between 1,000 and 10,000 years.

https://en.wikipedia.org/wiki/Geomagnetic_reversal

This is over a long enough time that the worms wouldn't even notice. &gt;I wonder what happens when the magnetic pole reverses. Do all the worms get lost for a few generations until they figure it out?

Probably not. These worms probably do not do any navigation by absolute orientation, but just use it for relative orientation. i.e. They do not care if 'north' is actually North, just if 'north' consistently points in the same direction. Pole changes are very gradual except on geological timescales, so the 'moving' geomagnetic field is effectively stationary. Pole flips take thousands of years to complete.  I imagine the worms would slowly adapt/evolve to compensate for the new magnetic field.

Source: This is my area of study. I wonder if there's been an extinction event based on pole reversal.  Suprisingly, no genetic bottleneck exists in paleohistory that correlates to geological evidence of pole magnetic reversal. Pole reversals happen all the time (geologically speaking) and don't really leave much of an impact on the fossil record. magnetic poles gradually change over thousands of years, the world doesn't just spontaneously flip it's magnetic axis. Could happen for a hundred generations and the worms would only be a few miles off course by the time they evolve a corrected version. These guys migrate *vertically* based on magnetic field.  Finding yourself "a few miles off course" going up or down is "problematic". A few *miles?*

My god! They'd be... *spaaaaaaaaaace woooooooooooorrrrrrrrrrrrrrrrrrms!* I guess experiments would have to be conducted to test this (or I would have to research if it has already been done), but perhaps the coding for how to move in relation to the magnetic field is *not* genetic, only the initial "programming" is.  In other words, perhaps when the worms are immature, they "learn" or perform an "initial calibration", like *this way* (relative to the magnetic field I sense) is food, and *that way* is air (or whatever).  Once that initial programming is done, if you transplant the worm to another side of the world, it can't relearn.  Or something. If their brains are anything like ours, they'll adapt to the change without issues. Similarly, if an adult has their vision flipped upside down (by a lens), [it can take less than two weeks to adjust before that vision seems normal.](http://www.theguardian.com/education/2012/nov/12/improbable-research-seeing-upside-down) &gt; For instance, Australian worms moved upward in tubes.

The hilarity of this is not lost on me. When they unexpectedly emerge on the surface they must think "blimy! We've gone too far! We're gone right the way through!" And they wouldn't be wrong, if they discovered they were in a laboratory in Texas. The only time this joke has ever made sense! I'm a little confused by this - 

&gt;For instance, Australian worms moved upward in tubes. The magnetic field's orientation varies from spot to spot on Earth, and each worm's magnetic field sensor system is finely tuned to its local environment, allowing it to tell up from down.

I'm surprised that worms don't simply move against gravity? I'd have imagined that magnetic sensing was used for directional pathfinding, not for geotropism? Can anyone chime in on that? Perhaps the weight is too low and the force from all sides on the surrounding soil means a the sensitivity needed is too high so they chose another sense. Geotropism is something plants use on a cellular level. Worms are multicellular organisms, so I'm a bit surprised they don't also possess some cellular mechanism. Shrug. I guess they don't! That's likely a good example of how the undirected nature of evolution misses "obvious" solutions.

Plants use statoliths, a specialized form of amyloplast that exist within statocyte cells the at stem and root tips, which are denser than cytoplasm and "settle" to the bottom of the cell.  Animal cells don't have amyloplasts at all, and I suspect that this settling process is too slow to be of much use in an animal cell analog.  Some aquatic invertebrates have a multi-cellular analog called a statocyst, though, in which a mineralised mass stimulates sensory hairs within a roughly spherical cavity.
 But as I mentioned, Drosophila display geotropism.  Gravity directionality sensing is something that certainly exists in animals, which is why I'm surprised that worms seem to rely on magnetism to do so instead of spatial orientation/information. Well, even if you have 2 mechanisms:  
If they contradict each other, what do you do? [I'm not sure, but it's worth pointing out sensory conflict is something that happens and may explain motion sickness.](http://www.ncbi.nlm.nih.gov/pubmed/2178753) Also, maybe the worms use their sense of direction for other things as well?  To turn around, for example, or to keep from going in circles. While I have not read the paper this portion of the article seems to imply that the worms do use the magnetic sensing for up-down orietation:

&gt; [The researchers] also showed that worms which were genetically engineered to have a broken AFD neuron [used for magnetic sensing] did not orient themselves up and down as do normal worms.

I think humans sense the direction of gravity by feeling pressure via sense of touch. If I were a worm underground I imagine there would be pressure all around because of loose soil, making the sense of pressure an unreliable measure of gravitational direction. Yes.  I think people get pretty disoriented in low visibility water despite gravity still being present.  Things would probably be similar for a worm underground.   in this [Fresh Air interview (extreme medicine)](http://www.npr.org/2014/02/10/274799339/practicing-extreme-medicine-from-deep-sea-to-outer-space) the interviewee says the procedure for figuring out which way is *up* when you escape (at night) from a submerged helicopter is

&gt; pick a direction and swim

because in the absence of visual cues (e.g. light to see which direction bubbles go) you have no idea which way is up.

If humans, with their inner ear, have this problem it does not surprise me that worms need to sense a magnetic field to figure this out. I think the best way is to release a few bubbles and feel which way they travel.   Similarly, when caught in an avalanche, one often loses their sense of direction, even without the buoyancy of water. The recommended procedure for determining the direction to the surface is to spit and watch the direction it travels. In the paper they discus a control. They applied a reverse artificial field pointing up against gravity and found that the worms followed that. I believe their findings, I'm just expressing my surprise that this is the method of up/down sensing they employ. I found it really interesting that they had their own local idea of up and down. So the Australian worms (when transplanted to the other side if the world) would dig up instead of down to find food. Interesting and surprising too. I would have assumed that any organism relies on gravity to orient up-down, and that the magnetic field comes into play only for lateral orientation. Presumably when you're a worm, gravity is very hard to discern, because the pressure of overlying dirt on your upper surface and the pressure of your lower surface on the underlying dirt are very much the same. We perceive gravity as the thing which keeps our feet on the ground, but to a worm gravity is what keeps the soil sticking together, and has no obvious directionality. Not quite so, we have something in our inner ears that helps with that. Usually it's to detect acceleration (an accelerometer is based on the same design) but it works ok to detect gravity as well provided you're not spinning around. It's also not that accurate but combined with visual information it creates a pretty clear picture

Edit: Your vestibular (inner ear) system has nothing to do with gravity, only acceleration. The sense which determines gravity is based on nerves in your skin, muscles and joints and is called the somatosensory system, essentially feeling where the most pressure is and relaying that information back to your brain.

Edit2: it turns out that it is a bit of both.  Your vestibular (inner ear) system has nothing to do with gravity, only acceleration. The sense which determines gravity is based on nerves in your skin, muscles and joints and is called the somatosensory system, essentially feeling where the most pressure is and relaying that information back to your brain. [deleted] [deleted] [deleted] [deleted] Welcome to the body of a being that can so lots of things but nothing good. I'm eagerly awaiting cyber technology so that I can finally see infra red and ultra violet. And all the other cool things you can do with optics that our eyes can't. Do you think your brain will squish the infra red and ultra violet into the ends of the normal colour spectrum, or invent new colours for the extremes? I think that's mostly up to people. Different cultures through history didn't have names for some of the colors we have today. I don't remember the exact examples, but lets say one of those colors was orange. These people back then could physically see orange, but they just considered it a shade of red. I think I remember reading about a tribe somewhere that actually has MORE colors than we do. I'll see if I can find an article. 

Edit: I think this was what I was referring too. So not exactly more, but different context. 
https://m.reddit.com/r/todayilearned/comments/u9usi/til_there_is_a_tribe_in_africa_who_break_the/ I don't think that's how it worked, the language simply didn't have words for many colors they saw. It's part of the reason some ancient prose is so illustrative. If the word fere means any yellow/red color, then in text one would need to say "her hair was fere as the sun" to distinguish that it meant yellow, or "as an apple" to show red. Actually there is a physical thing that humans can do better than any animal. It's long distance running! Lots of animals can run faster than humans but none can keep up a average speed as high as a human over long distances. Source: http://www.slate.com/articles/sports/sports_nut/2012/06/long_distance_running_and_evolution_why_humans_can_outrun_horses_but_can_t_jump_higher_than_cats_.1.html Also communicate. No other animal can express the same kind of detail to one another that we can. If you like that then you'd like the [2014 Ig Nobel Prize winner for Biology](http://www.improbable.com/ig/winners/) (scroll down): a Czech, German, and Zambian team who show that when dogs defecate and urinate, they prefer to align their body axis with Earth's north-south geomagnetic field lines. Plug for the other Ig Nobel Prize winners - - check them out! A similar study used Google Earth images to study cow grazing alignment: http://www.nature.com/news/the-mystery-of-the-magnetic-cows-1.9350 Without having read into this, couldn't this have something to do with preferring to have the sun on their side than in their face, or at their back? Maybe the alignment was consistent regardless of the weather.  But Google filters out cloudy pictures so you only get to see cows grazing on sunny days. That could skew the study in some kind too of course. That sounds amazing haha. I can't get the link to load though - how 'reliable' actually are they at this? Like say I was lost with a dog... There's this thing called the sun that you may find produces fairly more reliable results. so wait we already knew some had this.. does this just mean we know "what part does it" now?  Yeah this is what I'm wondering! We already knew butterflies and birds can sense the Earth's magnetic fields to orient themselves... We know they can, but we don't how they do it i.e. we can't find the organ or section of an organ that enables the sense. I thought [this](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3552369/) was how birds do it. (Sense magnetic fields that is.) I hope your comment is seen, because the review you linked nicely summarizes a long history of finding magnetically active structures in a wide variety of organisms, from the flagella of bacteria to the beaks of migratory birds.  While the findings in OP's article are definitely cool, the headline makes it seem more novel than it really is.

EDIT: Reading the author's comments, I'm wondering if the novelty is that they found a *neuron* which possessed an intrinsic mechanism for sensing magnetic fields.  That would be a bit different than having an iron accumulation embedded in tissue that stimulated nearby neurons ... kind of. I am far more interested in electromagnetism sensitivity in humans. The World Health Organization regards electromagnetic hypersensitivity as a real problem, and currently only one country in the world treats people for it (Sweden). The most important study of it was conducted in England, and used self-reporting, so the results were wildly skewed against electrosensitivity being a thing.

We need to study humans. It is **imperative** in our world full of electronics.  Couldn't you also argue that self-reporting would have skewed the results in favor of electrosensitivity being a thing? I thought I read about three or four years ago they found some cells in bird's eyes that react to magnetism. I looked it up, [here's one article.](http://blogs.discovermagazine.com/notrocketscience/2010/07/08/robins-can-literally-see-magnetic-fields-but-only-if-their-vision-is-sharp/)  The article says that they found that vision was linked to the ability to use the magnetic sensor info not that the eye is doing the actual sensing. "We found that blindfolded birds kept slamming into things, supporting our hypothesis that their magnetic navigational abilities are connected to these eye cells." I think they were able to train birds to fly a path blindfolded, but they can't in a variable magnetic field.

That's without reading it. Haha sounds like something the onion would write.  If you read the OP link you could see the difference But I thought we knew the exact mechanism in sharks (ampullae of lorenzini)?  &gt; ampullae of lorenzini

This senses the bio electro fields from other animals, there is some speculation that it can sense fields caused by ocean currents affected by the earth magnetic field but I don't think it can directly sense the Earths magnetic field. We didn't actually know they can. We THOUGHT they can. Navigation in animals was thought to be due to movement with magnetic field, but due to lack of evidence was often refuted. In case of pigeons, for example, navigation is thought to be a combination of many factors, including memory, magnetic field, genetic imprinting, nav using landmarks such as cloud caps etc Didn't we know cows too because they always eat pointing north or something? Dogs do it too, for pooing I believe They identified a neuron specifically involved in magnetic sensing. So, yes. &gt; "It's been a competitive race to find the first magnetosensory neuron," said Pierce-Shimomura. "And we think we've won with worms, which is a big surprise because no one suspected that worms could sense the Earth's magnetic field."

They think so. Cool! wonder if birds sense of direction worsk the same way, or very similar..  Birds just absorb the worms' powers. How do creatures evolve to get this power? Yeah that's pretty much it. Many animals have been found to be aware of magnetic fields, but we didn't know where this awareness actually came from physically.

Particularly interesting to me is the fact that dogs have been observed to poop in alignment with the earth's magnetic field during stable electromagnetic conditions (only a couple hours a day). That's the first evidence of a biological dipping compass in mammals, which I think is super cool. What do you mean by "stable electromagnetic conditions"? I don't know enough about the geophysics of it all to give you a full explanation, but the earth's magnetic field is in a fairly constant state of flux (haha flux, get it? Magnet puns...) which is ironic in that I mean the magnetic flux is not constant. Our magnetic field is generated by slow currents of molten iron in the earth's core, which is a rather unsteady process. As a result, the field is inherently unsteady. Combine that with the fact that cosmic radiation "blows" the field around and you end up with something that is rarely at steady state. For about 2 hours a day (I think, but I might be off on that number) the field is actually quite steady, and during that time dogs are capable of detecting it. Other animals such as birds are either more sensitive or better able to compensate for unsteady conditions, so they are always able to align themselves relative to the field.

I hope that does a slightly decent job of answering your question. I think I just might use some of the electronics I have lying around to log the magnetic field and try to detect that. MinSegMega (Arduino Mega derivative for Segway-style robots, which happens to have an HMC5883L magnetometer/compass chip) plus SparkFun OpenLog should do the trick.

cc /u/Toraeus It does. Do you know if that steady state is predictable, or is it just at random times that happen to be calm? I am afraid I have no idea. I would imagine if you had equipment capable of measuring core currents and you combined that data with readings of solar radiation and ejecta in some sort of extremely fancy computer model, you could probably make some pretty good predictions. I dunno if that's remotely possible with current technology, but again I am just guessing anyway. This might get burrowed but still: My group is one of the few that does research on a related topic (also spin chemistry). As many of you stated: It is already known that birds navigate with a cryptochrome (a protein in the eye) and their tip (its ferromagnetic) and the mechanism behind it is also known. The mechanism is called the "radical pair mechanism" (RPM) and it involves a photochemical triggered reaction that creates radical pairs (unpaired electrons). These electrons interact with the surrounding nuclei of the atoms and form so called triplet and singlet states. This states "depend" on the magnetic field (not super accurate, but you get the point). These states also create polarization on the nuclear spins, which then can and will control chemical reactions. The theory behind seems well understood. It was first stated by Kaptein in 1969. But now there is some kind of discussion around it, since it seems that the mechanism behind the RPM is different to what we thought it was. It still involves triplet and singlet states but the creation of polarization need to be described in a different way. 

What is interesting is, that this mechanism not only applies in birds (and also insect or may also be involved in our sense of smell) for navigation, it also seems to work in photosynthesis! This is actually also my research topic and the one of the group I'm working in. It is super interesting how nuclear spins with next to no energy can and will control complex chemical reactions and therefore lead to navigation or such highly efficient processes as photosynthesis.

If you are interested in the bird-navigation topic check Peter Hores research on it, if you are more interested in the RPM in photosynthesis I encourage you to check my boss' research (Jrg Matysik) or just ask me right away.

edit: as stated below a review was missing. So here you go for more information: http://www.mdpi.com/2079-6374/4/3/221/htm (open access!) Could you tell us a little about what happens when the Earth's magnetic poles flip? What would keep the birds from getting confused and flying North instead of South for instance? I'm not totaly sure what would happen if the Earth's magnetic poles flip. The thing is that the mechanism itself contains variables that are field dependent and by itself have an orientation dependence, meaning they depend on the molecular structure and where in the molecule the radical pair is located. So on the first look it doesn't matter if the Earth's magnetic poles flip since the magnetic field is vectorial, that means it points in one direction. The mechanism itself doesn't care if it points up or down, but it is important how strong it is (locally). ~~Now if the Earth's magnetic poles would flip, I can imagine that also the local distribution of the magnetic field change, which means it gets weaker/stronger in some positions of the earth. This could lead therefore to birds not flying north instead of south but maybe west or east or any direction, since (as far as some people think, it is not fully understood so far)~~ (I was wrong here, see the edit) they know they arrived in there position according to the strength of the local magnetic field which they might feel with their tip (as stated: it is not really understood, it could be different). 

Interesting side fact that comes to my mind: If we are right and photosynthesis also depends on the magnetic field, it could also happen that plans (and other organisms that do photosynthesis as algae and some bacteria) change their structure. Some people did some research on how plants grow in a magnetic field and showed that they grow differently (mostly orientation I think) depending on the direction of an applied magnetic field. But I would need to look this up, I don't remember too much about it.

Edit: I have to correct what I said: I talked to my boss and to another expert on the field and they said it doesn't matter if the magnetic field is reversed or not, because the bird itself can't differentiate between it. It only depends on the angular orientation of the magnetic field relative to the bird and this would not change if you would flip the magnetic field. Also they would also use the stars and other things to navigate. That's very interesting, thanks! I was wondering, do you know why this might happen:

&gt;Depending on where they were fromHawaii, England or Australia, for examplethey moved at a precise angle to the magnetic field that would have corresponded to down if they had been back home.

If they had a magnetic field sensor, wouldn't they able to figure out which direction was down if they were moved?  This makes it sound like the sensor figures out which direction is down once, and then does nothing.   I don't know the answer I can only give a guess: 
They might have developed some kind of "fixed" system that works best at an certain angle. In the case of RPM there is a variable that is dependend on the orientation of the magnetic field (it is the g-tensor which is anisotropic). That would imply that if your system is fixed in a special angle that produces the maximum signal at the angle that correspond to "down" in a special place (Australia, Hawaii, England), the worm would orient in that way, that it gets a maximum "signal". 

A test would be to let the worms repoduce and see how the "new" worm would behave corrensponding on where it grew up or was born (idk about worm-reproduction). You would see if it is really dependent on evolution or if it developes according to the magnetic field of the place of birth. I thought the radical pair grouping was just noticed to be present with cytochrome. I didn't think it was ever officially pegged down as a mechanism yet. Would you mind pointing to a paper where it has been shown to be necessary for magnetic orientation in animals where it occurs?  article:

http://elifesciences.org/content/early/2015/06/17/eLife.07493 Thank you! I didn't find the media piece satisfying. Not trolling, I really want to know: I thought it was known that other animals (pigeons, whales, etc) have this already. Is that not the case? This is about how, not whether.  Scientists have been finding magnetically active structures in organisms since the [1970's](http://www.sciencemag.org/content/190/4212/377.short), and more recently have [identified magnetites in the beaks of birds](http://link.springer.com/article/10.1023/A:1009214526685).  It sounds like a finely-graded spectrum of scientific semantics to me:

&gt; "It's been a competitive race to find the first **magnetosensory neuron**," said Pierce-Shimomura. "And we think we've won with worms, which is a big surprise because no one suspected that worms could sense the Earth's magnetic field."

So, we've found structures that detect magnetism.  Perhaps the novelty here is that they've found neurons which themselves have the ability to detect, transduce, *and* relay information about the earth's magnetic field? from the article they said, paraphrasing, that now that they have found where it is in the worm they are going to look back at other animals to see if they can find them. As of right now they have predicted that animals can sense the magnetic field. Predicted being the key word. "Chances are that the same molecules will be used by cuter animals like butterflies and birds," said Jon Pierce-Shimomura.

'Maybe then we can get some damn funding.' He continued bitterly. Love this quote. TIL: "cuter" is a term used by zoologists.  I thought this was hilarious, also considering the context might have been some super serious paper or interview. I also imagine Jon Pierce-Shimomura is a large man who loves butterflies and birds and spends his free time making doodles of them with hearts and flowers. So do they know if this evolved independently in each species. Or is it something like common ancestor and each of them kept it and developed it as they evolved into different species.

What I'm really asking is do people have them. I don't know but I've read people can train themselves to orient to North via a ankle bracelet that has a compass and a vibrator in it and whenever you are oriented north it will vibrate. After a certain amount of time they can take it off and orient north. What?  Tell me more  I've read about the human ability to assimilate input. Basically if you give someone a new sensory input, like a belt that always vibrates the part that faces north, then in a short time they no longer feel the vibration but can orient naturally that way, without having to think about where the vibrations are coming from etc. Similar studies have flipped people's sight and given 360 degree sight, in all cases people soon adapt. I've never heard of them retaining a new sense after losing it though, they are generally confused and need to readapt to normal for about the same time it took for them to assimilate the new input. Maybe OP's memory is a bit fuzzy, either that or he's talking about something I haven't heard and would be interested in seeing. Links to 360 sight studies (or any further info at all) please! That's something I've wanted to experiment with for a long timehaving some foundation to start from would be great. [deleted] It's extremely far back in evolutionary history. Worms were one of the first multicellular organisms to appear after single cellular organisms. I'm not sure that there is a "too far". We share a bunch of DNA even with worms. &gt; What I'm really asking is do people have them

[There's some evidence](https://en.wikipedia.org/wiki/Magnetoception#In_humans) that humans can sense magnetic fields.  This is the kind of stuff that should be on the front page of world news, not murders and stock market. &gt; The sensor, found in worms called C. elegans, is a microscopic structure at the end of a neuron that other animals probably share, given similarities in brain structure across species.

Can't believe we still learning something new from *C. elegans*. It shows how little we know. Hey! Shout out to all the other nematologists! "Chances are that the same molecules will be used by cuter animals like butterflies and birds," said Jon Pierce-Shimomura, assistant professor of neuroscience in the College of Natural Sciences
...wow Clearly he understands how research funding works. [deleted] Uhm, don't these two statements contradict each other or am I just a bit too stupid to properly understand science?

&gt; When the researchers brought worms into the lab from other parts of the world, the worms didn't all move down. Depending on where they were fromHawaii, England or Australia, for examplethey moved at a precise angle to the magnetic field that would have corresponded to down if they had been back home. For instance, Australian worms moved upward in tubes. The magnetic field's orientation varies from spot to spot on Earth, and each worm's magnetic field sensor system is finely tuned to its local environment, allowing it to tell up from down.

So their movement in the lab corresponds to what would be downwards on their hemisphere of the globe. The specimen from Australia moved upwards in its tube, since that would correspond to downwards in Australia. Its behaviour did not change, albeit the orientation of the magnetic field in Texas differs from the orientation in Australia.

Then we have another statement:

&gt;The researchers discovered the worms' magnetosensory abilities by altering the magnetic field around them with a special magnetic coil system and then observing changes in behavior.&gt;

This time the behaviour changes, induced by the manipulation of the orientation of the local magnetic field.

Could someone shed some light on this for me? I think you confuse two different things. I don't know if these are actual science terms, but I think they illustrate the problem.

You are mixing up internal and external behaviour.

Internal behaviour, is the ruleset the worm follows internally. In this case "Dig down.". This behaviour does not change.

External behaviour, is the behaviour we observe from the outside. Which is, that the worms dig in a certain direction. When we mess with the magnetic field, this direction changes, therefore we observe a change in behaviour.

So internally, the worms all follow the same rules, but because we can change which direction they percieve as "down", we can cause a change in their external behaviour, despite them still following the same rules. &gt;"Chances are that the same molecules will be used by cuter animals like butterflies and birds," said Jon Pierce-Shimomura,

This is hilarious. I just think we should be informed about what measure they're using to assess cuteness. Are they using proportional measurements to quantify neotenic characteristics, etc., or are they using human-culture indicators such as representation in global children's sticker production? In fairness, C. elegans is fun to watch but not cute. 1 mm long, so you can only look at it under a scope, no eyes, can't really discern anything except its reproductive organs. Don't underestimate the power of charisma in your study organism-- my friend studied butterflies and dung beetles and had funding out the wazoo despite not being able to be rigorous with her experimental questions It should probably be noted that these are C. elegans nematode worms, not earthworms (common assumption). These guys make their living (we think) on rotting vegetation and fruit; closely related species live on insects. They are about 1 mm long and have a generation time of 3 days. They only have 302 neurons and both the development and the connectivity of all neurons is known. This is not the first example of a sensory organ being discovered for the purpose of following the magnetic field.

Magnetic response has been documented many times throughout history. Magnetite has been found the abdomens of bees, and the brains of birds and bats to serve as a magnetic compass. Some fish also possess a sensitive sensory organ to detect magnetic fluctuations.

Wiltschko W, Wiltschko R 2005 Magnetic orientation and magnetoreception in birds and other animals. J. Comp. Physiol. A. 191, 675693. doi:10.1007/s00359-005-0627-7. This is the first magnetosensory nueron as was pointed out above. I always wondered if humans have the ability to sense magnetic fields. I suck at maintaining a sense of direction at night but some people have few issues navigating. Could be just a difference of experience in driving at night or knowledge of an area. Who knows. 

Also some neighborhoods don't give me as "good" a feeling as other neighborhoods, and it's nothing to do with the quality of the area or perceived crime risk, it's like an internal sense something is off. I've always wondered if the local magnetic field in the area doesn't jibe well with whatever sense I have based on where I live or where I grew up.  First bit - Yea that's a really interesting question, could be such a small area of the brain (because we haven't used it for a long time) that we simply haven't discovered it yet

Second bit - Wut? The second bit is wondering whether humans have a bad mojo detector.  Not so much a mojo detector as much as a magnetic field detector (or something else) that gives a subtle odd feeling if things are different, but not necessarily "bad".  Sometimes magnetic fields can have localized differences from geology so I wonder if that can be sensed by humans.  Not sure about magnetic fields, but we *can* use the polarization of sunlight to orient ourselves. https://en.m.wikipedia.org/wiki/Haidinger%27s_brush
 I'm actually surprised that this hasn't already been discovered. I always assumed this was an accepted thing because it makes so much sense. It's been accepted that some animals can sense the earth's magnetic field, but this is the first time they've identified the sensor specifically.

What I want to see is how the neuron works to detect changes in magnetic field. Interesting how worms, birds and butterflies have this yet, us humans, who are MUCH  more complex lac such a feature.  [serious] wasn't there a thing were research was done that showed that dogs poo pointed north? I read about it, don't remember where the research was done. it was an accidental finding.  I thought that it's already been discovered in certain birds.  I thought there were previous studies that found magnetic sensing organs in pigeons? There was a fairly famous study involving pigeons and magnets done in the 70's where the pigeons became disoriented if it was cloudy and they had magnets glued to them, but not one or the other because they can also use sun position to navigate. It didn't go into the actual mechanism behind it though. this is huge! could humans have "cellular antennae?" [deleted] I thought homing pigeons could do this already? We've known about magnetoreception in many animals for a while, but this is the first actual magnetoreceptive neuron. (HT u/mutatron) Ah, I see the distinction now.  I thought we've known birds do this/identified the structures that allow them to for quite some time... it was a Chinese study in 2012 if my memory serves me correctly &gt;"Chances are that the same molecules will be used by cuter animals like butterflies and birds," said Jon Pierce-Shimomura, assistant professor of neuroscience in the College of Natural Sciences and member of the research team

...but we feel guilty about chopping the cute ones up so we work with worms. Pardon, but isn't this intrinsic in other animals like birds, dogs, and cats? Otherwise how would birds navigate, cats cat, or dogs align themselves with magnetic field to poo? I thought we long ago discovered that foxes were using the magnetic field to hunt mice in the snow?

http://phys.org/news/2011-01-predation-foxes-aided-earth-magnetic.html We've known about magnetoreception in many animals for a while, but this is the first actual magnetoreceptive neuron.  Pretty sure we found this years ago in magnetotactic bacteria.

If memory serves, these are the bacteria that have helped to break the "bacteria internals are disorganized messes generally separated, badly, into genetic material and not genetic material".  They form magnetosomes in precise arrangements that allow them to swim up and down water columns according to the time of day.  Spin one way, swim down to the bottom to feed.  Spin the other, return to nearer the surface.   [deleted] Haven't they known this about homing pigeons for quite a while now?  I thought they already proved that  pigeons, bees and another bird I can't remember, already did this?  Read more than the headline. Its a short article and mentions exactly what you said  Finding the "organ" responsible for this detection has proven very difficult, though, because it can be microscopic.  I remember learning in school that some birds "see" the magnetic field, so they know which direction to fly. Yea, the magnetic fields overlay their vision like a HUD in a fighter pilot's helmet. Microscopic structure addition to brain for magnetic navigation?  How long before the genetic modification is available? When the researchers brought worms into the lab from other parts of the world, the worms didn't all move down. Depending on where they were fromHawaii, England or Australia, for examplethey moved at a precise angle to the magnetic field that would have corresponded to down if they had been back home. For instance, Australian worms moved upward in tubes. The magnetic field's orientation varies from spot to spot on Earth, and each worm's magnetic field sensor system is finely tuned to its local environment, allowing it to tell up from down

Cool!

 Tl;dr what are plasma tubes? Did you ever do the science trick in school where you chuck a lot of iron filings at a bar magnet and they all align in pretty patterns?

Plasma tubes are the same kinda pattern but using gas. 

What's happening is the sun's radiation is ionising the gas in the upper ionosphere (making it conductive with electrons bouncing all over the shop) and the planet acts like a big assed bar magnet.

The way everything self organises into strands like the magnet trick is called a plasma tube cos it's filled with electrically polarised gas.



 [deleted] [deleted] [deleted] [deleted] Thank you! That was a great visualization/example. I think I kinda get it! Here: watch [this](https://www.youtube.com/watch?v=8llkHQtaOlg)

The important thing here is to realise the field lines don't actually exist without a conductor. Mag "field lines" are just something used to illustrate the field shape. (they're basically a teaching aid for kids).

The filings *self arrange* because opposite poles attract and similar poles repel each other. (try getting two bar magnets and set them side by side what happens? They shove each other apart yeah?... The filings become millions of little dinky bar magnets)

The behaviour is a consequence of the properties of the material within the field and not an inherent property of the field itself. It's an emergent phenomena.
 I don't think I agree with this. The directionality and continuity implied by the field line visualization are definitely fundamental to the field, which exists independently of any medium. It's just that you don't "see" this property without a medium like iron filings (or plasma tubes). There are no discrepancies and no lines. It's a continua.

The 'lines' are emergent properties of the conductor caused by the behaviour of the conductor within the field.

You create the behaviour your see. You're not watching a property of the magneic field itself, your watching the natural self organisation of conductors within a field due to induced polarisation.

It's how we show kids a magnetic field but never mistake it for the field itself.  Thank you!  
  
15 years ago, I still remember, having this argument with my science teacher when he told everyone these lines literally exist in the field while showing how the iron dust arranged into strips and lines around the magnet. It made no sense to me and I clearly could see the iron itself forming those patterns and I refused to let the issue go unless he explained the properties of those lines and how they exist. He didn't.  
  
Glad to finally put that memory to rest. I was right! Interesting how nature repeats patterns on different size scales and in different mediums. It's almost like there are laws or something.

What i've always found weird was the way people acknowledge them at one scale and deny them at another cos apparently adding zeros made you special.

 memorization of facts vs understanding and global application. I'm sure you know, but keep in mind that many laws hold true in certain situations, but are not accurate when you start changing multiple orders of magnitude (adding/subtracting those silly zeroes). fractals! I know plasma is an ionized state of matter, but ionized gas is not equivalent to plasma, right? These terms seem to get used interchangeably in the article and the comments here. A fully ionized gas is a plasma.  A partially ionized gas could be thought of as two superposed fluids, a neutral gas plus a plasma "mixed" together.  I put scare quotes on "mixed" because the two fluids typically interact weakly so rather than actually mixing the both occupy the same space but don't collide with one another.  (I'm speaking of the tenuous space regime, not tokamaks or inside stars, where the densities are much higher.)  But weak interaction is not no interaction, and many interesting space phenomena come from the interactions of neutral gas and plasma. Apparently, highly ionized gas particles that have aligned themselves somehow with earth's magnetic field in a manner never before observed. &gt; highly ionized gas particles that have aligned themselves somehow with earth's magnetic field in a manner never before observed

Erm...  That is one of the fundamental properties of ionized gas (plasma) is that it interacts very strongly with magnetic fields and produces "tubes" or otherwise, filaments.  This is due to the incredibly high number of free electrons within a plasma.  The electrons will move along field lines. Makes sense to me. It's just that we haven't seen it before. I'm very curious just how contained the phenomenon is within the "tubes". I'm pretty sure the phenomenon being discussed here are the tubes themselves and not the plasma...

Also, we simply haven't seen them on earth before.  These things exist on the sun...  It's what solar flares are. &gt;it's what solar flares are. 

Thanks for relating "plasma tubez!!" To something I actually kind of am familiar with. Now I see how crazy cool it is that plasma tubes are anode earth.  [deleted] These tangled magnetic fields are in the form of tangled magnetic flux tubes. They reconnect to form un-tangled tubes. Highly-tangled tubes hold more energy in there magnetic field structure, and this re-connection to a simpler state leaves free energy, which drives the flare. Ah yes, Sweet-Parker reconnection. I wish I could actually see the reconnection happening in our magnetosphere in real time. I wonder how large the diffusion region actually is. I know I learned it's on the order of a few km long, but I still have trouble visualizing it.  [deleted] Could we like....do anything useful with these plasma tubes?  Could I stand at one end of the tube and talk to Carl at the other end of the tube? Absolutely!!

He wouldn't hear you, but you could! Probably work out something to predict or minimise satellite communication interference? I thought solar flares were due to spikes in energy. They don't have to be mutually exclusive They are "spikes of energy". The suns magnetic field gets twisted up to the point where it arcs in various shapes and sizes. Depending on how twisted up the mag-field is you can get a solar prominence or a solar flare (higher energy w/ a plasma discharge from the surface). Like its stated above, plasma is conducted along these mag-lines and becomes visible to us. I assume that if the magnetic field was strong enough to separate the plasma into "tubes" then it must be strong enough to contain the plasma within itself It will just be that the conditions are such that there regions that are plasma, and those which are not. The *tube* is just the shape of the region with the properties necessary to create a plasma. How big are these filaments Not only the electrons. Both ions and electrons will moves along with field lines in what is known as gyro-kinetic motion, in which the center of rotation of the electrons and ions follows the lines.

One of the most important characteristics of the plasmas (and one of the definitions of a plasma) is the Debye shielding, which is due to the high degree of ionization and the free movement of electrons. But both ions and electrons follow the field lines.


That is basically the fundamental of any magnetic confinement fusion device. This is the first time they have been measured, but the article states that they have been predicted for 60 years. [deleted] Are these tubes of ionized gas just gas that was already present in those levels of atmosphere except energized, or are they tubes of gas that have risen to higher levels as a result of their ionization?

Also how do these tubes behave differently from the rest of the atmosphere with respect to fluid dynamics (viscosity, density, etc.)? The difference in behavior is that they are effected by magnetic and electric fields. The plasma is likely drawn from our own ionosphere, and just being rerouted through the mag-field. How are they different from the Van Allen belts? Tl;dr what are ionized gas particles? Imagine a neutral gas, hydrogen for example, which consist in a proton and an electron. Now you take away the electron, creating a positive charge (ion) and a negative charge (electron). Your atom is now ionized.

If most of your gas atoms are in this form, it is said to be a plasma, which is a very high ionized gas. Hannes Alfven. Look him up. Learn something. Absolutely fascinating. I love how the equipment was there, the theory was there, but some brilliant person saying "What if we used it this way instead?," and bringing the two together like a key and a lock is what unlocked such a huge discovery.  The way this discovery played out, Cleo sent around her GIF of ionospheric corrections showing these amazing regular structures that nobody expected, moving across the sky.  Keep in mind, she was in a group of astronomers and astrophysicists, not ionospheric specialists.  It took a while to realise that the structures were actually fixed and the 'motion' was only the RA/Dec tracking in the imaging pipeline.  After a few attempts, by several people, to explain it as instrumental, she had to go shopping for someone who knew enough about the ionosphere to explain what she was seeing.

It was awesome work by Cleo.
 Could you post the gif? This [Australian article](http://www.smh.com.au/technology/sci-tech/astronomy/sydney-university-physics-undergraduate-maps-huge-plasma-tubes-in-the-sky-20150601-ghcc9g.html) includes it, I think. Her video explains everything. Video link: [Cosmic cinema: astronomers make real-time, 3D movies of plasma tubes drifting overhead](https://youtu.be/ymZEOihlIdU)

&gt; Published on May 31, 2015

&gt; By creatively using a radio telescope to see in 3D, astronomers have detected the existence of tubular plasma structures in the inner layers of the magnetosphere surrounding the Earth.

&gt; For over 60 years, scientists believed these structures existed but by imaging them for the first time, weve provided visual evidence that they are really there, said Cleo Loi of the ARC Centre of Excellence for All-sky Astrophysics (CAASTRO) at the University of Sydney.

&gt; Ms Loi is the lead author on this research, undertaken as part of her award-winning undergraduate thesis and published in Geophysical Research Letters today. In collaboration with international colleagues, she identified the structures.

&gt; Read more: http://caastro.org/news/2015-tubes

&gt; Credits:  
Video production by Dr Wiebke Ebeling / CAASTRO  
Animations by Mats Bjrklund / Magipics  
Swimming pool by Footage Island  
MWA driftscan and distortion pattern by Cleo Loi  
Aerial drone footage by MWA project / Curtin University   The real story here is she proved them wrong.
&gt;When they first saw the data, many of her senior collaborators thought the results were literally too good to be true and that the observation process had somehow corrupted the findings, but over the next few months, Cleo managed to convince them that they were both real and scientifically interesting.  

Pretty sure some of her colleagues weren't nice with their dismissals. &gt;Pretty sure some of her colleagues weren't nice with their dismissals.

Possible, but there's no reason to assume that.  When unexpected and unexplained effects show up in data there can be honest skepticism within a group.  These things can be some kind of error or unanticipated effect within the apparatus, so the burden of proof is to show that they are real.

In experiments, unexpected things are most often "whoops, this thing is broken or not working as well as we thought", not discoveries.  Thus it is natural to think that something unexpected fits in that category unless given good reason to think otherwise.

My own experiments have had innumerable setbacks and disappointments, so the few times I reported to colleagues that things were working better than expected I was met with *respectful* skepticism until I could prove my case. That's the point, right?  Like, this might read as "underdog sticks it to the MAN!", but really, assuming that everyone maintained their professionalism, it's how it should go... 

"Hey, I think this data means something unexpected!" 

 "I don't believe you, because of these reasons, can you explain them?"  

"hmm... yes!  here's more evidence!" 

 "NEAT!" I don't like how you used the phrase "I don't believe you." I don't think her colleagues thought she was lying or fabricating any data. More so they were skeptical as to the nature of the data, and whether it was due to an inherent issue in the method of collection. There are two possible results from this: the unexpected pattern turns out to be legitimate data and becomes a discovery; or, the pattern turns out to be caused by the apparatus, leading to an improved understanding of said apparatus and the data it provides, possibly leading into improvements to the technology to rectify the unexpected behavior. In the end, both outcomes are good for science.

Here's how I would imagine it sounded:

&gt;"Hey, I think this data means something unexpected!" 

&gt; "These findings are interesting, are you sure it's not due to how the data was collected/presented? *I'm skeptical* because of these reasons. "  

&gt;"hmm...  here's more evidence!" 

&gt; "NEAT!"

 I just reread the initial email chain.  Your imagined conversation is a perfect TL;DR.

I'd love to post it (the email chain), because I think it only reflects well on everybody, but I think that wouldn't be polite without all the writers permissions.  It probably wouldn't be legal I think about this same thing when I read about scientists like Louis Pasteur and the initial skepticism toward their theories. The people who doubted the germ theory of disease (and the necessity of sterilization of surgical instruments) are thought of as small minded for not being able to appreciate the obvious truth. At the time, occam's razor would have led even a good thinker to initially disbelieve something so extraordinary. Errors in measurement, deception, mental illness, at one point in history these explanations were just as plausible as "there are actually invisible little creatures that live on everything and that is what makes us sick." 

It's good to meet extraordinary claims with skepticism and only change our minds as more observation makes it more credible. It is actually to their credit that people initially doubted this student and Louis Pasteur, but were able to change their belief as more evidence came about. 

Edit: On a slightly related note, who knows, it may even be too early to conclude that "[reality does not exist until it is measured](http://www.reddit.com/r/science/comments/37wfte/the_bizarre_nature_of_reality_as_laid_out_by/)." I'd say that is poorly written, and it is more like: When they first saw the data, many of her senior collaborators were unsure of the results, but over the next few months, together, they managed to rule out all potential sources of corruption.

This is how science is done, usually when you find something to good to be true it is. However every now and again you find something that survives scrutiny, is correct, and it is awesome. No one says, 'this is annoying go away', they say 'cool, I wonder why'. This seems written more for dramatic effect. Every time I got an odd/interesting result, the first question my PI (s) would ask is "are you sure you know what your equipment is doing?" I'm guessing that attitude comes from a long career of wasting time looking some new physical force, when it was really just a loose wire. &gt; Pretty sure some of her colleagues weren't nice with their dismissals.

There's a reason why it is called "thesis defence". If your theory can stand attacks, which hers did, it is probably correct, which it is.

Countless others can not stand the attacks, and they are usually* not personal attacks or character attacks.

*Unless your thesis is on LENR. That's just not true.  What would make you think that?

People were totally professional and very supportive.  They were just concerned that it was more likely to be instrumental than real; specifically that it was "side-lobe structures of an uncleaned source somewhere to the south."

These emails are littered with comments like "This is clearly a substantial bit of work, so well done!.  They just suggest things like "It might be worth checking to see if there is an A-team source somewhere to the south that might be causing this e.g. Pictor A, Cen. A, etc.


 That's the wonderful thing about science, all the things you can "what if" about, and then sometimes being right. Kind of like magnetic field lines, but with ionized gas particles instead of metallic material?  It is exactly that.  It is something that astronomers fully expected to be there.  She observed something that everyone had assumed was there. &gt; assumed

But didn't actually know if it was

Edit: apparently I was mostly wrong. Now they know how the phonomenon looks like in 3d and in a higher quality They did, it has been observed both indirectly and directly by satellites. As well as by radio sounding devices using to probe the ionosphere.

This, however allows observation to a high resolution, and large extent very easily. Which offers many interesting avenues for further investigation. It is a very cool find. Not exactly. We've observed the effects of this plasma in satellites for decades. Engineers have to design electrical systems for these interactions. &gt;She observed something that everyone had assumed was there.

This sounds a lot more snarky than you might mean it to be. There's a lot here I'm not understanding. Magnetic field lines are a means of illustrating magnetic fields, though the field itself is smoothly/continuously graded. The alignment of iron particles into spikes or lines around a magnet is due to their ferromagnetism, which locally concentrates the magnetic field in that example. Charged particles are not ferromagnetic, so they should not display this behavior; so why are they forming tubes?

Also, magnetic fields should cause particles to curve in a plane orthogonal to field direction, yet these curves seem to be along the field lines themselves. What gives? Charged particles are not ferromagnetic, but they will align themselves with magnetic fields. This is because charged particles in a magnetic field will move in a helical motion whose radius is defined by r = m*v/(q*B) where m is the mass of the particle, v is its velocity, q is its charge, and B is the strength of the magnetic field. For the ionosphere. This radius is so tiny compared to the size of the magnetic field that this helix is essentially a straight line (for an ion this radius is ~5 meters and electrons are even smaller). Charged particles typically spiral around field lines. So they're both orthognal and along the line.  Check out how plasma behaves in the chromosphere of the sun.  The prominences and plasma filiments are aligned along magntic field lines. No, the point is there are NO field lines. The charged particles will simply start spiralling once they enter a strong magnetic field. We can envision a "field line" when we notice the spirals occur around a common radius. Well yeah field lines are just a mathematic construct to help visualize a vector field.  However the behavior of charged particles can help make the the vector field and field lines visible. http://en.wikipedia.org/wiki/Field_line#Physical_significance What I got from this thread:The "tube" is the area that has the conditions necessary to form plasma.And the spin of the charged particles align with the magnetic fieldEdit: Check out /u/mutatron's comment here below Not exactly.

The plasma is already out there, coming from the sun. It's mainly protons, electrons, and alpha particles. These particles get captured by the Earth's magnetic field, because moving charged particles generate their own magnetic field. The interaction is complex, and the particles tend to end up spiraling down the field. A lot of them get reflected as the field gets denser near the poles, so they go back and forth from one pole to the other.

The tubes are what happens as a result of all the complex magnetic fields from all this motion. The plasma flow attracts more plasma particles up to a point, then it rejects them. So you get volumes of greater density alternating with volumes of lower density as the plasma configures itself into a less unstable pattern. Interesting. I would appreciate more detail on both, if you have time. The magnetic field turns all movement that isn't aligned with the field. Thus you get movement along the direction field (the particles are going in spirals). Fascinating. Amazing work for an undergrad. Her career is worth following. She's got a PhD scholarship for Cambridge or Oxford now. This girl just solidified her career in science. Provided this ends up being accurate she will be able to make any comment on any issue in astrophysics and people will have to at least acknowledge it. This is a career maker. Congrats to her. Okay, so maybe I'm missing something here, but isn't a magnetic field continuous? That the "field lines" are really just a human visualization technique for showing the direction of the 3D field? It's my impression that these "field lines" no closer represent the field itself than a contour map represents the true topology of a mountain range. If that's the case (which I'm pretty sure it is), then how are tubes formed along these lines - since the lines don't actually exist?

I have to imagine that the tubes aren't really being formed along any "lines", since the lines aren't really there. If so, then what determines where in the field the tubes form? There must be some other mechanism causing these to form, right?  This is a great question. Something similar happens when you shake iron filings around a magnetic field. Here's [an explanation of why the filings form in lines](http://van.physics.illinois.edu/QA/listing.php?id=27163&amp;t=magnetic-field-lines-dont-really-exist):

&gt; First of all, electric, gravitational, AND magentic fields are all completely smooth. The "field lines" taught in many classes and used by physicists to visualize field strengths are purely to guide the eye; they don't have physical meaning. (Even so, they are useful lines to draw, and even contain weakly quantitative behavior, since the field strength is proportional to the density of field lines. Just remember that between any two such field lines, the field strength is just as strong as on the lines themselves. The field is smooth.)

&gt; The reason for the creation of these pretty iron filing shapes is that each little iron filing, when subject to a magnetic field, becomes a little dipole itself. This dipole feels the force of the magnet, and aligns in the direction of the field lines. In addition, each little dipole feels a small force from the other nearby dipoles, and they move to minimize their local energy. This causes the clumping into lines that you see, as the opposite ends of the dipoles move together.

&gt; The clumping is NOT a property of the magnetic field from the large magnet, it is a consequence of the magnetic fields of the small iron filings.

Plasma is also self-interacting, and it's dynamic, that is it moves around a lot. Movement of charged particles induces a magnetic field, and because of interactions with the Earth's magnetic field, plasma movement tends to be restricted to flow "along the field lines". This creates instabilities that can only be resolved by clumping into lines in a way that's similar to how iron filings clump because of their own magnetic fields.

edit: I hope this is correct! I used to work in for a team that researched the magnetosphere, but it's been a while. Perfect, thank you. I figured something like this had to be happening to create these structures. No doubt plasma and iron filings have some differences in how they behave, but I'm sure the analogy is strong. 

This is really interesting! So while the magnetic field, absent of plasma, is completely smooth and continuous, the addition of moving charged particles creates local magnetic fields in accordance w/ Maxwell. Since Earth's field is forcing charge flow in the same direction, these small flux fields should cancel at the edges... so at the nodes where things cancel out, only the Earth's magnetic field is left, and the particles would flow along that path and probably try to open up a new "tube". Is this how the wave-like propagation of these things occurs then?
 The analogy is not really correct. One of the big reasons structures like his form is that plasmas are to an extent 'pinned' to there magnetic fields. This means that when two separate plasmas, constrained by there separate magnetic fields meet, they do not like to mix (unless forced to). So once a tube of plasma forms, it won't want to mix with the surrounding material.

(I hope this makes sense, I am very tired...) Her explanation was very clear for a layman. Excellent.  [deleted] The double-edged science sword of one of a kind equipment  [deleted] [deleted] [deleted] 1) Take one of a kind setup. 

2) Bring in unaffiliated scientist

3) If he can reproduce findings, then proceed to profit.  If not, go directly to jail.  Do not pass go, do not collect $200. An issue with doing this is that it could well be a phenomenon of the equipment being used. So the unaffiliated scientist reproducing the results isn't actually confirmation they exist. True, and this would be the case if this was the first time they were running the equipment, but, iirc, they had been running experiments at lower energies for years and were seeing results consistent with what had been previously observed at Fermi and other accelerators, which means that the detectors were working as expected.   O_o Another approach, now that we know what we are looking for, we can build more instruments to look particularly for this. 

No need for a massive expensive radio telescope array, can use a bunch of smaller ones spread out.  Also true! That's why I have issue with the word "prove" in the title.  Science very rarely actually proves anything.  Layman use this term far too often. Agreed.  "Obtain results consistent with expectations" is better, but a little long for the layman.  :) Its also lacks that click bait quality Also, "Found" or "Discovered" is even worse, in my opinion.  "Ben Franklin discovered electricity!"  No he didnt, he found results consistent with lightning being an electric phenomenon.   Well this is published so it shouldn't take too long (or has already been made)

http://onlinelibrary.wiley.com/doi/10.1002/2015GL063699/full usually a lot of the data is public, so your free to go ahead and conduct your own research. 

A lot of good astronomical data is available for free off the Canadian Astrophysical Data Centre.

http://www3.cadc-ccda.hia-iha.nrc-cnrc.gc.ca/en/  Why?

Edit: To all the people harping on about the scientific method, chill. I wasn't saying more review/replication/verification shouldn't be done. I was asking why this lab or this instance of study made the OC decide to totally disregard it in favour of full belief when 1 more lab replicates it. I thought the point of the article was that this student actually got her work peer reviewed. Does the OC have a problem with the lab in general or can another team in the same lab reproduce the result to their approval? Is it the data that needs more review? The radio telescope array? Both?  Even if this result is some kind of outlier why does OC not believe it at all? Surely this needs many more than just 1 more lab to study it if that is the case.  Because that is how science works. It'd be silly if we believed something someone says without some sort of peer review.

I mean who does that? Not just peer review: replication Replication is the soul of scientific proof. [deleted] [deleted] [deleted] Ugh, every time I see a perfect graph I want to yell at the person showing it to me.  

 "Yes, the findings were exactly as we expected them to be. No, we did not hastily paste them onto a graph that we came up to validate our study/project." "What do you mean I just graphed the function instead of plotting the data?" You must have hated the blackbody radiation results vs theory graph.

http://www.explainxkcd.com/wiki/images/7/74/science.jpg Except that many physics findings are "proven" without replication in the cases when it is impossible. ie the Higgs Boson. When there's only one machine big enough to observe/create something, then you just settle for a bunch of scientists looking at the data to make sure it's without error. One-of-a-kind equipment is usually cutting edge, but if you simply dismissed all findings until the experiment is replicated, then it can be hard to move forward. The Higgs boson was detected through two completely separate experiments, the [ATLAS experiment](http://en.wikipedia.org/wiki/ATLAS_experiment) and the [CMS](http://en.wikipedia.org/wiki/Compact_Muon_Solenoid), both set up in different locations along the LHC. The two detector arrays are completely different and thus don't suffer from the same systematic errors. Both observed the Higgs boson (or rather its decay products) and the masses they measured agreed with one another. I stand corrected. Discrediting my poorly researched example doesn't mean that my point is invalid however. There are many one-of-a-kind space telescopes, mars rovers, deep sea vessels, etc. that produce results that become generally accepted and published. Don't they have two detectors on the LHC for this reason?
 Fundamental part of the scientific method; more data confirming findings makes hypotheses more reliable, and if they're wrong they can be further refined or eliminated completely in favour of something else.

Just look at cold fusion and the FTL neutrino anomaly at CERN as two examples of incorrect conclusions. Although I seem to recall the CERN one being a leaked memo rather than a publication... The CERN one IIRC was approximately "We're having these anomalous results, we've done X, Y, Z to verify that they're not method artifacts, plz help".  When, not what. I think the real magic here is that an undergrad student convinced her peers she made a discovery. Tenured professors and experts of their respective fields are incredibly critical, and quick to dismiss. Quite an impressive feat there Why is this controversial?  We've known about [Birkeland currents](http://en.wikipedia.org/wiki/Birkeland_current) for over 100 years.  We've studied the aurorae for quite some time, and [THEMIS](http://www.nasa.gov/mission_pages/themis/main/index.html) has established that the Earth is electrically coupled to the Sun, and flux transfer events are dynamic and continual.

It's certainly a breakthrough to utilize existing telescope arrays in a novel new manner to better image these plasma structures - but haven't we been aware of their existence for at least a decade if not longer? There are many theories in science. Using this array doesn't 'better image' the structures, it images them for the first time, proving their existence. It also offers real time data that can be used along with other readings to give a better understanding of electromagnetic interactions. Anything new is somewhat controversial I don't know if its some sort of jealousy, but the scientific community doesn't seem to handle 'new' that well in general.  [deleted] [deleted] In your opinion, what's a piratical application for science that can be derived from this discovery?  Black holes were shown to exist by Einstein and Schwarzschild's equations in 1915 (but Einstein didn't think black holes existed).  However, a century later, we still haven't directly imaged a black hole, but have observed some of their effects.  There is a [14m project](http://www.space.com/24002-black-hole-image-event-horizon.html) in progress to view one for the first time. Nikolas Tesla hinted on the possibility to tap this flux for "free" electric power. Tesla wrote about using the Earth's resonance to facilitate wireless power transmission.  That's not *free* energy. Why be rational when we can sensationalize!? I don't really see any sensationalizing here.  There was a theory and this is the first experimental proof of that theory.  That's cool. Now there needs to be confirmation experiments. That is science.  I don't remember seeing anything sensationalized in the text of the article or the video.  Its not just a big sky...its a series of tubes! More seriously this is very exciting. I m interested in knowing why nobody has tried the left-right approach before to 3D those I agree, it seems like such an obvious thing to do. Usually we use radio telescopes to look at objects that are much further away (stars and galaxies) and so the stereo imaging trick wouldn't work. Because we were looking at the ionosphere (which is very close to Earth) Cleo thought of this trick and it allowed us to measure the size of the tubes. &gt;Ms Loi said the drifting plasma tubes could distort astronomical data, especially satellite-based navigation systems. It may also mean we need to re-evaluate our thinking about how galaxies, stars and clouds of gas behave and what they look like.

this is the part im most intrigued by...generations of data wiped out/or need asterisked b/c we didnt realize there was plasma lensing in the way?? Luckily the effect of the ionosphere is only a significant problem at low radio frequencies. So for most telescopes this wouldn't be a problem. However, for the MWA and LOFAR, which operate at ~100 MHz frequencies, we have to model and correct for this effect to ensure that the measurements we make of distant galaxies are accurate.

Basically the ionosphere makes the position and intensity of distant objects (e.g. galaxies) constantly change by small amounts.  Late to the party, but let's cut through the speculation.  Here's the paper:

http://arxiv.org/pdf/1504.06470v1.pdf

Note that there are over 40 authors, yet the linked news article chose to focus on the lead author.  Not saying it's not her (very cool) discovery, but it's not clear to me what everyone's role was.

The paper notes that the so-called "plasma tubes" (a.k.a. "field-aligned ducts") are "widely accepted" as the explanation for particular observed waves, and have been observed previously.  The key advancement here seems to be the realization that an existing radio telescope array could be used to probe the atmosphere due to its high field of view.  They mostly confirm what was previously known, but suggest that the plasma structures may be drifting.  Cool stuff. The data was collected with MWA, every paper from MWA will have a large number of co-authors as a project this large has many people involved in the data pipeline etc. Realistically the majority of this science was done by her, and maybe her supervisors (so 3-5 people). However many people have spent countless hours making this science possible, so they are included. All large collaborations are like this SDSS, MWA,SKA etc from astro, or check out the LHC papers for even more authors.

edit: From looking at the authors, some of whom i am familiar with I can assume: She saw this went to her direct supervisor, who realized it was cool and talked to I. Cairns (a near earth space physicist as opposed to astrophysicist). He then talked to Waters + Menk, who are Ionosphere, plasmasphere and magnetosphere specialists. As she is first author she would have written the majority of this paper, with input from those mentioned above. I would say the rest are involved in the MWA project. Helpful insights - thanks!  Obviously I'm familiar with how names get onto published papers, but I'm not familiar with this team or this particular technology area. I can roughly summarise the contributions. Cleo was the first author because she led the scientific analysis and interpretation, and wrote the paper. Many of the key ideas were her own, and she was able to establish that what we were seeing was real.

Some of the senior authors listed next are ionospheric experts and helped interpret the results in the context of the existing scientific literature.

Then there are a number of postdoctoral researchers who wrote the software that takes raw MWA data and produces images. There are many steps to that process and each of them is responsible for different parts of the pipeline.

Then there are some senior collaborators in the MWA who were involved in the discussions about how to interpret the data and the results.

Finally, there is a the MWA builders list, in alphabetical order. These are the people who made a substantial contribution to building the MWA telescope. The MWA publication policy stipulates they must be included to ensure their work is recognised: http://www.mwatelescope.org/team/11-team/39-author-list

Although this work involved the contributions of many people, I can confirm that a majority of the ideas and analysis done by Cleo :) **Just so everyone knows. This is [Tara Murphy](http://www.physics.usyd.edu.au/~tara/), Cleo's advisor.** &gt;It may also mean we need to re-evaluate our thinking about how galaxies, stars and clouds of gas behave and **what they look like**.  
  
Why? We have telescopes in space, right? The ionosphere starts at an altitude of 50 km and extends to 1000 km high. On the other hand, the Hubble Space Telescope only sits at an altitude of 559 km, directly in the middle of the ionosphere where these plasma tubes pass through. And even higher above the ionosphere, we have the plasmasphere that contains cold plasma. This will not effect visual observations done by Hubble, it is a radio signature. But visual is only a fraction of the observations made, radio is a big part of observational astronomy, and being able to correct a small error (probably only a fraction of a percent) will be useful for many people to come. How is this different from the aurora borealis? Edit: so they're invisible then. That was my first thought too. I'm surprised there isn't more comparison or association mentioned to the auroras. It definitely seems the easiest way for laymen to grasp what they are talking about. Is this what tesla wanted to use to power the earth? I think there is a connection there for sure. He was playing with the idea of plasma, electromagnetic fields, and mentioned the ionosphere a lot so it has to mean something. I recommend studying plasma in the vicinity of earth and sun, its role and movement, and you may answer some of your own questions. It could very well possibly be!

Plasma is a HUGE conductor, so... imagine large wires or bigass batteries that we can "tune into" free power like we would tune in a radio station, or turn on a cellphone.  Pretty much exactly what Tesla wanted to do with Wardenclyffe tower... 

Plasma research is useful for us as plasma is used already in plasma gasification, which is already a source of energy for us! 

This kind of research is also good for the field of magnetohydrodynamics.
 Love these kinds of discoveries! Neat trick! The video made it more clear to me. Now if we can figure out how to ride these things.  I'm quite interested in what implications this has for spacecraft, if any. My first thought: communication. Can they be used as conduits for any kind of radio signal? Wait, haven't we known this since the Van Allen belts were discovered? Discovery is the result of work on phased-array radiotelescope stuff. Article shows researcher looking through the spotter scope on an optical telescope. Yup.

 It's a stock photo of Cleo Loi the student the article is about. [deleted] It's a more romantic view of astronomy then her staring at a screen.

It actually would be great though to get her near the array they used.  Cleo has never seen the array up close  :(

We'd love to have Cleo and Tara over to visit.  It's just a matter of finding dates that work!
 When I went to college the professors didn't even bother going to  going to the telescope they just had the results sent right to the school and they called the telescope if there was a problem or they wanted to discuss something. Science that has to work in remote locales is harder to conduct than science that can be done anywhere. I'm calling that a win

I can't wait for aerial drone based rainforest science for instance Reality vs. Image. 

 The theorists come into the laser lab all the time when people need a photo because it's more interesting than what they actually do. Media always wants me to hold a rack of test tubes and get a closeup of my hand running the minifuge. "But I don't actually use the minif-" "Just turn it on, ok? It goes WHIRR, it looks scientific." This is awesome!  A few years ago, I came across a friend of mine who was in my electrodynamics class.  He didn't seem to be doing one of our problem sets at the time and when I went to ask him what he was doing he told me that he was trying out theoretical models for lightning on planet earth.  Apparently its so complicated that though there were theoretical models out there, none of them were really confirmed.  This seems like something that could falsify some models and support others.  The weather might be chaos but there is order I'm chaos! Why does the earth have magnetic field *lines* instead of just a magnetic field soup or whatever? Why are there lines? The lines are usually just a convenient way to visualize the field, but in this case the plasma's interactions with itself cause it to "clump" into tubes, much like the way iron filings clump into lines around a magnet, because each iron particle has its own field. 
 I wonder if this explains [red sprites](http://en.wikipedia.org/wiki/Sprite_%28lightning%29). [deleted] Side note: I love it that gender was not glorified here. No, "female scientist discovers..." Just scientist. Nice.  But... You just... Oh, never mind. Yes, I know and I knew that as I wrote it. But I feel  now days you should applaud equality when it happens. Positive reinforcement.  What role do plasma tubes play within the universe? http://en.m.wikipedia.org/wiki/Plasma_cosmology

Guess the universe is one big electrical circuit  What are the implications of discovering these.  Are these possibly important factors in maintaining our atmosphere? Us this article saying that they may be interfering with measurements taken from earth into outer space? Now we need journalists to make a puff piece about how these cause cancer. Video has been reddit hugged. Probably no mirrors out there yet I would imagine. Sure would be nice if news sites would make use of a reputable streaming service.

edit: Never mind, my flash blocking plugin didn't allow their ad to load. It played fine for me on mobile just now, if you want to re-try it. Worth watching. Geezus, girl is an undergraduate student to boot.  Funny how even I, a female engineer, assumed it was a male student. [deleted] What does the discovery mean in terms of application or use? Not saying there necessarily needs to be one, but if there is, could someone help me to understand?  There are lots of applications. For example, those plasma tubes move across the sky and can distort our astronomical data, but if we knew where they were, we could use that knowledge to correct the distortions. Infrared was once a novel curiosity of science so was DNA and laser technology. It can take some time before usefulness is fully realized Haha, that picture of her looking into the telescope. They always seem to do that kind of thing. It has nothing to do with her research but looks nice and sciency. My wife has been pictured in a few publications and they always get a picture of her "loading a sample" or letting LN2 evaporate because it looks cool. Anyone who has any idea about her research can tell it is goofy. What does this discovery mean? I wonder if these can be used to transfer data What does this mean? And why is this important? (I am just curious) wow... this is the kind of thing that can really make a career.

she's going to spend the rest of her life studying these things on an endless stream of grant money.

you can clearly see the channels in her imagery too...  Is this a discovery that is a "oh cool neat!" discovery or is it a discovery that could lead to something more massive? The discovery sounds kind of big in the field but I don't understand the discoveries potential for future developments.  How will this effect the way we measure stars and planets? Does it mess up the spectography, radio, xray? *THIS* is what science really is.  The stubborn pursuit of an idea, if you do find something, do the work to ensure that it's reproducible by others - otherwise keep hunting.  Shyeh Tjing Loi, you rock. That's not been my experience, usually it's a huge group of people all bouncing ideas off each other for awhile until they come to some sort of an agreement. Or a bunch of bored physicists get a new piece of equipment and play around with it until it does something weird and they go I wonder why it did that? Other than looking sweet, what do the plasma tubes do to climate and the environment? Here is the peer-reviewed journal entry: http://www.bloodjournal.org/content/early/2015/04/22/blood-2014-07-587329 There are a lot of big words in there, can I get a ELI5 please? The article that the OP linked has a good explanation

&gt;During laboratory testing on human cells, the process worked this way: The myxoma virus is attached to a type of white blood cell known as a T-cell. The virus-laden white blood cells can then be delivered as part of a bone marrow transplant from a donor. Thats when the virus gets activated and goes to work. It blocks graft-versus-host disease, a complication of bone marrow transplants that can cause problems including skin rash, shortness of breath, abdominal pain, jaundice and muscle weakness. In severe cases, these complications can be fatal. The white blood cells then deliver the myxoma virus to cancer cells, which are killed off by the virus. So why is this not a treatment yet? I'm not qualified to answer that, but I'm guessing it needs trials and testing? I'm guessing it's white blood cells in a petri dish too. Need a delivery mechanism, dosage calculate and long term study too.

\^_^ Well I thought the whole point of this article was the virus that delivered the cure so why would finding a delivery system be part of it?

Edit: I understand what a delivery mechanism **is**, what I'm trying to point out is that I thought that injecting a virus to the bloodstream was part of the mechanism, after all, viruses are used to carry cures all the time. Sorry for the confusion. &gt;The virus-laden white blood cells can then be delivered as part of a bone marrow transplant from a donor.

This looks like the delivery method sticking point to me.  I would think that it's applicable if you need a bone marrow transplant. But inconvenient if you have a type of cancer that isn't in the bone.  Yep, they are known as Oncolytic Viruses It's like war, you need a way to effectively move your troops. In this case they've probably examined the effects of the virus in a Petri dish, so now they need to follow up with multiple tests in live subjects. The virus is the cure, and is hosted in the white blood cells, but what is the best method of injecting those cells? How many cells need to be injected for an effective dosage that won't harm the patient? How little will be ineffective?  What complications can occur from the procedure not yet seen through the initial tests? 

All this and more has to be answered, and answered definitively enough for the health department.  This means many many many trials with proven success. Not to be a negative Nelly  but this is where in all likelihood the treatment will fall apart. It's not for naught, we learn something every time one of these potential treatments is found and an iota of knowledge is added to the war against disease but most of these collapse upon replication, live trials and peer review.

You just have to keep plugging along and not be afraid to fail a thousand times more often than you succeed.  Because you usually want a treatment to be focused on a point to be effective I guess, not randomly attach itself to every cell. Good point. This is a common problem for most of these hypotheses treatments Right. In vitro != in vivo. Not to mention FDA regulations and clinical trials.  Oncology Pharmaceutical Research here, yes the aritcle states clinical trials should begin within the year. Tons of treatment look good in the laboratory and animal models, but are not effective or too deadly in humans. My wife also has ALL Leukemia so this is something I am going to follow. Thanks /u/Libertatea  [deleted] Thanks so much. I do hope your wife will get better.

My friend also has leukemia, the standard chemo and transplant treatment has failed, and doctors have decided they have nothing else that they can offer. Do you know how can one apply for such experimental treatments?  It all depends where you are being treated. In the United States, you will need to go a different cancer center. Look up her condition on clinicaltrials.gov and that will tell you the locations and trials. It's a lot to sift through. Basically you will probably end up traveling to a different cancer center,  such as Dana Farber in Boston, MSKCC in New York City,  MD Anderson in Houston, ect. Sorry to here that. Is your friend located in the US? If so this website one can look for clinical trials by diagnosis and geography. clinicaltrials.gov :( Really sorry to hear that! I hope she gets better soon. Stay strong, brother.  Thanks we are over a year into it and she is doing well, I just really appreciate you posting this. Just remember, it may kill cells in a Petri dish, but so does shooting it with a hand gun. 

that's an old xkcd or whatever comic.  Yep http://xkcd.com/1217/ How do they make sure the virus doesnt mutate and start to become harmful to humans? The other thing is that since it's a DNA virus its genome is fairly stable (relative to RNA viruses). If it did get a mutation or two, depending on which gene it is in, the virus would likely either become attenuated (even less harmful) or just not grow at all.  Essentially what amounts to genetic manipulation. You can remove segments of viral DNA or RNA that reduce its ability to cause disease. 

Source: molecular biology major.  I'm not sure about this one, but a lot of the cures for cancer are like this: http://imgs.xkcd.com/comics/cells.png Just scanning this I'm thinking evolution / mutation - and in this field I'm a layperson. 

So I'd say I'm not qualified to answer that but I'd take a relatively educated guess that they'd want to see if anything negative can happen.  They're dealing with a viral infection here - don't want it to potentially mutate like Influenza does - then do what it's not introduced to do ... which is do whatever it does to rabbits but to people instead. &gt; but I'm guessing it needs trials and testing?

thats a good guess Well, I imagine they're going to want to refine it a bit more before injecting people with a virus that wipes out T cells.  It's good that they can avoid graft vs. host disease, but bear in mind that your body also relies on T cells to protect you from a whole world of pathogens. Your body makes more of them.

In the mean time, patients literally do have *no* immune system, which is why bone marrow transplants involve at least some time (weeks to months) in a sterilized isolation ward specifically for patients undergoing BMTs and intensive cancer treatments (usually leukemias and lymphomas) because their immune system must be depleted to little or nothing in order for the procedure to be successful.

All hospital staff working in the ward needs to be completely healthy (no sniffles allowed) and often wear sterile gowns (EDIT: over their clothes) to enter patients' rooms.  Visitors are thoroughly screened and not allowed to bring in outdoor clothes and not allowed to visit if they've been sick or in the vicinity of anyone sick within the previous week.

The entire ward has a dedicated air filtration system.  Any construction work that takes place can only happen room-by-room and must be sealed air-tight, as the slightest bit of dust could be lethal to patients.  Rooms nearest the ward entrance are negative pressure, so that air inside the room flows outward whenever the door is opened.

It's a very different patient experience than most hospital stays. We let visitors wear their own clothes. They just have to put a clean cloth gown over the top.  &gt;So why is this not a treatment yet?

~~It is! ...or, at least~~ something similar ~~a form of it~~ is.

I received a dose of rabbit ATG ([Anti-thymocyte globulin](http://en.m.wikipedia.org/wiki/Anti-thymocyte_globulin)) as part of the preparative regimen for my bone marrow transplant.

~~I think this is basically the same thing, with an added virus payload to increase the efficacy of the treatment.~~

**EDIT:** I was wrong, misread one the posts above.  Silly rabbit, tricks are for kids! That's not the virus that's antibodies.  Not quite, in this case the virus is actually used as a sort of super weapon against cancer cells! Yeah, they are both derived from rabbits, but the treatment being suggested here is pretty different.

How are you going? How far post BMT are you? About 3.5 years.  I'm still having some rough times, but I'm not dead, so I got that going for me... Just curious, do you know where your transplant came from?

I'm asking because it is literally my job to send transplants on their way in a German based clinic. 
So I'm interested in the whole process of the transplantation from the patients point of view, ~~and I am wondering if I handled your transplant (chances are good if it came from Germany in the last year) myself.~~

Anyway besides my curiosity, I wish you all the best for battle against fkng cancer. Woah... OK, this is interesting.

So, I wasn't actually allowed to have direct contact with my donor, but my nurses did say that my donor was likely European (something to do with the timing of the transplant procedure) and my lineage is Germanic.

My transplant was 3.5 years ago, but still pretty interesting nonetheless. Genetically speaking you're lucky to be of germanic heritage, because Germany has the second largest donor file in the world. 

It's a really interesting job I'm doing but I regret that I didn't pursue a different education so I could work on the medical side of things but that's how it is now.

Basically I'm handling everything donor related and the hand over of the different *products* , I'm not a part of the medical team but the last one of our company who handles the bags.

If you have any questions I'll gladly answer whatever I can, but again I'm no doctor so I can't speak of the scientific point of things.

Btw. If your transplant came from Germany, chances are pretty high that it was donated in our Center. 

Keep up the fight against this cancer-son-of-a-bitch.
 I think I'm qualified enough to answer this:

Technically, it is. It's a type of therapeutic cancer treatment that utilizes things called Oncolytic Viruses. These are essentially viruses that are programmed to or naturally target cancer cells over regular cells. Many viruses, such as modified Measles, VSV, the common cold, and Duke University's famous use of the modified polio virus to treat glioblastomas (fast-growing and previously lethal brain cancers).

The reason it isn't yet a widespread phenomena is because 1) it is a much more dangerous way to treat cancer compared to other therapeutic approaches available today, 2) It is expensive and 3) there just hasn't been enough time for the technology to develop and companies to be formed around it. 

I work for a venture company and this was one of my most recent projects, so if I had to put a time on it, (based on the success of future treatment) I could see this being used more often in 7-10 years then regularly in around 15-20.

*EDIT: just over 1000 patients have been treated with Oncolytic Viruses* If I wanted to work on researching stuff like this, how should I start? Just to add to that- there is an oncolytic adenovirus approved for head and neck cancers in China, and a modified oncolytic herpes virus was just recommended for FDA approval for melanoma treatment too.  Yep! H101 (Oncorine; Shanghai Sunway Biotech) They just discovered this and most of the work has likely been *in vitro* and in animal models. They need money to go forward with human trials, particularly safety trials (make sure virus doesn't have unintended consequences,etc.). They also need to generate clinical grade virus (yes that is a thing). So they can move forward from this discovery, but it requires a lot of money and time. It takes sometimes years of trials and testing to turn a discovery into actual treatment. They have been using rabbit and horse atg to try and treat aplastic anemia and prevent graft versus host disease. I don't know if these are related but they've been doing this for over a decade. It takes a long time to get through clinical trials and FDA approval.  There are expectations that virus treatments for cancer will be available to the general public within the next few years.  Several other genetically modified viruses have been used with great success to treat cancer, such as measles, HIV, polio, and the common cold. anything involving a live virus can be extremely dangerous though not readily apparent. There were a dozen or so people treated with transformed viral vectors for SCID, and while the majority actually were practically cured (so long as they continued the treatment), most also developed cancer down the road. It'll take a lot of successful animal testing before this ever goes into a living person, let alone becomes widely available. I think an important distinction to make here though would be, in the case you mentioned, that they were using the viruses as vectors which would integrate the delivered gene into the DNA - problem was the DNA could integrate into tumour suppressors or cell cycle regulators. Using oncolytic viruses in this case isn't doing the same thing- you're not integrating DNA or anything like that. One modified herpes virus for melanoma treatment was actually just recommended for FDA approval and has gone into many people.  [deleted] &gt; graft-versus-host disease

Wait, wait, wait. So this is an actual thing and not just a made up disease in Arrested Development? The transplant can be curative for hematological cancers. But it gives you a chance on Graft vs Host Disease, which can be nasty. But GvHD goes hand in hand with Graft vs Leukemia. GvL is what you want, because you don't want the cancer. If you stop the GvHD you will also stop the GvL. But now they found a way to only get the GvL effect and not the GvHD.  TL;DR

* This for treatment of leukemia and other blood cancers which require bone marrow transplants.

* A common complication with bone marrow transplants is Graft vs. Host Disease. This is when the newly produced T cells attack the transplant recipient's body because it recognizes it as foreign. 

* This virus can be used to suppress the amount of newly produced T cells, minimizing the amount of damage done by Graft vs. Host Disease.

* The second punch of the one-two punch is that these T cells also transfer the virus to any leftover cancer cells after the transplant. The virus kills off these cancer cells, and helps prevent relapse after a bone marrow transplant. I lost a friend to GvHD, if this works and people don't have to go through this in the future it's a fantastic thing.  If you have a blood cancer, a standard treatment is to give you a heavy dose chemo to kill all the caner it can find.  This also destroys your immune system.  To restore your immune system there are two options.  The first is to re-transfuse in stem cells taken from your own blood before the procedure.  This is called an auto transplant.  The other is to transfuse stem cells taken from a donor allogenaic transplant.  

The allo is usually better at treating the disease but also has a much higher chance of killing you.  For multiple myeloma, the death rate from the allo procedure is 15-20%, and this uses a modified version of the allo to reduce the risk.  This compares to 3-5% for the auto.  

The risk from the allo comes from two things.  First, you have an extended period of no or very reduced immune response.  So you can easily die from catching the flu.  (Which is why I am sooooo pissed at the anti-vaxers).  

The other is that the new immune system can decide that anything in your body is foreign and attack it.  This is called graft vs host disease (GVHD).  Here "graft" means the new immune system from the donor.   And "host" means the patient in whom this new immune system has been grafted.  This can range from short term (acute GVHD) to long lasting or permanent (chronic GVHD).  

In order to fight GVHD, the patient must take immune suppression drugs.  The longer they take these and the stronger they have to be, the more likely they will die from some random infection.

By reducing the GVHD, this new treatment helps in two ways.  First, lowering GVHD directly helps the patient since the GVHD can result in severe impairment or death.  Secondly, by reducing the need for immune suppression drugs, it reduces the likelihood of death by opportunistic infection.

source:  My wife has multiple myeloma.  It is considered incurable, but treatment is much better than it was in the past.  Life expectancy has increased form about 2 years to about 5.  Normally when we transplant hematopoietic stem cells to a recipient, the mixture that is injected is not pure. It contains donor T-cells as well. Sometimes these donor T-cells can be activated by the recipient dendritic cells which result in the donor T-cells attacking the recipient's tissues. This is known as Graft vs. Host Disease. The obvious solution here is to eliminate the donor T-cells and inject only hematopoietic stem cells into the recipient. But it turns out that those donor T-cells is very good at destroying cancerous cells in the recipient (What they call Graft vs Malignancy). 

What these researchers discovered was that the rabbit virus MYXV can bind to the human T-cells (from the donor), but remind inactive until the T-cells themselves are activated. Once the T-cells are activated, MYXV becomes active and enters into the T-cell and slows down proliferation/signalling, preventing GVHD. T-cells are then able to deliver the MYXV virus (which is oncolytic by nature) to cancerous cells without the complications of Graft vs. Host Disease. 

I'm not sure of the mechanisms of how this works, but that's what I interpreted from the abstract.  Depleting the T-cells also leaves patients far more susceptible to infections. I'm only able to read the abstract but here's what I gather.
T cells are the cell mediated adaptive branch of your immune system. Imagine every cell in your body holds of a picture frame with a picture of what's going on inside of them. A given person only has so many picture frames and the T cells are educated to only approve of cells that have a familiar picture frame with a picture of healthy cell stuff. If a cell holds up a foreign picture frame, let's say a stainless steel instead of the familiar wood, a T cell gets activated and tries to kill it. If their a familiar picture frame with a picture of a virus in it, a T cell will get activated and try to kill that as well. If you get a transplant you now have donor T cells which see your own body as foreign and try to kill it. This virus infects the activated cells and slows them down and weakens their killing ability. So this claims to be the holy grail of transplant science.

Basically, there are many people who do not respond very well to our standard chemotherapy treatment for blood cancers.  Some of these people are fit enough to undergo what is called an 'allogeneic bone marrow transplant', which is a very high-risk procedure and basically involves giving lethal doses of chemotherapy (and often total-body irradiation as well), while teeing-up a donor who will donate their bone marrow so that it is given to the patient just at the end of receiving all their chemo and radiatherapy.

It was originally conceived to treat blood cancers, because blood cancers originate from the bone marrow - essentially your body's factory of blood.  The idea was simply to wipe it out and use someone elses, and so cure them.

It did work at curing patients from cancer, but it didn't work in the way they thought it did.  To cut a long story short, they discovered that it was the new immune system from the donor that was key to finishing off the cancer.

After an allogeneic bone marrow transplant, your body becomes something like a country that has just had a civil war.  The previous authority has been almost completely wiped out, and now there is a new power in town.  But this new immune system doesn't have the history and the knowledge of its new environment.  Its not sure what is good or what is bad, and goes on genocidal missions wiping out perfectly law-abiding cells - particularly in the liver, the gut and the skin, but in theory, absolutely anywhere.

However, it's a two-edged sword, and if luck is on your side, it will also recognise the blood cancer as something that needs dealt with - something that the old authority allowed to grow and get stronger causing the disease in the first place.

So how well you do after receiving one of these transplants depends on how bad the graft-versus-host disease is (the damage that we don't want on healthy organs) and how good the graft-versus-tumour is (the damage we do want).

Unfortunately, the immune system is so complex, we can only manipulate it in very basic ways.  The most simple of which is we can 'tone things down' a bit.  Using steroids and other drugs, the immune response can dampen down, which can avert fatal consequences of graft-versus-host disease.  However, we know that when you do this, it also lets the cancer off the hook as well.

I've had a look at only the abstract of this paper, so I can't see the details, but it looks like they're using the rabbit virus myxoma to infect the transplanted T-cells.  The T-cells are like the conductors of your immune system, and they are the ones responsible for both the graft-versus-host and graft-versus-tumour effect.  It's only when the T-cells get activated (i.e. go into their 'killing' mode) that the myxoma virus cuts it short.

As the T-cells will interact with the tumour (in this case myeloma), they also deliver the virus to the myeloma cells.

So the idea is that you can tone-down the T-cell response, and reduce the graft-versus-host disease, but you can also kill myeloma cells with the myxoma virus.

This research sounds very much in the 'laboratory-bench' stage of things, and it's purely theoretical that this would do any good for patient, which is why it is unlikely to be used on patients as part of anything other than a phase-1 trial on a few patients.  Potential problems I see with it are: 1. It still seems to suppress the immune system generally - which means you may lose some of the graft-versus-tumour effect you had from the donor T-cells. 2. The T-cells may well deliver virus to the tumour and result in tumour cell killing, but presumably the T-cell will deliver the virus to elsewhere in the body as well - maybe even worsening graft-versus-host disease?  I don't know really know enough about this to know if these things are even possible, but those were the first two thoughts that popped into my head reading it.

As a side-note, something similar to an allogeneic bone marrow transplant might soon be regular use, which may bring the benefits without having to undergo a transplant.  It's call 'immune checkpoint inhibition', and it works by (get ready for the double negative) blocking the inhibitory signals that stop your immune system killing the tumour.  (put PD-1 inhibition into google news for further info...) &gt;Submitted July 16, 2014.
Accepted April 13, 2015

Took their sweet time reviewing it. TLDR: They probably requested a number of changes that took that long to complete.

For those not familiar with the process, when you submit a paper to a journal the reviewers will usually request that certain things be clarified or even new testing run in order to actually accept the paper.  For example, they might say that you need to show X data, and to do so you have to run some more experiments.  You're given a specific length of time to do it in, and if you do it satisfactorily the paper is accepted.  While it's not a perfect process, it ensures that the submitter has a relatively sound basis for which to state their conclusions.

In this case, they were probably asked to do a number of follow-up tests and given nine months to do it in.  This is a little long, but not outside the realm of possibility if major testing needed to be done.  After doing all the follow-up testing (and probably some additional clarifications and edits) the paper was officially accepted. Could the fact that this reduces host vs graft disease mean that it could also be of use in treating autoimmune diseases such as rheumatoid arthritis and MS?
 What kind of cancer cells can be killed? looks like its only affects immunlogical cancers, myltiple myeloma, perhaps lymphomas etc. eli5 cancer types like immunlogical plz* cancers of cells that are part of the immune system. b cells, t cells, maybe lymph nodes.  thank you. can you tell me some others types ? (big categorizations not all the million forms) http://en.wikipedia.org/wiki/Tumors_of_the_hematopoietic_and_lymphoid_tissues

 Cancer is basically like a cell deciding that it wants to grow non-stop - normally the body has mechanisms to restrict where and when a cell grows, but when these break you can get cancer. One of the ways to classify cancers is by what cell they started out as before they became cancer because to varying degrees the cancer will act like the cell type it used to be. Immunological cancers would therefore be cancers of cells of the immune system (B cells and T cells, which are called lymphocytes and use antibodies, and a whole class of cells called myeloid cells, which do things like eating bacteria). These form the leukemias if they're in the blood and lymphomas if they're in the lymph nodes.

Source: Med student, took a final on this stuff yesterday. [deleted] [deleted] So what stops the virus from killing off normal healthy cells? How does it specifically target cells that are growing non-stop but not normal ones?  its safer to live with all dead t cells that you can eventually get back than with cancer. most chemo and transplant patients are already chocked full of immunosuppressants or have their immune system severely inhibited anyway. i only read the abstract but it looks like it inhibits t cells regardless of whether theyre tumor or not.

a method to specifically kill cancer cells and not healthy cells is the holy grail of cancer treatment Cancer cells are very disregulated in many pathways, for instance, their immune signalling pathways or cell cycle regulators (ie. To keep growing and rapidly dividing). In normal cells, viruses encode genes to turn off these same pathways in order to help them grow. In the case of myxoma, its a rabbit virus, so it would encode proteins to screw up rabbit pathways. In a normal cell, the myxoma can't grow because the pathways are functional and therefore the virus is stopped. In a cancer cell, these pathways have been shut off to enable to cancer to grow, but at the same time it now can't stop the myxoma virus, so myxoma then grows and kills the cell Mxyoma has been tested as an oncolytic virus for a couple of different cancers but the issue has typically been that it doesn't spread well in solid tumours. As an alternative researchers have taken human pox viruses (mxyoma is rabbit pox) such as vaccinia. Pexa-vec is one oncolytic pox virus in clinical trials again against other cancers. Phase 2 I think.  Is this the virus that was introduced to the Australian rabbit population. To control their population growth? Yep! Explain this like I'm 12 please. The article that the OP linked has a good explanation

During laboratory testing on human cells, the process worked this way: The myxoma virus is attached to a type of white blood cell known as a T-cell. The virus-laden white blood cells can then be delivered as part of a bone marrow transplant from a donor. Thats when the virus gets activated and goes to work. It blocks graft-versus-host disease, a complication of bone marrow transplants that can cause problems including skin rash, shortness of breath, abdominal pain, jaundice and muscle weakness. In severe cases, these complications can be fatal. The white blood cells then deliver the myxoma virus to cancer cells, which are killed off by the virus.

 [deleted] There are dozen of oncolytic (cancer cell lysing) viruses. They take decades to shepherd through the bioengineering process before they can be sufficiently modified, replicatively-hamstrung, and repurposed to fight tumors in humans. While this is *novel* it's not worth any real praise from the lay community. It's very esoteric stuff.  Almost all of the popular and controversial stories in /r/science are like that. They're about some fairly esoteric finding, that's actually pretty interesting if you know anything about the science involved, but that freaks out the general audience /r/science reaches by virtue of being a default subreddit.

Then for a story like this you get all these comments like "Did reddit cure cancer again?" which are mercilessly deleted by the moderators.

For stories involving a refined date for some evolutionary event, you get a lot of "Science wrong again? How can I ever believe anything they say?" which are also mercilessly deleted.

Some science stories get a lot of "Duh" comments, because most lay people don't understand the difference between "common sense" and verifiable results. 

It would be nice if non-moderators could have a toggle to show deleted comments, because they're a good window into the minds of regular folks, if somewhat disturbing to think about. All this work certainly deserves praise, but as part of the "lay community"  who has dealt with cancer in the family, my lack of understanding and hope caused by these headlines is disheartening every time I realize we're talking about potential treatments that are years away from being helpful to those I know who are suffering now. I know I should be more forward thinking and understanding of the process, but it's extremely difficult.  It's almost like, look at all the cool stuff that probably wont be ready in time to help anyone you know right now who is dying.  Well, the media is the culprit here. They even goad the scientists into thinking that flash-in-the-pan fame on nightly news and articles circulating the internet actually equates to something. Meanwhile, the major lesson for scientists has always been the incremental advancement of our science. There simply aren't any big single discoveries anymore. Everything is massively collaborative and piecemeal.   That's a really cool development. Graft versus host disease is a devastating complication. However, I do wonder if this approach could also pose a risk though and that you might get more (secondary) infections with other killers such as CMV, EBV and other opportunistic pathogens. I hope their animal model studies will shed some light on this as well.. I've always wondered why there aren't virus' that are good for humans Viruses aren't inherently good or bad for humans, per se. They are transposable elements that copy-paste themselves throughout the genome, but they've become so advanced that now they actually encode genes not just to copy and paste but also to create a protein shell that allows them to mass-multiply, and  escape form one cell and infect another.

Influenza even evolved a mechanism to induce sneezing! Now it can spread on the air. The only reason you sneeze when you have th flu is to spread virions. Ponder that shit for a fucking moment! So theoretically we could put a virus into a baby in the womb (a zygote for example) and turn it into something completely different with the right virus (like the hulk or super human)? Why isn't virus genetic research a bigger thing?  &gt; Why isn't virus genetic research a bigger thing?  

For one, it's hard to target the effects. And also money. A lot of medicine is for profit, so if the thing you're researching isn't likely to make a profit, it's likely going to be done in low funding slow moving settings. 

&gt; So theoretically we could put a virus into a baby in the womb (a zygote for example) and turn it into something completely different with the right virus (like the hulk or super human)?

Well I'm fairly certain that would kill the cells, seeing as viruses don't generally leave the host cells alive. You'd just end up killing whatever baby cells were there in all likelihood. 

Though I don't know much about this, so take it with a grain of salt.  [deleted] [deleted] [deleted] [deleted] So it's a way to suppress the immune reaction to the transplant? No, it's vice versa.

In Graft vs. Host reaction, it's the transplanted immune cells that attack the host. So the Donor T cells are suppressed but the virus is allowed to attack the cancer cells?

Why would the T cells need to be suppressed when they can help with attacking the cancer cells as well.  I had this treatment for my pediatric PNH. Two years before I had the same treatment, but from a horse. 

It saved my life. Yep. I got a chance to speak with Dr. McFadden about this. Very interesting stuff. Myxomavirus may kill rabbit cells but not normal human cells. Also, its RNA may be injected into all cells but only certain cells with certain cell processes already activated that could support its replication can be eliminated. Yes! Really cool, and really smart to use myxoma for this type of cancer too- I was saying earlier it doesn't spread well in solid tumours so this seems to be a great niche for this oncolytic virus to fill oh hey look another cure for cancer we will never hear about again [deleted] When I was 14 I had leukemia and before I had my bone marrow transplant the doctors gave me what they playfully called bad bunny juice any ideas what it was? So this is another T-Cell modification therapy. I was talking to a cancer researcher and he was saying thanks to technology these sort of things are just going to explode. The next issue is funding. The funding for these things keeps drying up so now we have more ideas than ever before but less funding.  
Edit
Less funding available per idea due to having so many new possible ideas and no increase in funding / minor decrease in NIH funded research. 
 This kind of discovery is what I like to see more of.  Billion dollar grants would enable us to tell more stories like this. .  [deleted] Isn't this virus sometimes artificially introduced to cull rabbit populations? You are correct.  being tired and without my glasses, I read that as the Reddit virus...  Late to the party, but does anyone here think that with the leaps we have been making cancer will be much less dearly in a few years?  [deleted] So cool. I've lost 3 family members to blood cancers in the past 7 years. I hope this continues to be promising. [deleted] I'm probably just not properly caffeinated yet, but this article seemed to go in circles. 

90s grads moved to trendy tech cities, 00-10 grads aimed for large cities with large labor markets. Cities can only attract new grads by having a robust job market, but not by attracting tech startups? Just have lots of jobs? 

It seems like they were trying to suggest something, but fell short of actually suggesting a solution for smaller cities. Just have more jobs!  90s grads also had the benefit of the dotcom bubble. That thing exploded over everyone and I'm sure put a lot of those groups out of work. I'm set to graduate within a year or so and am under the impression that I can work pretty much anywhere with the IT related major I'm in. Silicon Valley would be cool but companies all over are looking for what I specialize in.  If a company were to tell a city of people "if you are to graduate from university and obtain ______ certificate, we *will* hire you" everyone would do that.

Thats not how it works, we kinda just have to pick something we think will be in need of workers and PRAY they do in 4/5/however many years it takes to graduate.

edit- ok everyone, drop whatever you are doing in college and take computer science because everyone who graduates from a CS course gets:

&gt; "Minimum ~~30,000~~ **50,000** a year to start, guaranteed,^^^^*disclaimer:if ^^^^you ^^^^don't ^^^^mind ^^^^working ^^^^120+ ^^^^hours ^^^^bi-weekly" 
:\^) As a student, start reaching out to people within the companies and industries you're interested in. Ask them for advice on direction - like specific courses and/or certifications if you're considering IT. I have 3 different people who have made just the offer you describe, from reaching out in those ways. Who do you reach out to and how? LinkedIn only lets premium members send messages to non-connections, and those general company email addresses tend to be dead-ends.    
Edit: I feel like a lot of people are missing that the original post was about how to choose the right industry/career path out of high school (one that's in demand), and not how to get a job when you're graduating college. You know all those lame-ass company sponsored "lunch &amp; learn" events that your college hosts on a Tuesday afternoon?

Go to those events. Not to mention the job fair that just about every college has at least once a year. Depending on your school those can be worthless. The only people at my undergrad school's job fairs were companies that donated to the school or were associated with alumns. Also, it was only jobs within a 50 mile radius of the school, so only one or two big names were there.

They also tend to be HEAVILY biased towards what your school's known for. My school had a strong computer science and science program, but is known for its business program. As a result it was 90% "logistics engineering" positions with 1 or 2 little tech companies. Worst case scenario, the job fairs are good practice for networking. Or practice on how to tie a tie. :) Sure, it isn't your only resource, but job fairs can be a valuable resource.  Even if the only thing you get out of going one is some feedback on your resume, they can be helpful. I was able to obtain an internship through a job fair and the same company hired me 1 week after graduation.  I'm going on 5 and half years now at the same company.  Job fairs can be career starters. Then get a logistics engineering job, and suddenly you're not unemployed! I have never heard of such a thing. My schools would have symposiums or colloquiums, but nothing like a Lunch &amp; Learn like you are talking about. [deleted] Not LinkedIn, facebook, or anything impersonal like that, because lots of people try it and those messages either get ignored or send a form letter. You want to connect with people in person before asking for something like that.

Go to events, conferences, job fairs, whatever you can to get face time. 
I'm lucky because I'm working on IT with the end goal being information security, and there are a lot of open conferences that focus on that field. I'm involved in an information security club &amp; competition team through my school, so I have meet people through that. 

Also by attending every industry and hacker/security con that I can get into and afford to attend, and offering to help at at smaller events. I've been way over my head at times, but it's also pushed me to learn way more than I would with school alone. One of my goals is to actually present at one before I graduate, but I don't really have anything I consider advanced/interesting enough yet for that.

The 3 people I'm still in touch with, who have made future job offers contingent on certain certs &amp; education, I met though:  1) A sponsored cyber-security competition event for college students - I ended up taking one of the reps from their career fair out to lunch and she gave me a lot of advice on narrowing down my interests and focusing them on a career path. 2) A capstone project that seniors in my school do for local business leaders, that includes a round of interviews. I was just there to volunteer and help with the set-up and photos, but ended up interviewing as well and snagged an internship and possible position once I have my 4-year degree. It also got me a temp-contract position for an office with this company when they were doing some migration. 3) Reddit &amp; cons. I used to be more active on a subrreddit focused on my field, posting my tentative plan for school, certs, and learning, and had one person PM me with a lot of advice and a link to his LinkedIn profile. We met in person about 2 months later at a con, and have met up a few times since. I've had six jobs since graduating college.  I got one by going to my school's career fair, one by applying online, and the other four were all because of someone I knew.  Once you work in the industry, you'll get to know people, some of them will leave, and often they'll recommend you if you want to work at the new company. The thing with IT and CS is that its really easy to get a job compared to most other industries. Pretty much every IT, CS, or engineering major I went to school with didn't have trouble finding a job.  Ah yes, the cyber security competition. Nerf guns and real-time intrusions. Be on the lookout for networking opportunities at your school; join clubs related to your major or interests, go to career fairs, look for internship opportunities through teachers, the career center, and older students.  It can be exhausting because you really have to hustle sometimes, but networking really pays off for years to come.  

Go to vistaprint or similar and print off 100 simple business cards with your contact info.  If you're not using your school email, make sure you have a professional-sounding alternative.  When you meet people who have cool jobs, work at intriguing companies, or have connections you're interested in, ask to exchange business cards, then look them up on linkedin or send them an email asking to meet for coffee or something.  Some people won't want to take the time, but a lot will, and if you come prepared by doing your homework, having a set of questions, and acting professional (not necessarily dressing professional; that's up to you and your take on the situation), you will leave a pleasant taste in their mouth and they'll remember you the next time a colleague or friend mentions some internship, grant, or other opportunity. &gt;Who do you reach out to and how?

If you're a student: 

1. Reach out to professors (at your college) who teach in or around the field you like. They have a network that most students don't know about. Send him/her an email to meet during or outside of office hours. Ask them how they reached their position and what they like or do not like about it. Ask them if they can give you advice to where you want to go. Hell, you might not even know where to go... ask them that.

2. Once you get to know them more, ask if they have any opportunities you can help with (a) for research, (b) for experience, (c) to obtain supplemental understanding of the field. If they say they don't, ask if they know anyone or can connect you to another professor who might have some opportunities for students. If that fails, ask if they know anyone and would be willing to connect you with 'outside of the college setting.'

**This can be accomplished within 2 months given consistent contact but don't overdue it. That is, at least meet your professor(s) once or twice a month. Once a week is okay, assuming you're already seeing them during class. You don't need to suck up to them... just be honest about your circumstances, your worries, and show them you want to succeed. I found a company near me that is doing what I want to do when I graduate. I did a bit of research and found the email address for the lead engineer on a project that sounds really interesting to me. I sent him an email letting him know I am a college student that wants to learn about what his company does and what his specific job is to see if it would be a good fit. I offered to buy him a cup of coffee and said I'm not looking for a job or an internship, just want to meet and ask questions. We have a meeting set up for next week.

I expect to come out of that meeting knowing two things, what sort of people the company hires (background etc) and whether I actually want to work for that company. I will also find out what kinds of classes would be most helpful. That sounds pretty smart. Good luck with your meeting! You have to go out and network with people, electronic outreach will likely not cut it. Also LinkedIn frequently does one month free trials, you could sign up for one and reach out to some folks. There's a lot more you can do. Problem is, what 18 year old knows to do this stuff (or has time to do it)? Nobody tells them these resources even exist, and this information/skills are almost never included in curriculum.

- The US Bureau of Labor Statistics compiles information about salaries and which jobs are projected to grow in the future.

- Find a mentor in a field you're interested in. Use them to get information about new trends and skills to develop.

- Apply for a co-op with a company (co-ops are admittedly unpopular in the US, but there seems to be some interest in reviving them).

- Find outside projects or competitions to participate in. For example, the US government and many organizations sponsor design competitions for building apps, websites, and innovative products. Many art communities have competitions as well.

- Join a local professional organization. Attend their events, learn what skills may be valuable to develop, develop a list of contacts.

So, even though there are a lot of resources and options, I really do sympathize with college students and young adults. I thoroughly believe that the current economic troubles are due to huge systemic failures of the education, social support, and economic systems rather than simply "lazy, ignorant" students. &gt;Thats not how it works, we kinda just have to pick something we think will be in need of workers and PRAY they do in 4/5/however many years it takes to graduate.

You work internships during the four years. You don't have to wait 4-5 years, graduate and then apply for a job. 
You should be applying for a job while you are in college, so you can find out what companies need and thus adjust your coursework accordingly.  Tech jobs remain in high demand to this day.  The 90s "bubble" was a pale shadow of the modern tech job market.  We haven't reduced our usage of computers or need for developers, sysadmins, and tech support since 2000.  Quite the contrary. Salaries dropped significantly, and required qualifications went up by a lot. Utterly untrue about help desk and tech support.  Baby boomers are retiring and companies aren't running huge IT Budgets anymore trying to teach them how to use a computer.  We've cut our firm back from a high of a 12 person IT/Help Desk team in the 1990's to 4-5 today.

Sysadmin, same thing.  Our onsite data center team from the 00's was laid off and everything moved to a colo cage with 1000's of other businesses using the same datacenter support team.  



   &gt; Sysadmin, same thing. Our onsite data center team from the 00's was laid off and everything moved to a colo cage with 1000's of other businesses using the same datacenter support team.

A lot of people think tech is safe from automation, but its really not. Right now tech is growing faster than its automating out. I'm a software developer and I can do a job that would of taken a staff of 6 just 10 years ago. Better software tools, cloud infrastructure, automated testing means you can do more with a lot less now a days. If anything, trades are harder to automate than technology. &gt;That thing exploded over everyone and I'm sure put a lot of those groups out of work.

In the dotcom bubble you had people with high paying jobs that had no business working in tech.  It was a needed culling.
 [deleted] [deleted] [deleted] &gt; no business working in tech

Or companies that were way over-valued and idiotic. Oh I'll put a sock puppet on a website! That's GENIUS!  [deleted] [deleted] [deleted] I'm so torn. I don't know how to get involved with this, and I don't know if it's reliable money, and I don't know if I might regret it later, but it would make my life pretty objectively better. As an 18 year old with 0 experience, I'm just the person to ask!

Make sure prostitution is legal there, buy Mace, place ad on Craigslist, always get them to pay first, pretend to be super interested and be like "I wish I could stay another hour but I ***need*** to go make more money." &gt; companies all over are looking for what I specialize in

[map of "all over"](http://i.imgur.com/UWcDmxW.png) Hey, he never mentioned writing anything in Java... Today's grads look for jobs within commuting distance of parents house.  Also not surprising, since so many grads find themselves not finding a job by graduation and moving back home to live with mom and dad while they hunt jobs. And more students grew up in big cities than in small cities because big cities are bigger. Thus, grads are heading in a higher percentage towards big cities. Logically it makes sense, someone would have to run the actual numbers. Well I think you would first have to consider what percent of the population of big cities actually go to college. I would assume it is much less than surrounding suburban areas. I bet this trend is due to the gentrification we're seeing in so many large cities around the country. I can say for sure Philadelphia is way more white collar friendly than it was in the 90's. It makes sense that the college grads follow. If this article is correct I am living in a booming metropolis!   Come on kids, come to North Dakota!  If you close your eyes you can imagine it's South Dakota. 

In all honestly the job market is nuts up here.   Walmart will start you at 18 an hour.  Combine that with a very cheap cost of living and you can be doing alright.   Seriously everyone is hiring up here.  Just don't expect to see lakes or hills ever again.   &gt;Just don't expect to see lakes or hills ever again.

Well then...what the hell is in North Dakota?! Wheat fields and dry, dusty plains. How's the internet? It really depends. Almost anywhere has some Internet. Better Internet in bigger cities. Well, relatively bigger cities. Like Bismarck or Fargo or grand forks or like, Jamestown. I pay 50 for a 50 mbps connection. If that helps any.  *gasp*

It's him... Ask it something. This makes for quite the iteration of /r/beetlejuicing &gt;We're headed out west. We heard there's some internet out North Dakot-y-way. Have you ever been north of the wall? It's like white walker country  Except the white walkers come down here to shop every weekend.  They're horrible drivers.  I blame the dead horses. There's a road that leads directly out of North Dakota. It's pretty much the biggest attraction.  Fraking wells. (http://www.ceres.org/industry-initiatives/oil-and-gas/gas-flares-from-space)

Also, six months out of the year, snow. Holy crap that is depressing People think in joking when I say you can read a book outside at midnight in North Dakota. Flying west out of Minot looks like you are flying over a major city, when in reality you are flying over unmanned oil wells  Wal-mart workers. Is the cost of living low?  I've heard rental prices are high with all the temp workers. No, it's not low: http://www.theverge.com/2014/2/19/5425040/williston-north-dakota-most-expensive-place-to-rent-in-us [deleted] Fargo is really reasonable.  Its on the rise but when you get closer to the oil field it goes up quite a bit.  If I was younger and didn't have a family on my own, I would seriously give North Dakota a look.  Shoot, I understand that the oil business is quick impressive and many do well starting with little in the way of experience.  But then I also heard that some folks with fast money with minimal expenses end up getting deep into meth. North dakota - come for the Meth, stay because you sold your car for meth That's Arkansas you're thinking of (or at least the state Colbert referenced with this quote) North and South Dakota actually have a growing meth problem. From what I've heard, North Dakota is much much worse though.  Minneapolis has a great job market as well. There are lakes and hills their as well.  shhhhhhh. I am from Northern Minnesota.  I miss it dearly.  North Dakota has 35 Lakes in total...   That makes my heart hurt.  Minneapolis is a large city.

15th largest metro in the country. Also, doesn't have any competition for a good 500 miles in any direction until Milwaukee. Hills, in Minnesota? Unless you mean up on the north shore of Lake Superior not really. The closest thing to a hill would be the bluffs along the rivers.  the only place there really aren't hills is western Minnesota.  All of eastern Minnesota has hills. Minneapolis is flatish but still has hills and the suburbs have a lot as well.  Maybe I just have a different perspective on it growing up in central PA. My aunt lives in the suburbs of Minneapolis and I've been all over the state when I've visited. I always felt it was a very flat state apart from the northeastern bit above Lake Superior.  Isn't [Duluth](http://exploringtheusbyrv.com/wp-content/uploads/2011/07/DownTownDuluth.gif) one giant hill? Been to North Dakota, it really isn't that bad at first glance. Thought the Badlands look nice.

How lenient is getting a driver's license there? I was barely able to get one at my home state with poor eyesight. I would like to see a venn diagram with "Cities with fast growing smart industries" and "Cities with the biggest labor markets".

I'd be willing to bet that it looks pretty close to a circle. About the only notable deviation is going to be oil field work but since the price dropped, that's being heavily impacted. The oil industry is arguably a very "smart" industry. Some of the technology and breakthroughs in the industry are on the forefront of their various fields...e.g., signal processing, remote sensing, chemical engineering, mechanical engineering, civil engineering, geology/geophysics, etc, etc, etc... Oil's got both extremes. They define 'smart' industries as those "ones that hired greater-than-average shares of college grads" which would probably exclude oil work however if 90% of the work is skilled physical labor. Oil operaters and some service companies (Schlumberger, haliburton, Weatherford, baker-hughes) all hire greater than average shares of college grads.  Oil operaters predominantly hire engineers and geologists. That was kind of my thought.  It seems like if you have a bustling city with lots of jobs and a booming economy, that economy is going to include tech jobs.  If you have a city with a lot of tech jobs, some other economic growth is going to appear alongside, since there will be a lot of money in that city.

I think it's just refuting the idea that it's sensible and sustainable to have a little city that just has a bunch of tech startups and incubators.  If you expect businesses and workers to come to your city, you need an actual city with various stuff going on. Lots of cities have labor markets, it's just that the market is comprised of jobs college graduates don't want because they pay minimum wage and they're saddled with $60,000 in debt.

Right now, you really have your choice of either working in one of the big tech areas (where cost of living is 3-4x anywhere else and your $100,000/yr starting salary barely pays for rent and transportation) or you can work in a place with reasonable cost of living and make $35k starting.

So yeah...until we solve the problem of "Entry-level marketing position at local packaging factory, requires bachelor's degree and 3 years experience, starting pay $35k", on top of students graduating with $60k in debt...it's only going to get worse. I dunno man, you can live in the suburban area of most tech centers, get paid a reasonable white collar salary and do ok on living expenses. I take a 25 minute train to and from Chicago to go to work... It isn't quite so dire as you make it seem. I found that a lot of the qualifications asked for entry level jobs at companies are extremely exaggerated for the sake of weeding most applicants out so that the people who are doing the hiring can practice nepotism or hire a business contact's nephew who doesn't have half of the qualities the job posting is looking for. Many of my friends who have graduated college in these past few years have either went straight to careers because of their successful uncle who owns a company or have left minimum wage jobs after getting in contact with the right family friend who happens to have a law firm. You should always apply anyways. Exaggerated experience requirements also weed out people who aren't confident enough with their skills to apply. I had to assist in listing some requirements for a new position in my department. Our HR dept changed it to "6 years of prior experience in the field" even when 1-2 years in the actual role and X many years in positions where transferable skills were exercised was adequate. I had originally said 1 year was fine since we have an extensive training period.  I'm sorry, but this is so stupid. I can't stand the moronic head games that many corporate hiring departments play. You're not that clever, stop trying to be. Yup. Company I work for, I interviewed someone who had direct ties to the CEO (very small company). I knew it was a bad fit, as did my direct manager, but he was hired and fired within a month. Then, to replace him, I interviewed 2 people - one I brought in and the other who was the sister-in-law of one of the managers above my pay grade. The sister in law was not a good fit, and again, both my manager and I knew it. Hired anyway. 6 months later, she was on the chopping block! Nice to know I was right about something... Here's the original study: http://onlinelibrary.wiley.com/doi/10.1111/pirs.12163/abstract [deleted] [deleted] [deleted] [List of metro areas by current unemployment rate](http://www.bls.gov/web/metro/laummtrk.htm), for those interested. Not only could you never find a job in Yuma, Arizona, but even worse is that you also have to live in Yuma. Small sample size, but the college grads I see (I graduated HS in 2011, so everyone seems to be getting their undergrad degree now) are moving back home. I am friends on facebook with about 400+ people I graduated with, and other than nurses, I know of only one person who landed a job that I would consider a career. Everyone is is hitting the bars M-F, seemingly doing nothing.  Graduated college in 2007.  Same story back then too.  Everyone I knew who didn't do nursing or accounting was unemployable for a long time. I graduated from college in '09. Loads of us worked jobs unrelated to our degree immediately following college. I've also seen a good amount of grads go back to school for nursing. Personally, I'm back and studying computer science.  In some ways, I think the advanced degree route is easier. Pre-med here, and people are always like "omgz I can't believe you want to go to school for 8 years!", and I just think, I'd rather follow a very specific path, be guaranteed a job as long as I work hard and follow this path, as opposed to finishing undergrad, having no real skill (for better or for worse, about half of undergrad curriculum is completely unrelated to the major- i.e. you're not an expert in marketing because you took a few marketing courses), then live back at home, strapped with debt, and again, having no real skill!! Much rather continue to pile up debt, pay it off when I have become master of my craft, therefore making it nearly impossible for me to not have my career going the second I'm done this process. 
 &gt; i.e. you're not an expert in marketing because you took a few marketing courses), then live back at home, strapped with debt, and again, having no real skill!!

This is the problem. Most of the younger people I know who are under or unemployed have degrees in things that dont actually help them do a specific job. Employers are under no obligation to hire you just because you got a random degree, you have to provide them value for what they are paying for. I'd say that largely, no degree prepares you for any specific job. They are more of a try-out or something.  You probably know that engineering degrees are the most employable degrees, but I swear we are ill-prepared for actually being engineers.  I would argue that some degree's do a much better job at prepping you than others. Legal, Nursing, Medical, Dental, Many Science and Engineering degrees as well.

Liberal Arts, Communication, Marketing, Business etc. Nope, not so much. Where do you get a job in "Liberal Arts?" The whole point of that degree is that it's a non-specific education that gives you a good general body of knowledge, but how could it possibly prepare you for any specific job? 

Then again, if it's anything like when I've searched for a job, I'm sure it qualifies you for an insurance sales job.  I wouldn't compare legal, medical or dental degrees to any normal 4 year degrees.

And Business degrees are pretty much on par with engineering degrees, at least from what I have seen.  We do a whole bunch of different classes and then at our jobs we end up specializing in something we touched on once in undergrad.   can confirm. graduated highschool in 2011, just finished college, moved back with parents yesterday  Maybe I'm an outlier, but I'm seeing the exact opposite.  Everyone I graduated college with is out living on their own, working careers or working other jobs that pay good money, and the only ones living at home could live on their own....but they're saving some crazy amounts of money by living at home for a few years.  graduated in 2013 and nearly all of my friends are full time or in grad school. I'm in the same boat. Almost all my friends, save a few that graduated with 2.something GPAs, are gainfully employed and have been for several years. I graduated in 2010. I went to UGA, so they're mostly concentrated in Atlanta (which, oddly enough, has one of the highest unemployment rates of major cities), but there are a few in NYC and DC.  Are they barhopping M-F because they have no job, or do they have no job because they're barhopping M-F? Yes. I have a job and I don't barhop. Who can afford that? I see this a lot too.  It's just kids that DO get a job that pays more than they were making in highschool (like giving $20 to a child, kids gonna think he's rich).  I don't think they consider the future at all either, they just want everyone to think they're doing good.  If I'm in a bar buying rounds for everyone M-F it might seem like i got the dough to do that, but people may not see that that's about 3/4's my paycheck.

I think they just get stuck in complacency.  I was so much more out of life, I don't want to be just another adult. Am I allowed to say the opposite? Graduated college in 2010. Almost every one of my close friends had jobs (maybe not great ones) in their field or grad school acceptance at graduation, mostly engineering, sciences, business, or teaching. Are there widespread instances of people who got good grades (say a 3.5+) with at least two summers of relevant internship work not being able to secure a job? If the job market is more competitive, its really up to you to buck up and compete. 2006 here. The only people with paying jobs are the engineers, blue color workers and nurses.  *collar There might be a large group of smurfs living in his area The blue man group has really taken off i definitely feel for college grads right now. i graduated in '98 -- undergrad-- people had jobs lined up before actual graduation dates.  For what it's worth, a lot of my friends had jobs lined up earlier this year before they graduated. Most of them were either in engineering or IT though. My husband is in IT and has no issues finding jobs in his field whereas I have had friends laid off that have trouble, makes sense.  The only friends I have like that are the ones doing computer science. Juniors in college and already have jobs set for them. welding engineer here: Had 4 job offers out of school, graduated 09 at height of recession.  Average about 3 job offers a month these days.  It's a specialty that's always in demand and yet no one gets degrees in it.  Can confirm - Computer Science student with a job in my field, haven't graduated yet Current grad here in the UK. 

I concur that this is true for me. Most jobs outside of London are sparse especially in IT or accounting. Retail can be found anywhere but the quantity of jobs is nothing compared to London.  The UK seems like an especially severe case. So much of the economy is centralized into London. It's kind of crazy.  And causes crazy housing prices.

 Nearly 40% of all houses sold on the market last year were paid for in cash, let that sink in a minute.

London is too expensive for Londoners. I saw somewhere an article by a guy who got a job in London and was astonished at the housing prices. 

He did some back-of-the-envelope calculations, and guessed that it would actually be cheaper for him to live in SPAIN and take a commuter plane in and out of London every day.  The article was slightly bollocks too...there was some very specific cherry picking of areas of london vs areas of spain vs cost of travel.

Think about it, if that were *really* true, literally no one but the super rich would be living in London, and thats just not the case. Millions of people live in London. They aren't all earning 6 figure salaries. I think it is just ridiculous to *move* to London. It's all old money.  If the last 2 generations of your family has lived there, then the equity has built up and you can be somewhat comfortable. But starting new there would be nearly impossible.  This applies to any old expensive city.   [deleted] I got my job because I live in the middle of nowhere.  There are good jobs out there where making $50-70k a year is quite well off for the surrounding area.    Oil rigging? Metallurgy.   I kind of used a different strategy after graduating from college.  I specifically looked in smaller cities for jobs because while there were fewer positions, there also weren't as nearly as many people to compete with.  And yes, the pay isn't as good, but at the same time, the cost of living in most smaller cities is much lower. This is why I'm moving from NYC to Orlando next year. Bonus, there's fewer people who went to elite colleges so my degree is worth more (by which I mean, actually worth something). College is the new high school. Except you have to pay thousands of dollars for it. And even when you get out the jobs you want ask for 3 years experience prior  I really think people under estimate the value of internships. A lot of kids pass these opportunities up and you really can set yourself up with good internships.  [deleted] I'll offer anecdotal evidence like the majority of thread. Graduated undergrad in 2013. 90% of my friends are starting their careers in one of the most expensive cities in the country. Perhaps there's a large amount of kids back at home with parents, but I don't keep in contact. So, from my perspective, it seems like the large parts of kids are well off.  I went to undergrad outside DC.  Almost everyone I know moved back home afterwards, I'd say only 10% of the people I knew from undergrad work and/or live in DC, which is insanely expensive.  I know more people who moved back in with their parents than who live/work in DC (like I do).  A lot of people flocked to Baltimore instead, which is far cheaper than DC. I'm just kind of on the verge of deciding whether or not to go to college at the moment just sitting around working for now. But my dad was explaining to me how back in the day if you didn't go to college you would get either a bad or unacceptable job. But if you went to college, which was more affordable, then you basically got a job either way. It's crazy how many jobs we are losing in America. Even people who go to college aren't guaranteed a job anymore the tech industry grew out of large labor market;  sf bay area, los angeles, boston, etc.  It seems that the reasons for moving to large cities may have changed, but the article doesn't provide any examples of small cities that were rich in tech that previously attracted grads and now don't.

and then:

*Cities cant do much to control their size  which was the main factor attracting grads in this study*

Cities directly control their size.  That's what zoning is for. The stock market has been reasonably strong for the 2010s. We are hitting all time highs in literally all markets and the large exchanges have more than doubled since the last true stock market depression in 2008-2009.  I know this isn't the same as job market or number of high paying jobs.  But there is no denying that corporate profits have been high in the 2010s and the stock market reflects that. 

Often it just doesn't trickle down and benefits mainly business owners and those who can afford to invest heavily.  Thank the federal reserve, it's arbitrarily keeping the stock market high from QE. If the fed stops printing money then the real state of the economy would be shown to everyone, the companies making these record profits are the ones benefiting from qe , they can borrow money from the fed for 0% interest and loan to anyone, no matter the risk for like 6% and make huuuuuge profits from it, when in reality that loan Probably would not have been done if it wasn't for the cheap money. So like I said if they stopped printing money the stock market would crash again What really sucked in the 90s was if you happened to be from one of the big destination cities and didn't make lots of money you basically found yourself priced out of the market for housing,  or you lived in sub-standard housing, or SRO hotels, or some other kind of improvisation. I did all three before giving it up and leaving SF in 2000. Now I'm seeing something similar in Portland, but this time I am a homeowner and financially much better able to withstand the influx of wealth. [deleted] [deleted] Horrible pay and worse conditions. IT workers consistently have to prove to business units why they should be kept around. The business units see them as added expenses and consistently question their worth. They take business managers and tell them to manage IT departments to make them the most cost-effective. This results in deadlines being set which are in line with the desires of the business but not the reality of IT's resources. When IT cannot perform up to the unrealistic expectations, they are told that they will have to work overtime or just get replaced with H1-B workers. Developers end up doing server configuration, OSS guys work overtime navigating licensing and contracts... etc.  IT seems like it might be fun, but the reality is that it supports people who don't really want it there.  I've always wondered what the IT workforce would look like, if IT was treated more like a trade. I don't understand the industry or work, but from an outside it seems to have a lot in common with traditional trades.  I am actually in a tech union, CWA - Communication workers of america. It makes sense, but trying to convince "salaried" folks to work hourly is rough.

Too bad, because we've fought for pensions, GOOD health insurance, and other mandated things (overtime compensation for IT anyone?) I would be for union if they would let me stay salaried. I worked hourly for last couple years for the first time since 2005 and HATED it. I prefer being paid to do a job, not being paid to man a desk.  If my daughter is sick and I have to take her to the doctor, I don't want to have to factor the cost of the co-pay and bills PLUS how much money I'm gonna lose not being at my desk.  Salary provides a lot of security for people with kids, and it's not always a terrible thing as I don't actually have to work much overtime, so it works out great for me.  

That's the whole problem with unions and tech, one size doesn't fit all. If getting those other things meant being hourly again, I'd say no too. Except the salary job you described is the absolute minority.  Most places that offer salary still require 40 hours worked, and track your hours, but don't offer overtime.  Salary for 90% of people is just a way for a business to screw them out of overtime pay. Nail on the head right here, and literally my exact situation at the moment.  

Unrealistic deadlines, everything needs to be done yesterday at least, all about cutting those costs, so what does the business manager managing IT do?  Bring in the contractors, the plague of the industry.  It takes money away from the people that are doing the actual work and pads middle men/women's pockets for "finding us", everyone I have worked with has 0 clue about anything IT/computer-wise.  Fake promises of contract to hire, the kickbacks to the contract companies last for YEARS, sometimes around 25-33% of your actual wage.  

If you are lucky enough to find a job willing to pay for it that is.  Most say thanks for the 6 months, out the door with ya.

Then there is the high turn-over rate managers get to deal with.  Constant lack of experience, training and knowledge gaps all to save a few pennies and keep the head count low.  This gets passed on to the end users, the people we are hired to SUPPORT.  Crap service, long turn around times on tickets due to no one actually knowing how to do it because we've all been with the company less then a year.

The most depressing thing I've seen are the people that spent thousands and thousands of dollars for a degree (myself included) to be working maybe $15/hr jobs, that only last 3-6 months.  I'm fortunate right now to have an indefinite contract that pays much better then $15/hr, but it took 3-4 years of grinding through the muck.  

Then there's the downtime between jobs, the relocating constantly, having to deal with the contractors themselves, many are very pushy and I've had a few ladies with Robert Half get REALLY upset with me for turning down crap 11-12 dollar positions, when my skills should pay at least double that.  

It all adds up to you feeling like a piece of meat they flaunt in front of the companies doing the hiring.  Nothing more.   [deleted] My dad was in construction and welding for 20 years.  Enjoy your back and knees while you can!  [deleted] This is kinda true too.  Working in an office 8 hours a day isn't good either.  Humans weren't made to sit for 8-12 hours a day. 

We're all screwed.  How do you even do that? I've got car payment, mortgage and such but honestly I'm not really in IT because I enjoy it, I just happen to be good at it. I feel like I'd rather be building things, or chopping trees or something so sitting here looking at servers all day. Exact same thing here, I am good at my job and do it since I make decent money. If digging ditches or some other labor type role paid the same I'd probably do it. The end of Office Space truly resonates with me more and more Are you talking about IT as in Information Technology (programming, hardware, web design, etc), a standard call center (you reference phone calls and scripts), or doing phone tech support (a specific type of the previous option).  It sounds like you could be talking about all three and all three are vastly different job types/fields with different pay and responsibilities. &gt; Today however the tables are turning and IT is now in demand but no one wants to work in tech thanks to the horrible pay. 

I'm sorry, but I just don't agree with this statement. Do you have anything to back it up?  

  
Even basic IT jobs out of college pay better than minimum wage these days--and if you studied any programming or scripting while studying IT, you can easily be earning a minimum of $20-30/hour as an entry level hire--at least on the West Coast of the US. Don't forget moronic HR departments that have no idea wth they're doing.  That pretty well describes *all* HR departments. &gt;you can easily be earning a minimum of $20-30/hour as an entry level hire--at least on the West Coast of the US.

This is a low estimate. In California you need to be paid something over $40 per hour to be exempt from overtime so it's rare to find software jobs paying less than that. Source for this? I was under the impression you just had to be classified as management.  Well, you'd have to actually be classified as management, and if you're just a dev or something and they try to do that, they're opening themselves up for a lawsuit.

&gt;To comply with the section 515.5 exemption, California employers will now have to pay otherwise qualified computer software employees a minimum hourly rate of $41.27, up from $40.38. The new rate translates to $7,165.12/month or $85,981.40/year.

[Source](http://www.tbowleslaw.com/blog/?p=2076) Go ask EA about how that worked out for them. &gt; For every sysadmin, programmer, db admin etc... there are hundreds of help-desk reps.

This is the point I was going to make until I read OPs edits. There is no shortage of people for help-desk and lower tier tech support positions. My brother currently holds such a position and had to compete with an awful lot of people for that job. With my job, of the 14 applicants (students pursuing a bachelors) that passed the pre-screening only 2 were found to have the required knowledge/skills. The shortages that everyone keeps talking about are for upper-level positions that require a bachelors level of knowledge at a bare minimum.   Yeah, having worked in tech for 15 years I can say the pay has always been great. Maybe not on the low end of the scale (tech support, entry level nerd, etc.) but I can say without question the pay is generally great. The problem has always been work environments, poor leadership, a lack of management, long hours. There are a lot of problems with IT, but pay is not on that list. I agree. The work is toxic but the pay is great. I'm putting in my time and then retiring early. This work stuff sucks. Just 20 years of experience and I'll bet an entire years salary that you couldn't land a job that pays 20$ an hour in my city in IT fresh out of college.

Know why I'm so sure? Cause people like me with 20 years doing the job will be all over it. The top IT shop in town pays 30k  a year tops and I guarantee you'll be on salary and working 45 hours a week and they won't pay that much to a recent grad.

I'm on the east coast of Canada btw. What kind of work do you do, if you don't mind me asking?

With a degree in either CS, Informatics, UX/UI, HCDE, Applied Mathematics, Business Systems, Systems Analysis, Networking, Web Development, Information Assurance/NetSec, and many other tech majors, you can easily (in comparison to other recent graduates from non-tech majors) get an entry level job that pays $25+/hour (and that's bottom of the barrel) on the West Coast.

Even students working at a public university's IT Help Desk make roughly $15 as their starting wage.

If you're sticking with entry level IT jobs within a fortune 500 company or even a startup you should definitely be making more than $20 an hour. I work for my university Help Desk. It's federal minimum wage. The FTEs only pull in roughly 30k a year.  Currently I'm a technical analyst working for the government. Fancy name for deskside support. Degrees in specific fields help but around here you need experience first as people with experience will be chosen over a recent grad.

I graduated with a networking/sysadmin degree. I have my A+ mcdst, mcitp and a few other certs but there simply wasn't any sysadmin jobs being offered to people with no experience.

I've worked for schools,  call centers,  fortune 500 companies,  xerox, IBM (worst of them all) until I landed the job I have now.

My current job pays double anything else I could find in the private sector. 

Oh and the kicker is that even with all my experience the main reason I got this job was because I know other languages.  The east coast of Canada isn't known for its high paying tech jobs...did you really expect anything different? If you had moved to toronto you'd make way more That's really interesting to learn. I knew, obviously, that geography plays a role in wage dissemination, but I suppose it has a much larger effect than I had previously imagined. Thanks for sharing. &gt;  that pays 20$ an hour in my city in IT

That is a important qualifier. 

What city are we talking about?

 Yeah... people always forget about this.

"I make 30 per hour right out of college"

"Jeez, I only make 20, I should go where you are"


People dont mention the 1200 studio apartments  If you aren't making a good wage in IT 5 years out of college, it's you. Bottom line. Truth. It's so easy to get comfortable in IT. I loved my first job and learned a lot but there was also a lot of down time where I got to screw around. This was five years ago now and I've moved on thrice, the most recent stint taking me out of my comfort zone and into new territory within the field. You've gotta make moves like that if you want to advance your career instead of stagnating. I turned my back on IT. I was one of the people that landed a call center job, along with the company doing more than they can chew. Very stressful job, difficult, not very rewarding, and I hated it. I walked away from the job and now I engrave urns. I love my job. [deleted] [deleted] Don't believe the myth that finding decent job in the 90's was easy. It was not. I guess those 90's grads are still in those jobs and not a lot of new ones have been created.  Pretty sure it's still easy to get hired with an engineering degree tho. I can confirm it is easy to get a job as an engineer. I didn't go looking for a job. Companies contacted me trying to hire me. I guess my professors liked me. When people called them asking if they had any good students my name came up. I graduated in December and now I work for a private contractor at a large NASA facility. I get paid well over the nation average for a new grad with an mechanical engineering degree. While the pay is very nice, the best part is I love my job. I came to Connecticut after graduating from Minnesota in 2010 with engineering degree. I was going after that "literally any engineering job anywhere" and I landed here and haven't left yet. its hard to move to these cities when you dont have any money due to student loan debt Why I moved to Toledo.  Found a job cold calling first day, 2nd day went in for interview and got it.

In WV I couldn't find a job in 3 years.  Can confirm. Just graduated with MSME and BSME and will take literally any job that pays a certain, pretty low amount for what experienced people say they started out at.

The career lady at my school said that people from our school with GPAs .5 lower than mine in my major should expect to apply to 20-50 jobs and if you are a foreigner you MAY have to apply to 100 to 200 jobs.

I have been actively searching since January and have applied to 22 jobs today...I don't even care about the city at this point as long as they have one decent gym and grocery store. Those frappuccinos aren't going to make themselves. [deleted] [deleted] [deleted] [deleted] [deleted] There was a guy working at Starbucks who would berate college students as it was a waste of time and money because he has a BA in Marine Biology and couldn't find a job (in a landlocked state, in a small town that doesn't even have a public aquarium, 250 miles from the ocean). Finally people are getting it. Being sentimental and close to your hometown is not always the best approch to become employable. That's what I'm doing. Living in Louisiana with a BS in psychology. Job makes just over 20k a year.

Moving to a bigger city in upstate ny and applying for literally any job that makes more There are more job seekers than available jobs. And those jobs aren't even restricted to local people, the entire world can apply. this is the worst article ever.   they don't even mention the cities? Sometimes I think the inability to get a job comes down simply to people not wanting you around. There are so many qualified applicants that it becomes a very simple thing to choose the person who best matches the supervisor's personality for competitive jobs.

I just need to find a hiring manager who is fat, cynical and anti-social, and I'd get a job in two seconds.  Most college grads I know got a job *before* they moved I graduated in December. Spent a few months unemployed but found something in NYC. Been working there for about a month now and just recently moved to the city. When I was interviewing a lot of the places were in New York, because that's where the jobs were. This is not actually new information for the brain.  It's new information for the hippocampus, but stable dendritic spines (synapses) have been documented n the cortex for many years now.

Holtmaat, A. J. et al. Transient and persistent dendritic spines in the neocortex in vivo. Neuron 45, 279291 (2005 What happens when I've completely forgotten something for years, but then one day something makes me remember? I'm no brain scientist, but my guess would be that the synaptic connections for the forgotten memory exist, but are unstimulated. Possibly just temporarily unreachable. Retrieving a memory probably involves a chain of several neurons. Any synapse in the chain becoming disconnected could impair retrieval. [deleted] [deleted] [deleted] I'd say less of a chain and more of a cluster. Many neurons would lead there, but they are also connected to other places. The individual connections for that particular group would weaken over time, but reestablish themselves if you can get the cluster to fire up. You could have one missing connection in that cluster and not be able to reach the entire memory that way, but find it through a different connection, which can help the missing connection move back into place and freshen up/reinforce the entire web of relevant connections. OK,  what about when you are blacked or drunk abs have no memories?  When you're black out drunk your brain, specifically the hippocampus is not forming new memories. So when you wake up the next morning its not that you forgot what happened, but rather the memories were never formed in the first place. I wonder if thats the difference with people who have perfect recall - those neurons and synapes remain permanently connected?

I'm incredibly jealous of them! Could groups of synapses be "archived" by simply being disconnected? And through some continued stimulation be reconnected? I think the problem with disconnecting any neuron(s) is that the behavioral/cognitive response, given a mapped input, is not dependent upon the neuron(s)/synapse(s), but that neuron's relationship with all the adjacent neurons and their adjacent neurons and their adjacent neurons...

Any one who studies neural networks in CS can see that it's the unique chain of signal sums from the input layer all the way to the output nodes that determines the response -- there's no real "information" in the individual nodes/weights of the network...I can imagine it's only more intricate of a system of dependence when you account for glial mass, neurotransmitters, etc.

Tl:dr -- Any given set of neurons rely too heavily on their vast network of connectivity to brain regions near and far to deactivate them. If you were to find the specific path of neural activation that corresponds to a given behavioral/cognitive pattern, and somehow preserve it through inactivation, you would probably disrupt the response mappings of a bunch of other inputs. Technically, then, you hadn't forgotten it.  Not if we mean "forget" to mean that the synapse connections are no longer there.

Instead, it would be more accurate to say you "hadn't remembered it for a long time".  

Remembering long-forgotten memories is due to summation of activation. Let;s use an oversimplified example of how this works.  

Say you have one neuron which needs activating to remember a memory about walking your dog in the park while listening to radiohead when you met a pretty Japanese girl.  Once this neuron trips, the memory floods back.  Problem is, this neuron isn't tripping because a strong enough signal hasn't hit it in a long time.  

Thinking about stuff that's *related* to the components of this event will help activate it, though.  Maybe you can't remember the event, but you remember radiohead. You remember Japanese girls exist.  You remember your dog, and the park.  When enough of these 'related' schemes are activated simultaneously, suddenly you remember the event.

This is why you might not remember, say, who Louis XIV was when you're trying to talk to your friend, but when you're in your history class surrounded by similar cues to the moment you learned this particular fact, suddenly it's a cinch to recall.  With all these similar schemes in the network of the memory getting "current", as it were, there's enough power in the circuit to trip the memory.  That sum of current is referred to as summation of activation, which I mentioned earlier.

A memory which was truly "forgotten" can't ever be remembered, because the connections between the neurons are gone.  (Interestingly, memories which are remembered become increasingly less accurate with each remembering, so "almost forgotten" memories like you described tend to be the most accurate) Is your last part due to memory being more reconstruction than recall? It's more that recalling the memory uses the same mechanism as laying it down in the first place does; as a result, over time the memory gets corrupted. The information is there and coded into your brain. You either have not had to retrieve it, or failed to retrieve it. Most memory issues seem to be failure of retrieval, even in cases where head trauma leads to retrograde amnesia. [deleted] You kid, but it IS actually kind of analogous to having the file contents intact on a disk, but the file system's FAT or equivalent has become corrupted and the contents are no longer indexed. That's incredibly interesting and makes a lot of sense. Thank you.  source? (I'm intrigued)  I'm actually teaching a course on Learning right now.  Here's a study showing that a 'lost' memory was recovered if the context in which the amnesia occured was reintroduced - http://www.ncbi.nlm.nih.gov/pubmed/17918418

Basically, rats were trained to be afraid of a box because they experienced shock in the box.  Then, the fear was extinguished (they learned they did not have to be afraid of the box).  Right after extinction of the fear, hypothermia was induced in the rats (hypothermia is a classic amnesia-inducing agent).  The next day the rats were tested to see if they feared the box, and they DID fear the box, meaning that they forgot the extinction.  At this point, we don't know if the rats forgot the extinction because it was never encoded in memory, or if it did get encoded and the issue is a failure to retrieve the information.

In experiment 2, they did the same thing but cooled the rats down before testing on the day after the hypothermia to see if they feared the box, and they DID NOT fear it.  This shows that if the context (being cold) was reintroduced, that 'lost memory' of extinction could be recovered, supporting the idea that the issue with amnesia is failure to retrieve information rather than failure to encode it. so you're telling me i have to go back to reddit to remember things i've learnt on here but forgotten? well played reddit.... You just forgot the "path" to that memory. Not the memory itself. The brain is like a database. The brain doesn't actually store memories in much detail (i.e you don't literally store "images" of things, you store the abstract information related to your subjective experience of them, that's reconstructed visually when you access it); the way it produces such vivid/accurate detail when necessary is through associative connections. I.e a memory of a room would be stored as a collection of abstracts; the objects contained in it, their positions, things you noticed about the way it looked, etc. This is the reason that memory degradation doesn't produce literal "holes" in memories of scenes (as in a hole obscuring part of the image you picture), rather you simply recall the scene incorrectly or missing certain components, sometimes with a vague sense of it being incomplete.

So in other words, when you recall something, you're typically not pulling an actual image out of "storage", your brain reconstructs the memory in your imagination as an image through the inverse of the process used to perceive visual information received from the eye. In other words, the brain just makes stuff up and tells you it's real.

Which is why memory is unreliable. Try to remember of how you learned about 9/11 and there is a good change it didn't happen as you remembered:

http://www.scientificamerican.com/article/911-memory-accuracy/
&gt; From that first survey to the second survey a year later, the overall consistency of the details of how they learned of 9/11 was only 63 percent. At the third survey, three years after the attack, consistency was 57 percent. So people were only a little more than 50 percent right for a lot of the details.

Despite that, people will feel very confident that their memory is correct because, well, they remember it. Trigger of that synapse firing pattern Trigger of that synapse firing patter.  So how can I make it so that I lose my undesired memories? [deleted] So, memories are actually continually reinforced every time you recall them and think about them, and every time you recall them and think about them in a negative way, you're actually ensuring that they hang around and bother you.  The right way to resolve them is to recall them, talk about them, and try as hard as you can to think about it in a positive way.  People have talked about using MDMA therapy for exactly this purpose, because it makes people comfortable talking about the worst things that ever happened to them, and they can find a way to feel okay about it.   This kind of reminds me of the auditing sessions that Scientologists go through. There may be some truth to what they are trying to perform on people, but it is definitely taken to the extreme and can be very damaging in the end, since the auditing is commonly used to discover a person's deep secrets for blackmail purposes. Auditing is an extremely primitive form of talk therapy. It basically follows the same pattern as Confession. Expressing your worries and fears externalises them, and allows you to consciously tackle them from different angles. I've found this works. Also, if I have negative attachments to a place or thing, I try to re-experience it in a positive light.

Of course, this isn't always possible, but when it is, you can really feel better about things. [deleted] [deleted] [deleted] [deleted] Does this mean that we are closer to treating or preventing memory loss/ will this impact Alzheimer's research?
 As I understand it, this will not impact Alzheimer's research. memory loss in Alzheimer's occurs to to neurofibrillary tangles and the resulting cell death.  the most promising field of research for Alzheimer's is targeting the gathering and coalescence of the amyloid protein. This research helps explain why neuron death causes memory loss but not how to stop it, Ah ok thanks! Shouldn't it be the other way around? Memories last as long as the synapses stay together? Not necessarily the other way around, it works both ways. This is just the direction they examined it in for this particular study.  Your right but I think we also must be careful to distinguish direction of examination from direction of causation or whatever formal relation brain states have to mental states. the brain states and mental states may have the same temporal persistence but there may be a direction of dependence. 
 Right, but in this case, I don't think the authors implied either directly or indirectly that there was a causal direction. They had to start studying this connection in some direction, and this is just where they chose to begin studying it.  I'm sorry, what? They may last the same amount of time, but does the length of the synapse determine the length of the memory or does the length of the memory determine the length of the synapse? 

He was saying it's important to study this phenomena from both angles instead of just the "direction" or "angle" this study observed it from. Since memory itself is an emergent property of a systems-level change, which itself is a result of cellular changes in response to stimulation, the length of a "memory" is directly due to the length of changes at the cellular level. Thus, if the changes revert back to baseline at a cellular level, the higher order properties will also return to baseline.

This is of course not entirely true if the memory trace becomes encoded by another network, but regardless even this requires some continual long-lasting change somewhere in the nervous system. Obviously this is a major problem, trying to look from the non-physical toward the physical. "Excuse me, memory, can you hold this flag when you cease existing?" The only thing available for observation is the physical.  &gt;The only thing available for observation is the physical. 

You can also use introspection, although obviously it has its problems and I don't know if it's applicable here. That would be the main issue. How can you notice that you lost a memory at the exact moment you lose it?  How can our mind's eye be real if we don't remember ever having one or fully believe it's real? Yup. Cosmic joke. How can the only microscope you have access to observe itself enough to prove it's own existence? As dumb as the meme is, your take on it is actually perfect. [deleted] [deleted] [deleted] Cause and effect. 

Title says we if we have memories, that the synapses must be that old. 

This guy/girl is saying, 'but what's the cause?' Do we have memories because the memories are that old, so the synapses have to be the same age... 

... Or do we only have memories where our synapses have survived long enough to still exist? 

If we don't have a synapse there to physically recall the memory, then the memory doesn't exist. There is plenty of things we don't remember. It isn't unreasonable to assume that the lifespan of the brain tissue is the cause, and the memory is the effect... Where the title suggests the opposite.  Agreed. If losing memory caused the synapse to disconnect, that would imply that something else entirely was driving memories and that synapse state is a byproduct. If synapses store memories, then the correct interpretation/statement would be that memories last as long as synapses. Ehhhhh  . . .

Memories could also swap between synapses. So forgetting the memories definitely means the synapses are gone, but it could be that you still remember something and the synapses are also gone, (because it's been stored in different synapses). 

In that way, memories could last longer than synapses, (but not vice versa), and synapses could still store memory. 

Since we know that we re-write memories somewhat everytime we recall them, this is very much a possibility.  Aren't memories re-stored again after recollection? As in loaded in RAM, used, and saved back to HDD in a different sector? As i understood it this was a cause of mistakes in memories. This hits on the question I had:  recalling memories tends to reinforce them so that they last longer. Does recalling a memory create a new memory, or does it reinforce the existing synapse? 

I know recap presents a lot of opportunity for error to be introduced but that errors could still happen in the reinforcement/renewal whatever of the existing synapse.  It doesn't create a "new" memory but... adjusts it a little. When you remember a memory, you're remembering the last time you remembered it, not necessarily the original memory.  Y'all some smart mother fuckers up in here. Could the brain also store a memory twice,  thus if one connection disappears, the memory persists, but the memory only disappears if both connections do?

EDIT: I mean in two different places. I can't even answer your first question factually but now I am thinking that what you described is memory reinforcement.  Each time you recall a memory in a particular way it has slightly different connections?  I'd also think that you can no longer access a memory if all the connections were cut off.

Edit:  Did some thinking, to answer your original question I'd say the memory would be inaccessible if it has no connections.  Also I made a picture of what I imagined from your comment http://imgur.com/q42poxS.  Does memory necessarily have this kind of direct correlation? Are some neural networks not reinforced over times perhaps? Muscle memory isn't episodic in the same way that sensory memory is... Or is it? Surely it would work better for the brain from a registry, access and redundancy point of view to reinforce pre-existing memory paths, rather than laying new tracks for every single stimulus? Is this not a function of dreaming perhaps - to concretize important stimulus impressions and form associations for future addressing of memory blocks? Write the important stuff to hard storage and flush the buffer for use the next day. Which is why long term memory suffers when we are sleep deprived. Well, that's enough wild speculation for today.  I don't get this.  Isn't it the synaptic connection (modulo redundancy) that allows the memory to exist? It's like implying consciousness causes the brain to exist. Both ways if they're directly tied of course, but memories are a reasonably accessible bit of data that we've been observing/measuring since the beginning of time (*EDIT: beginning of memory) whereas synapses are much more difficult to analyze, especially in a living creature.  This is the reason behind the wording: memories are an existing known quantity (if with many unknown details) in neuroscience whereas a synapse relationship to them is newly reported.  This is necessarily how it was investigated and therefore how it was reported. check out the concept of neuroplasticity, how experience changes the structure of parts of the brain related to to that experience, with examples given of taxi drivers in London (https://www.ucl.ac.uk/spierslab/Maguire2006Hippocampus) One of the clearest memories I have of being young is sitting watching a specific episode of Tom and Jerry whilst eating a sandwich made of that plastic dairylea square cheese and milk roll bread. I took a bite and my tooth came away.

Is this research saying that an actual physical part of my brain has lasted the past 25+ years with that specific information stored within it?

 The most likely explanation, is that you keep recalling this event from time to time, forming a new connection. This connection is slightly different from the original, and it's a memory of you remembering the event, not of the event itself. This is why memories are so inaccurate, especially as time passes. It's also why an event you recall a lot would be easily remembered, while it would be difficult for you to recall something you haven't thought about for a decade. What about when you recall a memory that you haven't actively recalled before? I remember damn near everything in pretty accurate detail if someone gives me a jump. A vague start like, "Pre-school" will send me down remembering things I've recalled, but a nudge in a more specific direction, I can still recall so much, even if I haven't actively before.

Are those connections still alive and well? Or at least somewhat? 

I have scary-good memory and make a point to remember all sorts of stuff, thinking it will help my brain keep longer... [deleted] Lucky you, I have very few and I mean very few fleeting memories of high school even and I'm only 24. This seems to be the most accurate interpretation. I've noticed in my life that every time I recall a distant memory, the details become a little fuzzier each time I try to recall the same memory.  &gt; It's also why an event you recall a lot would be easily remembered

The T+J episode is one of my favorites to this day (the 'zoot suit' episode if anyone is wondering) and when 'what is your earliest memory' style questions come up it's the memory I kind of default to, so that makes sense! I live that episode! I always found it so funny when Tom uses the coat hanger (I think) to give himself broad shoulders. I'm also interested in this. I have similar memories of the distant past, but I'm wondering if these memories are reinforced by memories of when you remembered it? For example, if you keep remember that point in your life, for years down the line, are you actually recalling the memory itself or the memories of when you remembered it? If that makes sense?

It's an interesting prospect because then it could account for exaggeration or fabrication of details in distant memories. Nostalgia becomes a game of telephone with our memories, where we think we remember this long term memory vividly, but it's only a fraction of what is real because of how many times we've revisited the memory and reshaped it through a memory chain kind of deal. Could this maybe explain physically induced amnesia? Like, the synapses breaking apart by physical forces, erasing the memories? Or benzodiazepine and alcohol related black outs? Are those memories never even created because the synapses never made any connections? Then how would you explain people regaining their memory?
 Places in the brain reconnecting to places that have these memories. Is this the same as actively recalling a memory, where the "memory" is accessed, broken down into parts, and re-assembled?  My guess is no. It sends a signal through that synapse, which triggers the memory in active part of your brain. No, our brain is dynamic, but not so instantaneous where conjuring up memory is actively creating novel connections But how does the brain know how to reconnect everything back? it isn't only an on off switch. the connections between neurons can be weakened or strengthened. Imagine an action potential traveling down an axon where it hits a weak connection and the energy from one side to the other is not strong enough to overcome the weakness of that connection. Every time that connection is accessed, though, it is improved and strengthened by cell processes.

So if you force yourself to recall a memory or think about it long enough to force impulses down certain neurons, those connections get stronger and eventually you improve the ability to recall the memory that was once lost. &gt; Imagine an action potential traveling down an axon where it hits a weak connection (...)

I am trying. If only I knew what those words mean. An action potential is an electrical charge that travels down a neuron, triggering an electrical or chemical signal that passes from one neuron to another. The axon is the "long" part of the neuron; it is responsible for sending signals to other neurons, where they are received by the next neuron's dendrites. Axons send, dendrites receive If something is built and broken (partially) it is often easier (requires less energy) to rebuild then to build something completely new and different.

I know little about the brain but this is a consistent theme in nature. The path of last resistance.  It probably doesn't "know", which would explain why some people get memories back randomly, some are triggered, and some never come back.  Sensory input in another part of the brain leaves a sort of trace maybe? Like, say like you get really good at putting square shapes into square holes, but you start working on circles. You are still very familiar with shapes, you just haven't put squares in in a while. But once you come back to it, all the thoughts that are associated are rekindled or something.

Like with mnemonics, you associate information with as many things as you can to increase your retrieval rate, its how people memorize 100 digits of pi, unless they are savants. Parts of memories might live on in other synapses, only waiting for you to re-engage the one the 'cap' of the hierarchical pyramid, if that makes any sense. This is just a guess though. If the road is closed, the town is still on the other side. Probably those issues are related those connections, while the memories are fine.  I like to think the brain is a really complex computer. Loosing memories is the same as defragmenting a hard drive, which just removes the "pointers" of that data. It's still there and working, but it can be recovered with the right "software".

The "software" in question can be as simple as providing the right stimuli. Given the distances involved with neurons, a small disturbance could be distance enough to disrupt the signal, but close enough that it re-attracts to the appropriate location (which  is how learning happens in the first place [fire together/wire together] AFAIK). If this is accurate, it would mean that: 

1) partial memory recovery would be far more common than a complete recovery 

2) the area of greatest loss would be the area closest to the site of the trauma

if these things are seen in cases of physically induced amnesia, I would suspect that /u/flurreeh is correct. The fire together/wire together learning thing is Hebbian theory which is mainly concerned with synaptic plasticity, that is the neuron connections changing their strength rather than all new connections forming which is Synaptogenesis which is more common when growing up then in adults.

Our knowledge on Synaptogenesis isn't as detailed as normal synaptic plasticity/learning so there's not as many theories on how that works. interesting, thanks A lot of times you get bits of memory back after an accident though, unless they are repaired or there's a 'backup' of memories elsewhere that your brain can access. It seems like this only covered episodic memories in the hippocampus. I would kind of be interested if the same is true for semantic memories from the frontal and temporal cortexes since generally you don't lose those forever in accidents (i.e. you'll forget your name or who the president is, but then it comes back to you). Synapses are just the space between the pre-synaptic and post-synaptic neuron and the relevant structures around it (the actual gap between the two neurons being called the synaptic cleft). Synaptic plasticity is the process by which the post-synaptic neuron increases the density of AMPARS and NMDARs (receptors that act as ion channels). This increase in receptor density increases the "synaptic weight" between the two neurons and allows the pre-synaptic neuron to more easily bring the post-synaptic neuron to threshold and fire. So a memory is essentially a bunch of synapses changing their synaptic weights over many different brain areas (different types of memory stored in different places, all coming together to form a certain memory). This is called the neural trace, which is spread out over the brain with some areas more active than others (like in fear memories the amygdala will have more of the neural trace within it).  So if a memory is a memory of a previous memory, are they held in different synapses or is one getting refreshed/ altered based off how the previous one was stored? Seems logical. The more you remember something, the easier it is to recall because of how many different synapses it is held? This might also explain why distant memories are exaggerated, because it's a chain of memories rather than recollection of the original memory.

Someone smarter than me should clarify though! So, Inside Out holds some truth? Does this mean Pokmon have, at most, four intact synapses at any one time? I got into an accident with a semi last week. I have a severe concussion and a week worth of memory lost. I have several snippets of memory from the day before the accident and the hour before. 

Any idea why I would have those small bits, but the rest is blank? The parts I remember are very mundane bits of my day, nothing to make them more meaningful than the rest of the week. Maybe there's a process that needs to happen to reinforce the connections/memories? Like hitting a semi? Whatever the opposite of that is So if i take the time to remember everything I can every day, will that make my brain synapses stay intact longer and give me greater cognitive ability when I'm older? Yes. There are some autism people who do this. This may be a dumb question, but could this be used postmortem? By analyzing the synapses of someone recently deceased? Is that even possible? I guess it'd depend on how fast decay set in.  I feel like the brain would go all oxidized really fast. A synapse is incredibly tiny, and there are over a trillion in a human brain. It's not just one cell connecting to another - a single neuron can connect to ('synapse with') thousands of other neurons. So this would be impossible to visualise to such detail post mortem! Ohhh okay that makes sense. Thanks! Very recently, I was reminded of a couple of things from my past. Things I haven't thought about in 20 or 30 years. interactions that were forgotten in the matter of a couple of hours and definitely by the next day. I don't see how those connections were held in my brain for me to remember those situations after 20 or 30 years.

I mean remembering everything from the color shirt a guy was wearing to the time of day to the length of the interaction to who was there. the entire thing came back. 

And I have a hard time believing that those connections were reserved, in full, the entire time that I had completely forgotten about it. Aren't synapses the space between two neurons? Not quite, that's known specifically as the synaptic cleft which is around 20 nm. The synapse itself is a structure composed of a presynaptic neuronal terminal "connecting" with a postsynaptic neuronal dendrite spine (or cluster of receptors). There are also astrocytic/glial components to many synapses. So if I want to remove a memory, do I sever the individual connections? 
Eternal Sunshine of the Spotless Mind....anyone? Question, I read somewhere that a lot of memories are actually memories of memories. When you remember remembering something instead of the original memory but you don't always notice.

If the synapses last as long as the actual memory, how often does it occur that memories they're replaced by these memories of memories? What you are getting at here with so-called 'memories of memories', is that each time we recall a memory, we alter it slightly. There are numerous physiological, psychological, and neuropsychological reasons as to *why* we alter the memory upon recall.

Here's a simple (slightly overexaggerated) example. I'm recalling my trip to Paris, and telling the story of my visit to the Eiffel Tower to a friend. I mention that I was approached by a scammer. The friend asks me 'what was the scammer wearing?'. I can't quite remember, but I *think* he was wearing a black leather jacket, so that's what I say.

The next time I recall the memory, I can vividly remember the scammer wearing a black leather jacket. In this situation, the memory has been altered, whereas before I couldn't quite remember what the scammer was wearing (let's say he was actually wearing a blue denim jacket), now I can vividly remember the scammer wearing a black leather jacket.

Basically, you **don't** store two distinct memories (e.g., actual memory &amp; memory of memory). However, you alter the actual memory each time you recall it, which makes it further and further from what actually happened. Human memory is unlike a computer, memory is not a certainty, rather it is an interpretation! &gt; The challenge to studying synapses in this region is that the hippocampus is so deep and the connections so densely packed that no microscope could easily monitor the synapses' longevity.

&gt; Now Mark Schnitzer, an associate professor of biology and of applied physics, has leveraged microscopy tools developed in his lab and for the first time was able to monitor the connections, called synapses, between hippocampal neurons and confirm what neuroscientists thought might be happening...

I think this should be the focus of the discussion, not just the synaptic duration result.

Think about it. The ability to monitor hippocampal neurons. Do you realize the implications? One day we'll be able to analyze our very own memories with a computer. The thought might seem too romantic and farfetched for some, but this is the first time that we've succeeded in analysing the hippocampus at such scale.

This is a boon for Connectomics, if you ask me. Why do some memories about useless or trivial facts or moments last decades I wonder while others do not? This is  the first step to creating artifical memories. The matrix will be here soon guys.  and so, she will never die as long i remember her. this made me happy Uh I just saw a documentary that showed footage of memories and, at the risk of sounding too contrary, they are actually different colored balls that are created by tiny people in our heads who represent the five different emotions. Yeah only five, I thought it was a bit strange too, but the footage is very compelling. Somewhat related question, does this mean when I have a more memorable experience, my synapses connect more strongly?  Give or take, yes. Look into LTP (long term potentiation).  I was kinda thinking along the same lines but not quite. 

Ever have a moment where, in that moment you feel a perfect bliss and you know that you will remember it forever? Doesn't have to be memorable for the sake of significance. You could be sitting down to eat a bag of Doritos after a bad day at work and BOOM, favorite episode of your favorite show comes on and in that moment everything is perfect. I have several of those, does that mean I can somehow tell when my brain makes a Super Saiyan synapse?

Would that also imply that if you are constantly recalling a memory you are essential doing cardio for that synapse so it stays in good shape? &gt; Would that also imply that if you are constantly recalling a memory you are essential doing cardio for that synapse so it stays in good shape?

Yes; the phenomenon behind this is called long-term potentiation. Essentially, repeated stimulation of a synapse leads to that synapse transmitting a stronger signal, often through an increased concentration of neurotransmitters and receptors.

EDIT: As for the experience of "everything is perfect", to me that suggests sensory inputs eliciting a strong (positive) emotional responses, most likely because there are previous positive memories associated with those sensory inputs. E.g. Eating Dorito's causes a positive emotional response, sitting down and watching TV after a stressful day of work - reduced physical and cognitive workload - is pleasurable, and your favourite episode of TV also elicits positive memories and emotions.  I had a similar thought... You know when you vaguely recollect something (tip-of-your-tongue)?  Is that a nearly worn-out neutral pathway that your brain cannot access as easily?  


Imagining the brain as a physical storage locker seems overly simplified. Is it about strength?  Or are there other processes in place that just turn them off when it's time?  (Also, just saw Inside Out... It's crazy how well Pixar paralleled actual brain functions.) Well, often memories become more "memorable" or "stronger" based on several factors like whether it's been rehearsed often, or the event itself having several potential "cues" (salient sounds, smells, relevancy to past experiences or knowledge), etc. So based on the fact that "strong" memories are often stored in several different pathways, I would think it's more likely that there are *more* synapses responsible for that memory, not necessarily that the synapses themselves are stronger.  This makes sense then! Thanks reddit and fsmpastafarian for an 'unforgettable' lesson This is a good question and one researchers are still working on. They are trying to qualify the biology of "stronger" memory. Does this mean more connections? Probably, but I don't believe that this is always the case. Ex pruning. There was a study (I think it was done by Elizabeth Loftus) where researchers asked people to recall memories from 9-11, because everyone has a very vivid, emotion-filled memory of that day (at least Americans do). And the study found that the memory loses accuracy over time like any other memory, even though people THINK they have a strong memory of 9-11 In part, yes! Your spines are what hold the memory. Stronger memory is theorized to happen when you recall a memory multiple times (and associate new stimuli with it to add more robustness to it, such as: Memory A: 22 = 4, but then you revisit the memory to associate it with new information Memory A ver. 2: 2x2= 4, and that's how old my dog is!). If I recall correctly, when you think of a memory, you're actually breaking down and rebuilding the spine which houses the memory. It is thought that when you recall a memory, you're not recalling the exact memory, you're actually calling upon the last way you thought of that memory. This access&gt;rebuild cycle creates a stronger spine, and therefore a stronger memory. Its still too complicated to answer well. One theory could be the amount of connections between cells, another may be different properties of the myelin sheath of a cluster or axons. we know that memories are stored with synapses. We dont really know whether theyre individual synapses or a pattern or firing or some other mechanism. 

I think its a pattern of firing that we are not able to map given the massive error and adjustment in fmri. Each pixel has to be individually corrected by an algorithm, so what you see on screen is ultimately a poor resolution. The reason I think its the pattern is because other senses trigger memories, theoretically by firing adjacent or related synapses to the elicited memory.  I have terrible memory. What does this imply about ways to improve memory? Well reading through the thread repeatedly remembering stuff seems to be the way. Ah, only a few days ago someone reported that memories are stored in a single cell. Somebody must be wrong, or more likely, both accounts are simplifications. How do I destroy memories?
Help. You need to stick a needle in your brain. Once you find the memory you want to erase, wiggle the needle in that spot until you can't remember that event anymore. &gt; until you can't remember why you have a needle in your brain and freak out.

Fixt :) Something that goes over my head here is how they are able to test when mice lose their memories. I suppose certain exercises could show it but is there a way to really know, biologically or through observation, what a mouse is thinking or remembering?  What does this really support though? Synapses are just the gaps across which the action potential jumps, right? So wouldn't the memories still be stored in the dendrites of nerve cells that are sending neurotransmitter across?  So it's not true that when you remember something you are actually remembering the last time you remember it?
Do we always remember the exact memory? Or just a copy of it? Can someone ELI5 a synapse? How do the connections form, and what causes them to break? Synapses are connections between brain cells to communicate with each other. They form through electrical impulse stimulating a path for more and more connections.  They break from non-use, trauma and reuse. Probably not an apt comment for /r/science (especially as a layman). But I feel a bit of apprehension about understanding memories and dreams. Like were headed to a place where we take special care in how we live certain ways and how we recall memories to ensure long term brain health. Or we'll completely miss something and think we've figured something important out.

Then again I'm a writer not a scientist so I guess my mind leaps to dystopian. there are some synapses i'd like to cut.  A couple things to consider is that memories often have multiple locations/redundancy. Also, this may be why it is harder to learn new things as you age (aka you have to overwrite existing memories) So, if I have a horrible memory, my synapses are short-lived? Does this put me at greater risk for dementia etc. as I age? but, if that is so, people with photographic memory should have these connections lasting very long then Butterflies and moths are able to retain memory through metamorphosis, even after they change from larvae into a pool of living cells that are devoid of a brain or nervous system. Is this process not evidence that memory is stored on the cellular level and not within synapses? I don't understand the "lasts as long as the memory". Don't brain connections keep the memory. It almost sounds like a circular statement. Can somebody please eli5. I'm confused and never took any neurology in college -- but aren't neurons simply the electrical connection between other cells that actually "store" the information?  Aren't the memories or other tasks stored in, and a function of, say, glial cells?  It wouldn't change much as far as losing a memory when you lose a particular neuronal connection, I understand, but why are the cells that are at either end of neurons rarely if ever discussed -- as if its the neurons themselves that drive a given brain activity? Neurons are cells themselves. The chemical connection between neurons and other cells is known as the synapse. However, there are electrical connections between cells too. Memories aren't thought to be stored in specific cells, rather they are thought to be stored in 'distributed networks' across the brain. If you imagine, that one memory was stored per cell, as soon as that cell dies, the memory would be gone forever. That's not an evolutionary advantageous solution to the memory problem.

You are correct that glial cells have been under-researched in the past. They were often considered as unimportant, 'padding of the brain', and support cells. That's all changed now, however, and researchers are uncovering more and more of the important functions of glial cells, such as astrocytes.  How the brain chooses which memories to stay and which to be forgotten?  I find this super interesting. I suffered a "head injury" of sorts recently, took out a couple things but my short term memory got hit pretty hard. Short term memory loss sucks, it's worse than it sounds.

I find if I can get a memory past the "short term" period, into the long term category I have no problem remembering things. For example, I often wake up in the morning recalling things from the previous day that I totally forgot about, parts of conversations, interactions etc. It's kinda like total recall in a way, waking up all the sudden with memories that seem new, cause I didn't remember it previously, not until it crosses whatever threshold it is between short and long term memory.

So yeah, memory has become a topic of fascination for me, it's weird how it works, (or doesn't work) and the concept and idea that it is memory that makes us who we are...so when something is wrong with that process and it isn't functioning correctly then are we still ourselves etc etc.. What does this mean in terms of memories we forget for a long time but remember later on?  Are those synapses destroyed and rebuilt or just inactive and being reactivated? Ever so often I sit down and try to reclaim an old memory of an event in my life that I haven't thought of since it happened, as far back as possible. I think it's a great brain exercise. Can this research be piggybacked off of to target specific memories then? I see both a very legitimate good and bad problem coming from this. A person could now remove only the specific PTSD memories, but a person could also commit a crime than remove the memory and create a huge disillusioned event. Question: I have a very strong memory, I don't like to use the word eidetic since that implies perfection but I don't forget things. It's basically just below photographic and audio-tonal-eidetic memory. 

What does that imply with the synapses of my neurons? Does it really differ that much from normal connections? 

Also, this just led me to another question, am I truly learning things at a higher rate or just regurgitating what I've read or heard? 
I've never really had to try in school since a lot of the stuff I have had to learn is dependent heavily on memory work. Although on the flip side, the applied concepts of math always killed me, I would memorize the formulas but struggle to apply them. Luckily I'm almost a senior in college so I don't take any math courses now. Seems like they proved the obvious. Could you overwrite memories? I've seen too much shit on Reddit So, if you were to somehow "repair" certain synapses, would it be possible to recover old/lost memories? Such as those lost from head trauma or Alzheimer's. Does that mean I don't remember my dreams because there is no synapse being made? But can't things be forgotten/unable to be recalled, but still somewhere deep in the recesses of your mind? Does anybody know how the brain determines where to store memories and if memories get overridden? (synapses get replaced?)

I'm kinda interested, since I know about the different techniques to storing digital data, so I'm wondering if the brain follows any similar technique. Or if it follows some well defined behaviour e.g., sorting specific things in specific spots. Is that why when you smoke cannabis often, your memory starts to falter? And your memory falters more when you smoke it casually rather than habitually? 

Because the connections are more active and stable when you're doing it habitually. Then, when you stop, your memory gets bad for a while because of the change in circuitry and dendritic nets. Because, that's my observations with the substance.
 Well, id imagine if you forgot theyre wouldnt be a connection would there, otherwise you would remember it? Does this mean that cryogenics should basically work then (once the thawing, repairing, and reanimating takes place)? I have been told one's brain loses cells when drunk off of alcohol. Is it possible to really drink away memories? The brain is nuts Does this mean that if I have a not so great memory in general that I have not so great synapses? Rats.  So my brain is like RAM?  If it loses power it loses the memory? Is it really the synapse that stores the memory? How would that work physically? I imagine the synapse is just (part of) the "path" to a memory.  What happens when I remember something inaccurately? Is it the recall, or the storage that is faulty?

 And what happens when I remember inaccurately, but then someone or something "triggers" the accurate memory and I have that, "oh yeah, you're right!" moment? In that case, it's not a matter of acquiescing to the other person's opinion, but rather a correction of the original inaccurate memory?  So let's say I've effectively forgotten something, but read about it, say from my diary, and subsequently "remember" the event. Am I a; creating a new pathway to the associated synapses, or b; creating a new memory reconstructed from existing current memories? (I don't know if the latter would mean creating new synapses, repurposing existing ones, or something else altogether.) 

 But wait, what happens when someone reminds you of something you would have never recalled on your own? I get that moment might be a new "memory" but if I remember again the initial one does it simply mean I never forgot it? Even though I had? Pretty sure no one will see this, but worth a shot!

My step-kids have reactive attachment disorder.  It seems like they remember EVERY SINGLE THING THAT EVER HAPPENED TO THEM EVER.  They are 5 and 6.

Will this information give any insight as to how to heal the effects of trauma?

(Also, any off-topic or slightly off topic regarding trauma and the brain, trauma and memory, etc., will be so very much appreciated. I was under the impression that memories were created as they were remembered. Wasn't there a study done on that which showed the memories weren't stored so much as created on the fly? 'stored' is really not the appropriate word but I can't think of a proper layman's term that would suffice otherwise.

My theory on how it works is a thought happens (say, the sight of your child as she opens a present) and several connections are made in the brain to represent that thought. That's the 'creation' part. 

As you remember that moment you are reinforcing that thought. The more that thought is pondered, the more 'permanent' (or stored) that thought becomes. The separation between a thought and a memory is just time (though, the 'impact' of the thought on you personally can reinforce it to becoming a more permanent thought also).

That is a very basic description of a much more complex process obviously. Take it as such I thought memory was distributed throughout the brain and not localized. It would be interesting to see what emotional or psychological pressure does to the brain and recalling memories/information.

For example, some people get every Jeopardy question right from their couch, but can't recall simple answers when put under pressure. With this in mind, What does it mean if you have a really poor memory? Hypothesis based off information in the article.

Meditation acts like a defragmentation allowing a review and differentiation of important/non important spines and allowing them to be flipped and forgotten if not important.  Your body would do it naturally but the meditation allows it to happen more frequently and keep a cleaner drive.. uh , brain.

Don't hate, just a theory! This is partly what dreaming is for. I would be curious of the effectiveness about dreaming compared to meditation, daydreaming, etc. I know of an author who shuns mediation - I think he suspects the process is one of erasing oneself. There are some people who literally remember everything, and cannot forget. I wonder what their brains would look like? Maybe our brain holds small NFC chips until the right voltage triggers a response in the brain to show those past memories. I'm no neural scientist but if our synapses are what we call connections with our neural pathways then isn't this rather obvious? How could a memory exist without the synapses connecting them?  so if they could magically go into my brain and rewire my synapses, could I have memories that never happened?      Perhaps this is why heartbreak is so difficult to overcome. The synapses were so strong that they take awhile to die.  Anyone have the experiment data? I'm really curious about the mice maze memory. Like just how does one gauge the memory of a mouse? This is the basis for why CPTSD occurs. So when I can't remember where I put my keys, it's because cells in my brain have just stopped holding hands? Seems like Pixar's new movie 'Inside Out' was surprisingly detailed and accurate As someone in the neurosciences, the thing that piqued my interest is methodological -- apparently this group got [super-resolution microscopy](https://en.wikipedia.org/wiki/Super-resolution_microscopy) (STED) to work for [intravital microscopy](https://en.wikipedia.org/wiki/Intravital_microscopy) (in other words, on live intact animals). Both of those things alone are challenging -- combining them has the potential to really disrupt the field. Could it be why zapping people make them remember new knowledge better? Happy to take the zap as long as it is not too painful. :( How can science/medicine keep them from dissolving? It wouldn't surprise me if the transfer of memories from the hippocampus to the neocortex happens most efficiently when we sleep. how is this not a chicken/egg issue? Can new synapses grow if they've been killed off in the past. Say via alcohol or drugs?

A friend wants to know...
 But it IS mechanics. If all connections in the brain are synapses and there is a neuron without any connections, then its not really a memory is it? Much like a phone cannot make calls without a mobile connection, if all synapses to that neuron are broken, the memory is no longer accessable. Thus, not a memory anymore. This isn't philosophical, its common sense. I'm probably wrong but unless I get a scientific answer, I'm not believing its out of the realm of possibility. So what does this mean for people with a photographic memory? Are synapses firing when creating false memories?  What does it mean when it says that a 'Memory lasts'? Is the process of forgetting gradual or sudden? Mmh I'll have to read this later and see if the results of this study conflict with  the results of one of the latest Tonegawa  studies which seems to suggest that synaptic connection strength can be dissociated from the memories. I think part of the issue is that memories aren't likely to depend on just a few synapses--robust memory formation probably involves a redundant set of strengthened circuits, distributed throughout the brain. On my first day of kindergarten a decade and a half ago (I'm 20 this year), my mum took a photo of me in my kindergarten uniform in my house in front of a window (the photo was lost years ago).

I never once recalled that moment at all until a few weeks ago when I was randomly thinking about my kindergarten years.

How is it that this piece of memory that was never once replayed in my mind since the experience occurred somehow managed to return to my mind (so vividly if I may add) after more than a decade? There was some research that suggests memories are actually stored outside of the brain (DNA maybe?). 


Flat worms have the ability to regenerate body parts. They were trained then had their heads chopped off. The body grew a new brain and it appears to have retained the previous memories.

[Interesting article](http://www.theverge.com/2015/3/18/8225321/memory-research-flatworm-cannibalism-james-mcconnell-michael-levin)



[Research](http://jeb.biologists.org/content/216/20/3799)

 So, without memories, you (your brain) die technically and emotionally.. Tl;dr
The participants were separated into two groups. The experimental group received a 1 hour cognitive behavioral therapy (CBT) session and a self-help pamphlet to read at home. The control group didn't. Within three months, 73% of the experimental group reported improvements in their sleep quality. 

I wish I knew what happened in that one hour of therapy! Since it was CBT probably some education about sleep hygiene, instructions on behavior tracking/thought recording to identify triggers, bad habits, etc and some work on replacing maladaptive thoughts/behaviors Where I go online for some education about sleep hygiene? Sounds useful  [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] Here is some basic stuff:
http://howtofallasleep.com/ What if the problem isn't falling asleep but staying asleep. Seconding this.  My issue is that I fall asleep for 2-3 hours on most nights and feel wide awake at 1-2 am.  By the time I feel tired enough to fall asleep again, it's time to wake up. In some cases early waking has to do with anxiety and stress. Fucks with your hormone levels, and thus affects your sleep cycles.  exactly this.  I have the problem whenever I am stressed for a long time. That's my secret - I'm always stressed. It could also be sleep apnea. There are a lot of people that suffer grom sleep apnea but don't think of it because they aren't waking up gasping for air. I'd look into a sleep study! This has some ideas: http://capitaldistrictvitalitycenter.com/blog/2012/10/wake-up-at-3-a-m-and-cant-fall-back-asleep-consider-low-blood-sugar/

I wonder if a hard boiled egg before sleep would help. [deleted] [deleted] Ah. 

I blame FedEX whose jets fire up every night about that time. 

And if I can't go back to sleep, I'm bad about opening up my tablet and looking at reddit. :(

I used to sleep 9-10 hours a night and 14-16 on the weekend. Now I'm lucky if I can sleep 5. 

I think I used up my sleep allotment.  One possible problem is drug influences. Caffeine and alcohol both interrupt sleep.

Another is physical discomfort, for obvious reasons. Since I've injured my neck in December I have problems sleeping through the night.

If neither of those are possible causes, sorry I can't help more. &gt;how to fall asleep



&gt;black on pure white &gt; instructions on behavior tracking/thought recording to identify triggers

Do you have more information on this? I would love to learn more about this method for personal use. With CBT in general, you're often instructed to take note of when you're having problematic thoughts, or what thoughts you're having when issues arise. In this context, I would think recording what thoughts may be keeping you awake.

You should do some research on Cognitive Behavioral Therapy. It really does amazing things. I did a course online which taught how to use and apply CBT. I've found it really helpful both for myself and helping others. 

You can find it on the  EDX website, run by the Karolinska institutet (KiX) called: Behavioural Therapy - A key to better health. Well worth the hours I put in and totally free. If you can't find it let me know and I'll get a link when I'm not busy &gt; Behavioural Therapy - A key to better health

Thank you! [Here's the link for others who might be interested](https://www.edx.org/course/behavioral-medicine-key-better-health-kix-kibehmedx). I'll definitely check it out. I think it's important that this was in comparison to just 15% in the control group. yes, it is. 

If it was compared to 72%, it would just mean that it passes after some time. The therapy session covered sleep education and individual differences in sleep need at different times of life. Professor Ellis then introduced the principle of sleep restriction, which encourages the individual to spend only the time in bed required for sleep. Using their recorded sleep diaries, the individuals were then prescribed a time to go to bed and a time to rise to improve their sleep efficiency.
 [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] The pamphlet was just really boring and they fell asleep reading it u/PainMatrix said this in another thread about getting to sleep ....     Hey OP, I'm a clinical health psychologist with expertise in insomnia and sleep. We know it's actually realistic to train most people to fall asleep within 15-20 minutes through stimulus control techniques. Essentially what you do is this: 

- if you're not asleep within 15 minutes get out of bed and go to another room
- do something low key 
- when you feel tired go back to bed and try again. If you're not asleep within 15 minutes get out of bed again.
- repeat as often as necessary. 

Eventually your brain cries uncle and you fall asleep. This strengthens the association with the bed and falling asleep quickly so over time this will happen more often the first time around. Feel free to ask me any questions and if you have any other health related questions or interests come visit us over in /r/behavioralmedicine. I just wrote a post there with some more detailed information on improving sleep as well. I think there are maybe 2 or 3 times in my life (for as long as I can remember at least) that I have fallen asleep within 15 minutes of going to bed.  Even with Trazodone I had to time it *just right* or I would be awake all night.  
 I ask one person, who seems in the know, everytime a thread like this comes up; so here it goes - Have you ever worked with someone with [Delayed sleep phase disorder](http://en.wikipedia.org/wiki/Delayed_sleep_phase_disorder)?  I use all the sleep hygeine tricks to help me get to sleep as early as possilbe - for me that's 3am.  I've had this problem for over 10 years.  I could be up 48 hrs straight and I still won't be able to sleep until 3am.  I'm also completely uneffected by sleeping pills. (If you look at the wikipedia page I linked - under "definition" they list 8 diagnostic criteria - I hit the first 7 exactly... it was a shock when I first read them... and the 8th one I haven't had the opportunity to test. Advancing your phase is easier than reversing it.  Force yourself to stay awake later, say 5am and do that for a few days, then 7am, etc.  Of course, this assumes you have the flexibility in your life to do such a thing.  Typically takes a person around 24 hours to adjust 1 hour of change.  

Consistent wake time is another possible way.  Wake up at the same exact time everyday.  No exceptions.  Pretend you have to get up to feed a baby.  Drink a cop of coffee (or whatever you do) outside in the sunlight.  The sleep debt should eventually cause you to fall asleep earlier (but it may take weeks).  Remember, not everyone needs 8 hours of sleep.  You should get enough sleep so that you are not excessively sleepy during the day.  

Develop a bedtime routine as well.  Take a warm shower/bath, then sit on your couch and read a book with just a lamp on.  Do not watch TV or get on the computer (blue lights mess with your sleep).  When you start to feel sleepy, go to bed.  Do not lay in the bed unless you are sleepy.  Do not do activities in the bed other than sleep and sex (no TV, tablets, books, etc.).  If you don't fall asleep in 15 minutes, go back to the couch and read more (try not to pick things that are too activating, biographies, histories, and other types of non-fiction are probably best, but you know yourself).  If you ruminate a great deal on all of the things that need to be done, get in the habit of writing to-do lists before you start your bedtime routine.  

The bedtime routine is about cuing your body for relaxation and sleep.  The not going to bed until you are sleepy is about conditioning yourself that bed = sleep (no matter what time).  The consistent wake time is about using your body's homeostatic sleep drive to help you sleep earlier.  It will not fix itself in a few days.  It may take many weeks. 

Bright light therapy and/or melatonin have also been shown to be helpful.  Do a bunch of reading or visit a sleep doc to get the timing down for exposure.   It is very important that the exposure be done at the right schedule. 

Keep away from sleeping pills.  They have their use, but it is for short-term insomnia (e.g., you have a surgery and it is hard to sleep because of the discomfort).  They are not helpful for chronic conditions and may make them worse. It takes me hours to go to sleep and I can only sleep for 30 minutes at a time no matter how tired. I wake up every 30 minutes on the dot. It's torture and its killing me. Also, I can only do 10 of these cycles a night. After 5 hours I can't falll back asleep. The only nice thing about it is I go straight to REM, I have no other levels. I wake up from REM too.

 I wrestled with insomnia real bad for about 13 years. I took any and every sleeping pill available, and even some less traditional medications like benzos.

Nothing worked. Not even 4mg Klonopin with 6mg of Xanax. Wide awake, playing chess online on the couch. 

And then I did CBT. I was sleeping within two weeks. Fast forward a couple of years, I'm taking minimal medication and sleeping very well. 

I can't recommend it enough for serious insomnia.  I'm a little late, but I have a Psychology degree, and during my undergrad work I had to take a class called "Psychology of Health". In this class we went over Insomnia and how to get rid of it. It's interesting but if you do what Psychology tells you, it is very easy to not have insomnia (unless of course you actually have some physiological or health issue on the way). From the top of my head some tips are:

- Leave your room just for sleeping. Keep your computer, TV and all other daily activities outside your room. This will help your brain associate your room with sleeping.
- Exercise: this is the obvious one in my opinion but very important. If you're tired you'll sleep easier and better.
- Try to go to sleep and wake up at the same time every day. This is actually a big one. Doing this will help your body be tired when it's time to go to bed. It will also improve the quality of your sleep.
- Have a set of activities that you always do before going to sleep. Example, brush your teeth, floss, meditate. Whatever it is. Try to have around 3 tasks that you do every time before going to sleep. This way, when the time is approaching and you're going through the motions, your body will associate those tasks with getting sleepy.
- If you're not asleep within 30 minutes, get up and do something that will not wake you up too much. Meditating is a great one. Don't force yourself in bed for over 30 minutes.

If you do a quick google search you can find more tips. But I can guarantee, doing these things regularly will improve your sleep A LOT. And it works. It's really that easy. If doing those things do not improve your sleep you should see a doctor as you may have something else going.

Edit: Format and grammar &gt; If doing those things do not improve your sleep you should see a doctor as you may have something else going.

Yep, I have heard that "easy" advice a million times, and done my best to follow it with no affect. I would say if those things are not helping you, you are not **an insomniac**, but you **have insomnia as a symptom of something else**.

My insomnia was so bad I would go over 3 days with no sleep, and when I heard someone say those things to me again I would get pissed at them. Especially if they came with the mindset of "It really is that easy". I was hallucinating I was so tired and I tried everything in the book and then some. Turns out my insomnia was related to my bipolar disorder. So no treatment to insomnia was ever going to work for me until I was first treated for being Bipolar.

If you have heard these again and again and they are not helping, it might be that your insomnia is just a small part of a bigger picture. &gt;  if those things are not helping you, you are not an insomniac, but you have insomnia as a symptom of something else

Thanks for adding this to the discussion. Many people will assume they are just not doing the basic things right, and will try to stick to them, when actually they may have more serious underlying issues that will sabotage their efforts.  I don't want to sound like an asshole but that advice is so generic, it's kind of insulting the way you describe fixing insomnia.  I have a sleep efficiency around 50%, I see a therapist weekly, maintain perfect sleep hygiene, don't drink, smoke etc. I have no real help because I constantly get told what you wrote, I'm depressed, my marriage is falling apart. 

It's not always that easy.  I agree, but generic is good for people learning about sleep hygiene, it's just not useful for people who have already started that and need more help. I had to finally see a sleep neurologist plus a psychiatrist before I was able to get myself into a good sleep schedule. Good luck, and I don't think you sound like an asshole, just frustrated. I don't think people realize just how often you get that same collection of tidbits when you have trouble sleeping. That's what I find confusing about the results. i can't imagine many people with recurring sleeping problems over 20 or so who hasn't heard that same advice over and over and over again.  Up with this comment. I started doing this exact thing a few months back after years of struggling to fall asleep. I think the meditation actually plays the biggest part, but I do the other things as well. Nowadays once I get in bed it takes less than 5 minutes before I'm fast asleep. It says in the article that patients were taught about sleep restriction and were provided bed times and wake times based on their sleep diaries. Sleep restriction is the most important component of CBT-I(insomnia). I would love to know what goes on in that therapy session, it sounds like they just told them basic knowledge of good sleep habits to me but that seems like a huge success rate for such simple info. That's probably about right. I'm in the field and you would be surprised how much people don't know about good sleep hygiene  I had bad insomnia after a car wreck. I had a subdural hemorrhage and let's just say the insomnia was bad for months.

After four 1hr sessions of biofeedback therapy, I could fall asleep in a couple minutes and sleep soundly.

Magic.

Edit: I think we focused on Delta brain waves. The process was basically wearing external sensors on my head that were hooked up to a computer program.
The thing would simply beep whenever my brain was doing the right thing. So my goal was to get it to beep as much as possible, which is weird but not actually all that hard considering that my brain isn't a muscle.
Additionally, I was taught to clear my mind by slowly scanning over my body from head to toe to identify tension. "Are my shoulders tense, my arms, relax this muscle, mentally scan to the next..."

Edit 2: could've been wrong about Delta waves.

Edit!: Finally got the info of where I went, thirteen years ago. I saw Dr. Atkins.
http://www.wellness.com/dir/4070178/counselor/tx/plano/neuroy-center-of-plano-inc#.VW05N0i0WIE.gmail
  Whats biofeedback? You get hooked up to a computer that shows you various measurements of what state your body is currently in, such as heart rate, blood pressure, brain waves, etc.  So, let's say for example your doctor says your blood pressure is too high, and you have to learn to control it.  He might give you some relaxation or breathing techniques to try, but without a blood pressure monitor, you really have no way of knowing if they're working.  A biofeedback machine will show you a graph of your blood pressure, so you can see the changes *instantly* as you start to change your breathing or thought patterns, so you can associate what works and remember it for when you *don't* have a biofeedback machine. So why does my heart seem to absolutely race at night when i'm trying to go to sleep?  Some people have suggested I have sleep anxiety and i'm nervous for some reason.  I have that too. From what I gather, I'm basically nervous that I wont be able to fall asleep and that I will be extremely tired the next day because of it. The more time that goes by without falling asleep reinforces the idea and makes the anxiety worse. After a while I become frustrated that I'm too tired to do anything, yet can't fall asleep and I'm just wasting my time lying in bed in a futile attempt to fall asleep. Very, very annoying and stressful.  I have occasional insomnia as well and doing what he said in the video really works. If you can't sleep, get up and do something to distract yourself from the fact that you can't sleep, but nothing with a screen. You have to break the association of sleeping/bed and anxiety. I like podcasts as a distraction. Also don't take naps if you didn't sleep the night before. Just power through it and you should be able to fall asleep more easily the next night.    [deleted] [deleted] [deleted] I used to have this. Sleeping is closer to dead than living, and obviously the fear of not waking up can make it tough to relax before bed. If you have the same thing I had, the solution for me was confront my mortality and not accept death as a favourable fate, but to accept the possibility of it rather than live in denial. A quote popularised by Robert Downie Jr. : "worrying is like praying for something you don't want to happen". I don't fear death.  I've been dead for billions of years already.  It's whatever *this* is now, that I don't understand. That sounds amazing http://en.wikipedia.org/wiki/Biofeedback

Example: I've used it in physical therapy with some decent results. It's basically a tool that helps you interpret the various signals your body gives you in such a way that you both understand, and hopefully to some extent, can control, your response to stimuli. What kind of biofeedback was it? What kind of practitioner? I've been looking for something like that for a relative.  [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] Then enlighten us about good sleep hygiene. 

If we are never taught, how are we supposed to know? The "Paleo" and "Primal" people- while perhaps a little self-involved with diet quirks that I won't address- [have some fine points on sleeping.](http://www.marksdailyapple.com/the-definitive-guide-to-sleep/)

Similarly, Tim Ferriss [has some interesting pointers.](http://fourhourworkweek.com/2008/01/27/relax-like-a-pro-5-steps-to-hacking-your-sleep/) Ditto with [his pointers in HuffPo.](http://www.huffingtonpost.com/tim-ferriss/11-tricks-for-perfect-sle_b_2527454.html)

There are some who advocate *absolute darkness* for good sleep- blackout curtains and so forth- and perhaps they are correct.

I wish this weren't so admittedly un-scientific for /r/science, but if I knew of any consensus in the peer-reviewed literature I would post it. I simply wanted to provide perhaps an insight into what some consider "good sleep hygiene," given you have had no reply after 9 hours. Yeah I know sleep is important and its easier to maintain on a schedule. But that isnt helping me get more. Theres gotta be more. It isn't just about a schedule. It's about making the whole sleep routine separate from everything. No phone in bed. No tv from bed. No computer or books or anything. Your bed is a place for sleep and sleep only.  &gt;It isn't just about a schedule. It's about making the whole sleep routine separate from everything. No phone in bed. No tv from bed. No computer or books or anything. Your bed is a place for sleep and sleep only.

I understand how that might work for some people, but when I had my worst bout of insomnia making a ritual of going to bed increased my anxiety levels about getting to sleep no end. I'd be guaranteed about forty-five minutes to an hour and a half or so if I just drifted off on the couch when I was utterly spent, but making the conscious decision to go into another room and climb into bed usually resulted in a wholly sleepless night, and that could go on for days.  When they say "bed isn't for other activities," that includes lying awake being anxious about falling asleep. If I'm lying in bed and feeling wide awake, I get out and go over to the couch. If I fall asleep on the couch, sometimes when I wake up in the night, I'll go to the bed, sometimes I'll just stay on the couch. Their study helped a good amount of people who were poorly informed on good *habits*. Your issue isn't a habit but anxiety. You have something else to address.

I'm an arm-chair non-doctor. One of the main uses of CBT is actually treatment of anxieties. 

Quoting from Wikipedia:
&gt; CBT is effective for a variety of conditions, includingmood,anxiety,personality,eating,substance abuse,tic, andpsychotic disorders. I've had chronic insomnia since I was at least 3 years old, so I've some experience here.  But I understand where you are coming from.  I didn't have a TV in my room, and there wasn't internet or cell phones.  A consistent bed time just meant I had 3 hours of boredom staring at the ceiling every night before finally dozing off and waking up exhausted every day.  There are only so many changes to sleep habits you can make.  I am under the care of a doctor for it now thankfully.  I think books of a certain type are fine for before bed. When I read for pleasure, I find that I become absorbed in the world of the book and it feels like it helps me transition into sleep/dreaming. Certainly takes my mind off of my worries about falling asleep during the occasional times when I have insomnia.

However, studying or slogging through difficult non-fiction reading material before bed may be counterproductive to falling asleep. I read every night for about a half hour before turning off the lights. It helps me transition from "awake and physically active" to "physically still and brain switched off from 'shit I gotta do' ". I read almost exclusively nonfiction.

I have no trouble falling asleep. *Staying* asleep is a whole different story...
 Another trick is focus on your breathing. In through your nose, out through your mouth. The focusing on something so simple can often hush busy thoughts.

Personally, I can't read or it excites my brain to think *even more* but I don't read fiction either. If I read fiction.. I can't stop. There is no relaxation in books, there's pure and utter curiosity. [deleted] I don't remember, but presumably it is okay to have sex in your bed, yes. As to masturbation, I'm not certain again. Personally, I find it helps me sleep if I'm not laying on a hard on. However, the clean up and subsequent actions I have to take to get comfortable seem to work against me in the settling down phase.  Grab a sock. &gt; probably about right. I'm in the field and you would be surprised how much people don't know about good sleep h

Do am AMA! I'd love to know more about this. You could change people's lives. [deleted] [deleted] [deleted] Where can I find some simple good sleep habits that are actually backed by some data? I feel like I see so many 'articles' about how to sleep well and none of them actually say anything of importance. I *feel* like I know the basics but I'm sure there are things I could do differently to sleep better! Do you have any sources people could read up on? Can you share some basic stuff or point to a good resource? There's a program called sleepio that will cost you less than similar treatment with insurance copays.  Might be worth checking out.  It's a good program.   I'm in this field and I can promise you it is not what happened in that one hour that made the difference.  It was that patients had to record their fidelity to the sleep hygiene protocol recommended by the doc.  Many patients  tell me they cant sleep, but then fail to track their sleep, avoid caffeine and electronics, restrict sleep to the desired interval, and restrict the bed to sleep and sex.  This is not a failure of therapeutic technique, but rather a failure of follow through.  These numbers are deceptively high because they eliminate patients who do not adhere to behavioral health prescription. Random question but you said you're in the field. I don't usually have trouble falling asleep and I usually sleep 9 hours. However, I'll wake up and feel hyper awake at 4 am, then at 6 am, and then sleep fine until 10-11ish (weekends). On weekdays I also find myself waking up at least 2-3 times throughout the night. 

Does this qualify as some sort of insomnia? It happens every single night for the last 4-5 months. And what do you think I should do about it? It would also be interesting to see the differences between different levels of insomnia. From what I gather, studies and diagnosis sometimes requires the insomnia to be at least three times per week for more than a three month period in order to distinguish temporary disruption in sleep patterns from insomnia. This study focused on 40 people with less than 3 months reported so it would be good to look at the differences between those groups. As someone who's undergone cognitive behavioral therapy at a top hospital in the US, that's more or less what it is.  Honestly I was pretty underwhelmed.  It's mostly about establishing structure, learning about the multiple different "rhythms" your body has (more than just circadian), and trying to limit things that negatively impact sleep health.  You're instructed to keep a sleep journal, which consists of tracking your hours and quality of sleep.

Personally, I've had more success with drugs. There is a caveat here - in my experience with clients with poor sleep quality not due to factors outside of their control, most/nearly all simply refused to do what they needed to do to get good sleep. They preferred to maintain poor sleep hygiene and rely on drugs, which then created a feedback loop of needing the drugs more and more. So then what's the answer?  The answer is to use techniques like motivational interviewing to get individuals more willing to attempt the treatment.  Notice how Bill_Cosby states he was underwhelmed.  There's an expectation that we have a magic cure, the silly thing is we do, but you have to listen to us and actually try the techniques given.  

When people first try it, you can expect a decrease in sleep.  For people already anxious/sensitive about lack of sleep this can spook them out of trying the treatment with fidelity.  After that, you slowly increase the time spent in bed as long as they're still achieving a high percentage of time in bed asleep.   It's frustrating to hear about simple solutions to common issues and discover that it's only because the average person is so amazingly ignorant and incompetent that a relative hand holding can help them resolve their problems and consequently looks like breakthrough medicine. I doubt it's less ignorant and incompetence than recent societal shifts over the last two or three centuries which have forced humans to *actively ignore* their biological rhythms -- which have had billions of years to develop. You think working 300+ days a year is normal throughout history? You think having 3 solid meals a day, *every* day, for your entire life is normal? Think fluorescent lights and being available 24/7 via your smart phone is normal?

Nah. We're asking the human body to not just do something it hasn't adapted to, we're doing so and pretending like people who *can't* do it are deficient in some way. Sitting for 8 hours a day in an office environment. Or just working in general for 40 hours a week... absolutely lethal. Well...so what, the average person doesn't deserve help, because it's so easy to help them? Just because it's a simple fix doesn't mean they don't need guidance. Sounds like this is for acute insomnia, rather than chronic insomnia. [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] Here is the peer-reviewed journal entry: http://www.journalsleep.org/ViewAbstract.aspx?pid=30045 What I really wanted to see was comparison to control group. The abstract doesn't list the three-month comparison, but at 1 month, it was 60% vs 15%. That's damned good!

I wonder what would have happened if the control group received a placebo therapy session and pamphlet. My guess is that the placebo would have been just as effective.

Despite what Mister Phil would have folks believe, psychological issues are rarely solved in under an hour. If the insomnia wasn't psychological in nature - if it was simply the result of habitual behavior - then it was likely the pamphlet that helped with it anyway. Placebo psychotherapy is a serious pain-in-the-butt.  Partially because of the ethics involved in actually providing it, but also because every time it's used it actually kind of kicks arse.

Just sitting down and talking about what's going on with you for an hour is ridiculously beneficial.

It's also a little worrying that they used self reports though.

Therapists tend to be pretty likeable, and just like how you always tell the dentist you totally brush four times a day, people are pretty likely to say nice things to make their therapist look good in a test, even if there hasn't been much of an improvement. As far as self-reports go, they are way cheaper and easier to manage than actually monitoring people for 3 months. How would you do that, cameras in the bedroom? Fitbits or something like that might actually be useful there. 

This study demonstrates a strong result over a moderate period of time. It's not definitive but it does demonstrate that this is an effective direction to pursue for future research, which is exciting.  You can't really sweep the fact that there is no control under the rug. It should always be relevant, it's good science. I'm also concerned their no-treatment control wasn't very good. There were likely demand characteristics encouraging people in the treatment group to report better sleep.

All the same, the article suggests this is just one piece in a larger body of evidence supporting this type of 1-hour therapy session as a treatment for chronic insomnia. So, I doubt any demand or placebo explanation would fully account for the massive 15% to 60% jump (and later to 73%).

Also, I don't know how meaningful the distinction you're making between "psychological" and behavioral insomnia is, and why that distinction would lead you to believe a pamphlet would work better than a 1-hour meeting with a professional. The key term here is "acute insomnia," meaning that it just happened, and that you were a normal sleeper before.  Chronic insomniacs need not apply. The title says cured; the article says improved. A sample size of 20 is mighty small... And how is 73% calculated from 20 people? It would be in increment of 5%. 14.6 people cured! Before everyone loses their godamn minds over this, bare in mind this is a very very small trial. Also I call shenanigans on some of their statistical reporting, 73% improvement sounds impressive but then when you look at the magnitude of benefit it only just reaches statistical significance, in a small trial its certainly encouraging, but that's all... encouraging for the future, more research needed.

I also don't see how they've controlled for placebo response within these patients, you'd think they'd have some sort of sham CBT arm instead of just the pamphlet. Psychiatric indications are prime for placebo response. 

That being said, well done, good stuff, a nice non benzo method for tackling insomnia is desperately needed as I've been wildly underwhelmed with Belsomra's magnitude of effect, thought the orexin targeting drugs would change everything, I hope someone designs a better one. Yeah, they basically helped 9 people get better (with a 1-hour information session) versus 3 people with placebo (1 hour chilling in a room). This doesn't qualify as publishable science in my mind.

http://www.reddit.com/r/science/comments/382vyw/a_simple_onehour_therapy_session_has_helped_to/crs1wft

http://www.reddit.com/r/science/comments/382vyw/a_simple_onehour_therapy_session_has_helped_to/crs1ly8 &gt;This doesn't qualify as publishable science in my mind.

Of course it does! It's a very interesting result and needs more testing. 

Now if what you're thinking is that maybe it shouldn't make the front page of reddit, then I'm with you  [Perhaps they read this book?](https://vimeo.com/27019462)

In all seriousness, general sleep quality is getting increasing recognition for its impairing effects on our quality of life and its ties to mental health problems. 

In the vanilla CBT for sleep paradigms, early sessions emphasize "sleep hygiene" and healthy ways to think about, monitor, and promote sleep. It sounds like that's what was implemented here.

Things to think critically about:
- sample size
- the very obvious nature of control vs. intervention
- response bias
- how enduring were these effects? (would be tough to test if everyone got CBT-I eventually or at least were offered)
- what's the nature of the sample? college kids are notoriously bad at maintaining structure and sleep hygiene habits (spend all day studying and socializing on your bed and then try to fall asleep later)

 Wth is sleep hygiene?  Therapy buzzword for habits that promote better odds of being able to sleep.For many people who have sleep problems, there are straightforward behavioral changes that can be tried as a first step to improving sleep quality. 

* **Using your bed for sleeping and sex.** Training your body to adapt to the bedroom as a place for sleep to facilitate that being a conditioned response on entering the bedroom. 
* **Doing things to promote being sleepy**, like exercising, reducing caffeine or sugary products (if you are sensitive to such things), etc. Or changing the timing of those things to try to get a better outcome.  
* **Doing things to make your bedroom a sleep-promoting place** (like turning off screens and distracting sounds, increasing darkness, adjusting temperature, etc). 
* **Education about what sleep does, and how it affects us** - including things not to fret about like needing to get at least 8 hrs sleep (not true for everyone), the fairly minimal effects of one bad night of sleep, etc. The idea here is that some people get so worked up about losing sleep that it inhibits their ability to sleep (this can happen during the night, and also before you go to sleep). 
* **Ways to monitor your sleep** that are helpful (more scientific) versus harmful (angrily checking the clock every hour). This can also be useful as a baseline for other investigations about individual differences in sleep patterns and more severe, chronic sleep disorders, for which there are more substantial treatment options. 
* **What to do (acutely) when you can't sleep** to try to maximize your ability to get back to sleep. 
* Most critically, **developing a schedule for sleep**. Trying to have a bedtime or, if nothing else, trying to have a wake time (which may mean you're tired but you're at least setting a rhythm to your sleep pattern). Related to this, how to nap if you're a napper (and how to break the nap habit if that's what's fouling up your sleep patterns).  
* Finally, **education about other factors that influence sleep** like medical issues (e.g., COPD, apnea, etc) and mental health issues (e.g., stress, depression), as well as how to get assessed properly for sleep problems, and what kinds of interventions are out there (e.g., machines, programs like [this](http://mysleepbutton.com/home/), &amp; medications - including ones that are recommended for long term problems)
 &gt; What to do (acutely) when you can't sleep to try to maximize your ability to get back to sleep.

Do you have any source where I can learn more about this topic specifically? Sleep hygiene is sleeping in a dark quiet room, maintaining a consistent sleep/wake schedule, avoiding all electronics in the bedroom, not doing work in the bedroom, having a bedtime ritual, doing relaxing things in preparation for bedtime etc. that would be good sleep hygiene.  You mention having a quiet room.  Is having a loud fan (I crank it to high) considered poor sleep hygiene?  I find that I fixate on little noises when trying to fall asleep, and a fan really helps distract me from those noises and soothe me to sleep.  

Is that considered unhealthy though?  I do feel that I may have screwed myself, because when I don't have my fan (traveling) it's harder for me to fall asleep.  But other than those situations, is it unhealthy for any other reason? [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] Article is [here](http://www.journalsleep.org/ViewAbstract.aspx?pid=30045)

The study doesn't have an adequate control. Basically they've compared waiting to do something about your insomnia (a "wait list") and ostensively doing something about your insomnia (a CBT session + a pamphlet).

We can't tell if the authors measured the placebo effect here or something else. Contrary to the author's claims, no inferences about the efficacy of CBT or the pamphlet can be drawn from this study. The appropriate conclusion here is: Trying to do something about your insomnia is better than trying to do something about your insomnia but not actually doing anything yet. [deleted] [deleted] I see an issue with the study. The therapy session may not have been necessary. The participants who received the therapy also got an informational pamphlet to teach him 3 things they can do to help with their insomnia. We need a third group of participants to only receive this pamphlet to see how much improvement this pamphlet gave on it's own.  Article: "73% of participants reported improvements in their sleep quality."

Article Title: "73% of insomniacs cured"

Sensationalistic journalism at its finest. I think the statistic was that 73% of people treated did not progress into having chronic insomnia (3 months or longer) but does not say it was only one, one hour, session that did it. Well, thre are serious problems with the study. First of all, they took only those who had a sleep problem for 3 months or less. That's the best responding group because it's early. If they took chronic insomniacs, the outcome would be clearly much, much worse. &amp; to think by just talking we can cure such neurochemical disorders doesn't make much sense. Sure it's cheaper, but dying's cheaper, too!!

We gave up "just talking" years ago as it became obvious that more was going on. As a result we have had very good results in treating psychological disorders as well. And believe me, depressives and manias have huge sleep problems!! What about sleep apnea as a sleeping disorder? We can't just TALK that away, either.

so we are dubious of this, NOT confirmed study. IN a small, select group of sleep disorder patients, we can get any results we want to find. But over a broad spectrum of sleep disorders? We are understandably dubious.

Frankly, psychoanalysis is dead for the same reasons. Treating with meds for most serious psych disorders is dozens of times more effective than just talk and such therapies. We are talking about many organic, metabolic and neurochemical problems here, not just "not being able to sleep." 

so we'll see. More than likely sadly, this is just more of a fad than something real, because they ignored sleep apneas and other sleep study disorders, too. [deleted] [deleted] [deleted] Oh please. In forty years of chronic, life disabling insomnia I have heard this claim no less than ten times, and nothing ever comes of it. Every time there's some "miracle cure" it disappears into the ether and everyone blames the pharma companies. Sounds like more empty promise to miserable people.  I dont think that 20 is a large enough sample size to be statistically significant &amp;gt;  One group received treatment of a one-hour one-to-one cognitive behavioural therapy session delivered by Professor Ellis and a self-help pamphlet to read at home. The control group received no additional support.

Could be the placebo effect.  Why didn't they do a double-blind study? [deleted] *Wow*. I'm certainly looking forward to a more comprehensive discussion of how exactly this process works. I understand that it's a CBT session and pamphlets, etc., but exactly what did the CBT session consist of? I'd also be curious to see how well this 'single-shot' style of therapy would work with different disorders and conditions.

Regardless, 73% is a damn good rate. Nothing to phone home about, but certainly something to be excited about. [deleted] It will be interesting to see if this holds up to replication in a larger sample size than twenty in each arm. I had a bout of insomnia last year for about three weeks - the first I've ever had. Normally, I sleep quite well and deeply and that really brief insight made me realise how horrible it must be for long-term sufferers. Sleep is not about stopping everything. It's about winding down and getting prepared. Sleep is a complete psychphysioneurological response that, typically, needs attention to all those components in order to be successful. 
As you say, laying down earlier may just provide an hour more time to feel stressed out and let a mind ruminate. 

Other simple suggestions are to create habits just for before sleep. For example: Drink a glass of milk before bed. Maybe drink water an hour before bed and have a last pee of the night. Or maybe you train yourself to have a bowel movement at night instead of after lunch. Do laundry or maybe you take a few minutes to tidy up your room, or your work desk if you work until before bed. Play with a Zen sandgarden for a few minutes. put on an easy-going album. Prepare part of your outfit and/or breakfast for the morning. These things don't mean you have to stop your life, they're just ways of rearranging some of the things you might do other times and putting them aside as before-bed rituals. This is an interesting idea, but this article is nothing but fluff. Literally zero detail why this is a study of a medical technique and not a self help sleep seminar easily accomplished by a YouTube video.

It makes me extremely skeptical anything has been discovered or proven.

Does anyone have an article that has detail beyond why Northumbria University is cool? While the study has promising results, 40 participants doesn't seem like a big enough sample to me. Also, these patients were all in the beginning stage of insomnia, and had never engaged in any previous CBT.

I'd like to see a bigger study with more variation in subjects. Could we please get some flair for submissions to this sub with big claims based on ridiculously small sample sizes? Seriously. &gt; instructions on how to use cognitive control and imagery to distract their mind.


Like counting sheep? Or ways to stop thinking about everything you need to do tomorrow.  Or worrying about how you handled something that day.  Etc.  As someone with consistent "racing thoughts", I learned before the age of 10 how to tell myself a story at night that would eventually blend into my dreams as I fell asleep.  That's very common among racing thoughts sufferers. [deleted] &gt; Detect  how to record their sleep diary; Detach  how to control stimulus that could lead to disrupted sleep; and Distract  instructions on how to use cognitive control and imagery to distract their mind.

In other words: "Counting Sheep".
 [Direct link to paper here](http://arxiv.org/pdf/1505.06815v1.pdf) [PDF FILE] Thanks a lot! No problem. It's somewhat technical, but a very decent read if you're into that sorta thing. :) Definitely too technical for me, but I love the *powerful title: Powering the Next Billion Devices with Wi-Fi

*Edited because I misused the word "poignant" Props to you for a less sensationalized title!  Really, I thank you for such a sensible title.  I don't think you understand what the word poignant means No for real everyone, poignant means sad. The title is optimistic, the exact opposite of poignant. Completely right, completely my mistake. Have corrected. Thanks for pointing it out! Cant wait for cordless garden hoses Think about all those privates who want to suck golf balls, though! What are they gonna do now? There's still de-chroming bumpers. Figure 16Wi-Fi power via USB. It consists of a 2 dBi Wi-Fi
antenna attached to our harvester. Using this, we charge a Jawbone
UP24 device in the vicinity of the PoWiFi router **from a no-charge
state to 41% charged state in 2.5 hours**.
 what is in the vicinity? This matters a lot 5-7cm from the very powerful router. The Jawbone UP24 has a 32mAh Lithium-ion polymer battery. That's 32mAh. It takes two and a half hours at 5cm from the very powerful router with a big ass antenna coupled to the USB to charge a battery that holds 1/60th of the charge in cell phone batteries.

This is not a revolution. This has niche applications, but it's not good for charging batteries. It hasn't been in the last 50 years, and it's not going to in the next 50 years. Signal is too low, capacity is too high. Generally, it's a bit better to link to the [abstract page](http://arxiv.org/abs/1505.06815) at arxiv.org instead of straight to the PDF. For instance, the abstract page will contains links to all revisions of the paper, so the link does not become outdated if the paper is revised. A Peter File?  Whos a peterfile? &gt; Direct link to paper here

They powered a 2.4V 60mA VGA sensor coupled with a ultra-low leakage capacitor enough to take a 176x144(0.025 megapixel) frame every 35 minutes on a distance of 17 feet with feeding of a very high powered Wi-Fi transmitter.

Impressive? Not really. Practical? Not really.

[EEVblog #55 is about this.](https://youtu.be/P8s3Xjeg0sk) How is this not impressive? Everybody expects every advancement to be flying cars these days. No kidding. I'm reading it and just thinking to myself this is really amazing. And this guy is like "meh". I mean it isn't going to change my day to day life yet, but the fact that it is possible though is truly incredible to me. We already knew we could do this. wasnt tesla working on this?....jp Morgan cancelled teslas contract over something that sounds similar....in Morgans words...if i cant put a meter on it and charge for it  im not interested. Kinda, Tessa was trying to pull energy from the ground and distribute it wirelessly. it was highly inefficient energy usage wise and if used today would completely mess up anything besides like a light bulb, which was pretty big back then, not so much now. Assuming the technology didn't advance at all from then till now With, like... Christmas tree lights, though, right? This is a pretty decent step above that. http://www.wolframalpha.com/input/?i=2.4V+60mA+in+watts

No, it isn't. I mean helicopters are practically flying cars.  I think you're right. Seems to me that a helicopter is like a flying car in much the same way that a motorcycle is like a horse on wheels. It's weird how we think about these things. If a person got hold of a current gen smartphone ~50 years ago, would they think of it as a phone? Does history of development constitute the defining feature of an appliance? Yeah, how awsome wouldnt it be to have a phone in 5-15 years that could passively charge through WiFi never having to be manually charged? &gt;  to have a phone in 5-15 years 

Interestingly enough, this is how long the phone would take to charge using this method.
 &gt;  5-15 years

 It won't happen. Geometry is a bitch with the inverse square law This dude is right, this sort of tech works well when things are close by. In 90% of daily applications you won't be close enough or your power draw will be too much. Like wireless charging, this is so marginally useful it's ultimate use will just be a gimmick. Now betavoltaic stuff on the other hand...  This is why most of this thread is talking about omnidirectional transmission being far inferior to aiming a transmission beam. The losses are far less if you have a way to aim at the device you are trying to charge. If you only look at the math, then yeah, you can compensate for path loss with more directive antennas. In practice, there's a limit because you can't really make an antenna with arbitrary gain. You can make a nice, big 30dBi antenna on each side, so you slap a constant on the received power equation, but it still falls off as 1/r^2, and that constant can't be arbitrarily large. A quick glance at Wiki looks like 70dBi is the biggest antenna in the world, and anything reasonably sized is going to be way lower than that. And if you try to use higher frequencies that are free (such as 60 GHz or W-band, for instance) the losses in the antenna go way up, so you can't even make super-high gain antennas. Surely a transmission beam could be aimed using accelerometers and a bit of vector math. [deleted] Conceivably a phone could have a tracking beacon in it and the power tight-beamed to it to keep the power loss down.  Not really practical for such a small item and probably dangerous to the user, but doable. You're right, but this, along with a few other technologies that are emerging at various stages of deployment/development (kinetics, thermal, ambient solar) put together would probably serve to recharge devices, not fully, but to at least help them run longer would be really cool.

Of course, the reality is that the above idea is probably less practical than just make batteries and capacitors that charge far more quickly, store more electricity, hold their charge longer, and are more durable. Oh, and make the devices way more power efficient. WiFi doesn't put out enough power. They have physics working against them.  Plus you don't want your neighbors eating up all the wireless spectrum charging their devices at the expense of your data transfer rates. You lost me on the last sentence. Data transfer rates have nothing to do with the random noise generated by a router. It'd be incredibly pointless and wasteful to be downloading random things to broadcast instead of just having the router sending out junk signals. If the router isn't broadcasting then you can't charge the battery.  If the router is broadcasting a lot, then the channel utilization goes up. Looks like you could get a whole extra 3 minutes of sot from WiFi charging.  Phone might be stretching it a bit. But you might get a combination Smoke detector with a camera + motion sensor that clicks a few frames when it detects an intrusion.  Why wouldn't you just put **a battery** instead of the WiFi antenna and converter in this device?

It will be smaller, cheaper and given that they need 10.4mJ per image capture [it will last about 28 years](http://www.wolframalpha.com/input/?i=%28%281200miliampere+hours*1.2V%29%2F10.4mJ%29%2F2%2F24%2F365)





So what's the point of putting a specialised WiFi router that pollutes the radio spectrum and using a specialised low-power energy harvesting device when a single AA-size battery would last 50 years? Or swarms of insect-like spy drones with indefinite lifespans. We have those, in fact. A company insists on calling them drivable planes despite their vehicle literally being a sedan with wings plus landing gear, and another company will have theirs on sale in 2017. If they can't see the results they lose focus.

We're on the cusp of one of the most fascinating propulsion methods of all time with EM drives and the general public has no idea. &gt; If they can't see the results they lose focus.

Hard to focus at 176x144... Pulling power out of radio waves is trivial, we've done it for over 100 years, every antenna does it. You can make an AM radio with a diode, cap, and a wire. This is just one tuned for WiFi. 60mA@2.4V is just 0.144 mW, far from useful. 100mW is super useful! Mostly for sensors and other low power devices. Shit even a remote control that never needs a battery would be useful. But this is not 144mW... It's being able to draw 144mW for long enough to capture one 0.025 megapixel image. Once. Every. 35 minutes.

It's not even close to 144mW. It's probably somewhere in the nano- or, if lucky, microwatt range. I'm aware. Devices with RMS power on the order of uW's or even 100s of nW's already exist and are in use. Typically with either a coin cell or other small battery. Removing the need to change the battery every 2 years is useful. [deleted] Well, 60mA at 2.4V is 144 mW so.... Wi-Fi uses frequencies around 2.4GHz, which is exactly what microwave ovens use to heat food. The reason we consider wifi safe is because it can function with less than 100mW while microwave ovens use at least 10W (for defrost programs). 

The reason microwaves use this frequency is because it's the frequency that gets absorbed by water with the highest efficiency. The reason wifi uses those frequencies is because they are free to use by the public (and not reserved for commercial or military purposes) and have high enough bandwidth. 

The efficient absorption attribute of the 2.4GHz frequency of electromagnetic is the reason it's a bad idea to use it on high power in public. Can it transfer 50mW safely? Maybe. We don't really know how it can affect our brains in long term. Can it power home electronics that need hundreds of watt? Absolutely no. 

It's really a lot easier to create electromagnetic fields. The reason wireless power of today (wireless charging etc) uses magnetic fields is exactly that, it doesn't get absorbed by our bodies. Microwave ovens use way more than 10W for defrost mode. At least 100W.

This is a bad idea because it crowds up the already crowded 2.4GHz spectrum even further.  It is not a good idea for powering equipment.  Plus, the efficiency of the system with a 55dB pass loss is about 0.00005%. A small solar panel would work better, working off ambient light. What _is_ so impressive about this? Look at the nearest wall outlet. We have one of those in almost every house in the world. We didn't a hundred years ago. *That* is an impressive feat of engineering. This wireless thing is bonkers compared to that.  Yet most houses probably did have a device similar to this 100 years ago. Your great grandfather's first radio was likely a crystal set that didn't require wall power or batteries because it drew power directly from the radio signal it recieved.  That sounds really cool. Could you give a link to an example of one? I'm not being snarky, genuinely interested. http://en.wikipedia.org/wiki/Crystal_radio - I had one of these in a kit when I was a kid, completely forgot about it. [deleted] [deleted] Nikola Tesla was wrong. This is not going to get better with time because it is limited by physics. The improvement would mainly be in the low voltage devices, not the wireless transmitting of power. [deleted] Tesla's wireless system was really using the earth as a conductor. From the Future of Wireless art.


   "It is difficult for a layman to grasp how an electric current can be propagated to distances of thousands of miles without diminution of intention. But it is simple after all. Distance is only a relative conception, a reflection in the mind of physical limitation. A view of electrical phenomena must be free of this delusive impression. However surprising, it is a fact that a sphere of the size of a little marble offers a greater impediment to the passage of a current than the whole earth. Every experiment, then, which can be performed with such a small sphere can likewise be carried out, and much more perfectly, with the immense globe on which we live. This is not merely a theory, but a truth established in numerous and carefully conducted experiments. When the earth is struck mechanically, as is the case in some powerful terrestrial upheaval, it vibrates like a bell, its period being measured in hours. When it is struck electrically, the charge oscillates, approximately, twelve times a second. By impressing upon it current waves of certain lengths, definitely related to its diameter, the globe is thrown into resonant vibration like a wire, stationary waves forming, the nodal and ventral regions of which can be located with mathematical precision. Owing to this fact and the spheroidal shape of the earth, numerous geodetical and other data, very accurate and of the greatest scientific and practical value, can be readily secured. Through the observation of these astonishing phenomena we shall soon be able to determine the exact diameter of the planet, its configuration and volume, the extent of its elevations and depressions, and to measure, with great precision and with nothing more than an electrical device, all terrestrial distances. In the densest fog or darkness of night, without a compass or other instruments of orientation, or a timepiece, it will be possible to guide a vessel along the shortest or orthodromic path, to instantly read the latitude and longitude, the hour, the distance from any point, and the true speed and direction of movement. By proper use of such disturbances a wave may be made to travel over the earth's surface with any velocity desired, and an electrical effect produced at any spot which can be selected at will and the geographical position of which can be closely ascertained from simple rules of trigonometry.

   "This mode of conveying electrical energy to a distance is not 'wireless' in the popular sense, but a transmission through a conductor, and one which is incomparably more perfect than any artificial one. All impediments of conduction arise from confinement of the electric and magnetic fluxes to narrow channels. The globe is free of such cramping and hinderment. It is an ideal conductor because of its immensity, isolation in space, and geometrical form. Its singleness is only an apparent limitation, for by impressing upon it numerous non-interfering vibrations, the flow of energy may be directed through any number of paths which, though bodily connected, are yet perfectly distinct and separate like ever so many cables. Any apparatus, then, which can be operated through one or more wires, at distances obviously limited, can likewise be worked without artificial conductors, and with the same facility and precision, at distances without limit other than that imposed by the physical dimensions of the globe. Do you have more on this? Http://www.tfcbooks.com/tesla/1908-00-00.htm that's the article they quoted I love Tesla. Opposite to his genius, he was a proponent of luminiferous aether, and was romantically involved with a pidgeon. It was a beautiful pigeon, with lovely white tipped wings. Here's the whole thing.  http://www.tfcbooks.com/tesla/1908-00-00.htm


 And More of Tesla on the subject.

http://www.tfcbooks.com/tesla/1904-03-05.htm


 THE TRANSMISSION OF ELECTRICAL ENERGY WITHOUT WIRES
by Nikola Tesla


When the great truth accidentally revealed and experimentally confirmed is fully recognized, that this planet, with all its appalling immensity, is to electric currents virtually no more than a small metal ball and that by this fact many possibilities, each baffling imagination and of incalculable consequence, are rendered absolutely sure of accomplishment; when the first plant is inaugurated and it is shown that a telegraphic message, almost as secret and non-interferable as a thought, can be transmitted to any terrestrial distance, the sound of the human voice, with all its intonations and inflections, faithfully and instantly reproduced at any other point of the globe, the energy of a waterfall made available for supplying light, heat or motive power, anywhere-on sea, or land, or high in the air-humanity will be like an ant heap stirred up with a stick: See the excitement coming!
  

 Most people with strong opinions on the man seem to have read articles about him and not his written works themselves. I have a collection of all of his writings. See you in ten years bud Inverse square law cannot be changed. For wireless power, either you have the object very close to the power source (like laying on the mat), or you effectively waste tons and tons of power. 

Inverse square law is roughly PowerReceived = PowerTransmitted/r^2

So if your charging mat is 1 mm away when your phone is on it, to charge it at 1 meter as well as it charged on the pad your transmitter would need to be 1 million times as powerful. To transmit 10 meters, or a reasonable range inside a household, it would need to be 100 million times more powerful. A typical phone charger is about 10 Watts, so that means you would need to be pumping 1 Gigawatt out of the transmitter. Most powerplants don't even put out that much juice. This assumes omnidirectional transmission though, does it not? Yeah, which is why it's narrow minded to dismiss future potential because there are so many variables this doesn't consider. This also applies to things like satellite signals etc, in which cases the inverse square law is easily overcome using parabolic reflectors and things like forward error correction. Yes, this is impractical if we assume omnidirectional transmission. But science is all about finding ways to get your desired results within the laws of physics. Take flight for example. Gravity says things always fall to earth, but with with trial and error we figured out how to circumvent the practical effects of that without disobeying the laws of physics. Interestingly, forgoing omnidirectional wireless transmission and using a "beam" transmission instead, looks to be a promising way of reaching 5G data transmission speeds:

http://spectrum.ieee.org/telecom/wireless/millimeter-waves-may-be-the-future-of-5g-phones

&gt; Samsungs current prototype is a matchbook-size array of 64 antenna elements connected to custom-built signal-processing components. By dynamically varying the signal phase at each antenna, this transceiver generates a beam just 10 degrees wide that it can switch rapidly in any direction, as if it were a hyperactive searchlight.
 Right, so why not dramatically reduce the power consumption of the device (which is always being done) and have a focused beam tracking the device and powering it up to like 5 meters away. What many people seem to ignore is that most modern wifi tech is no longer just a simple omnidirectional broadcast. Beamforming is the general term for this, and modern wifi routers rely on it, because they're smart enough to know that "shouting" in certain directions results in better reception than others. In essence, routers can broadcast their energy in a few directions at any one point in time, resulting in a much stronger signal in those directions where there's a much higher probability of being useful, and much weaker everywhere else where it'll likely be ignored--a much more efficient use of power.

The real magic behind how a router can direct most of its power in particular directions without moving mechanical parts is in how you can add multiple waves together to make another more meaningful wave. Phased-array antennas take advantage of this. They're basically a bunch of tiny antennas that broadcast simple signals that are meaningless on their own, but combined with the others, create a more complex signal that destructively interfere where it shouldn't be sent, constructively interfere where it should.

An early example can be seen in the US radar defense system. They used to look rotating satellite dishes, or spheres with a rotating dish on the inside. These took advantage of geometry to focus a signal. The modern radars don't have moving parts, and utilize the interference pattern produced by thousands of tiny antennas arranged in a regular array. This sort of technology appeared around the 70s (the math known probably for longer), and has only been miniaturized in the last 5-10 years for use in the consumer market for wifi. This isn't exactly true. WiFi routers that do use beamforming (and not MIMO) pretty much don't use traditional phased arrays, either passive or active (AESA or PESA), they do it with digital beamforming. There is a substantial difference in practice - phased arrays are still primarily used by the military, largely because they're outrageously expensive, power-hungry, and big/heavy. WiFi uses DSP to do the beamforming rather than phase shifters, etc. Exactly. The same concept is used in loudspeaker/acoustic applications.

LRAD. Acoustic Hailing. The main stage at a festival. All use wave interference designs to construct a more directional, better focused beam of acoustic energy.  &gt; Yes, this is impractical if we assume omnidirectional transmission.

If you don't assume omnidirectional transmission (or close to it) the application to "internet of things" seems to evaporate. If you have to have all your ducks in a row to make them work, you might as well run a wire. Why not have something that uses low power (battery?) omnidirectional signaling to initially announce its presence and location, and then power over beams after the initial handshake? If you assume stationary transmission. It could track and rotate. 

Also, this could be a breakthrough for more energy intensive human implants. &gt; If you have to have all your ducks in a row to make them work, you might as well run a wire.

I'd much rather have a spot I put my phone to charge than have to plug and un-plug it several times a day. I'd pay a lot for one assuming they lasted several years and were universal. But what if there was a thing on my ceiling that could find my phone anywhere in my room and send a beam straight to it? 

Take that a step further, what if phones sent some kind of signal that said "I need to charge" to any of these charging stations nearby? You could be eating at chilli's and your phone would automatically charge without you even knowing/caring. How much you care about wasting power matters a lot. If we achieved some pipe-dream energy break through in the future, we might not care too much about "wasting" 50% of power in the transmission. The inverse square law is **not overcome** by using parabolic reflectors. You can see this for more info: http://en.wikipedia.org/wiki/Friis_transmission_equation

TL;DR- the output "beams" of antennas (and all other emitters of electromagnetic radiation, e.g. lasers) are subject to diffraction. This is a fundamental law of physics that can not be overcome.

EDIT- That being said, it **is** important, as you said, to not dismiss research outright. However, as someone who is currently doing research in this area, I don't find this this particular paper or approach to be super impressive. There's a lot more promising and practical work being done in photovoltaics. Thermophotovoltaics, in particular, is a pretty cool area. I work with a technology that transfers 50kW across a 10 inch air gap at 92+% efficiency, so...  Granted, the transmitted field is very formed and is broadcast specifically to a receiver, it's not omidirectional like the wifi stuff would be... This may sound like a stupid question but.. what if you stuck your arm into that air gap? Would you feel anything? Good question. No one at the shop has volunteered to try. We assume it would be like getting too close to an am radio antenna transmitting a lot of power, and you would get surface burns.  Simple fix would be for the beamer to locate the device and aim for it.

Or does it not work that way? It would have to be laser accurate. The power still spreads evenly across the broadcast area. So if you could make a narrow beam, you would be able to slap a nice large constant on the front of the equation, but it will still fall off at a rate of constant/r^2.

Think off the Mythbusters episode where they aim the gigantic laser at the retroreflectors on the moon. They send out incredibly powerful pulses to receive only a few photons back. To the moon and back is IMMENSELY further than any distances this technology would need to work with on earth. Getting a few photons back with a relatively weak signal source relative to the distances in question using a less penetrative em wavelength than radio seems to me like good news. Plus with optics like lasers, it's not too entirely difficult to get a beam to go very far, it just needs to be well collimated which is doable. Also there was likely a lot of loss because for ease of aiming at the moon, the collimation was likely such that the spot size was at least multiple kilometers in diameter on the moon. The mirror is much smaller than that, so immediately most of the power for the return trip is getting thrown out. To me that makes the few photons back that much more impressive. I have faith the L&amp;O guys at my work could make the spot size on the moon much smaller than that, but then you'd actually have to try to hit the mirror instead of just vaguely aiming in their direction.  This effort is much lower when your distance is within a house and needs multiple orders of magnitude less accuracy than the poorly collimated moon laser provided. Point is, I think your moon example is actually an illustration of why this *is* possible.  But the point is scavenging free power from existing wifi networks.  If we are beaming the power directly, the wifi won't work for the guy in a corner office anymore.. Depends on how many transmitting beams it has This was Tesla's original ideadirecting energy towards a point. In the real world, isotropic radiators don't really exist and are usually highly undesirable because of the inverse square law. Cell service providers address the issue by assembling towers with sector antennas in a triple 120 degree horizontal by 15 degree vertical beam pattern. This provides 360 degree reception but in a very vertically flat pattern, following the ground, but wasting very little power to space where there are no receivers.

There's currently radio technology emerging with 802.11ac which can dynamically change the radiation pattern based on the location of clients. This boosts beam power only where the receiving antennas are.

So, the closer to a collimated signal you can generate, the lower your radiative losses are. So there definitely are improvements to be made in the efficiency of these systems.

What's actually fairly unavoidable is signal attenuation due to environmental losses, log-distance path loss model type stuff. Usually this is addressed by reducing the transmission frequency (inverse). Gains are dependent on antenna size, which are dictated by the center frequency of the system.

However, if you're looking to address the power problem with an already ubiquitous technology like Wi-Fi, where bandwidth is proportional to frequency, the problem is getting harder to solve, not easier, as bandwidth increases. RemindMe! 10 Years "Check if WiFi power transmission technology had indeed developed in the last 10 years" [deleted] It's not new.  Solar does this, very low current.  RFID does this, very low current.

It's the same shit, different wavelength.  Nothing impressive here.   It's not new. It's very old. In essence, a crystal radio on steroids. I thought this was already possible? I recall watching a presentation at the end of an engineering classes senior projects where this guy designed a sort of robotic docking system for tiny surgical bots(the system was capsule sized) and they had successfully powered them via wifi. That's.... Okay I guess. Wi-Fi isn't really meant to transmit power. Might be a good way to tell devices to turn on though. This isn't really even a new idea... At least to me. I remember wondering why they couldn't do that with radio waves like the ol' crystal radio Proof of concept? Yep. This is really interesting because my initial concern was about broadcast traffic on the already cramped 2.4GHz frequency band, but it looks like they have designed this technology with this in mind. On the router side in their experiments they made it so if the router has below 5 frames in its queue that means it's possibly being under-used so the router can broadcast a packet to contribute to a more stable power over WiFi broadcast during the "silent" parts of broadcast traffic. It also uses the ubiquitous CSMA/CA for WiFi transmissions to avoid collisions on the network. It's more than fair towards neighboring WiFi networks since it transmits its packets at the highest bitrate for the specific 802.11 protocol (a/b/g/n/ac) so the power packets are in the air for a much shorter duration than typical over the air traffic. Although they only tested using 802.11g @ 54Mbps, they mention that the better than fairness will still occur at higher ranges.  
&amp;nbsp;  
The thing that I don't believe they necessarily evaluated thoroughly, however, is the possible effect of multiple power over WiFi routers broadcasting in the same area. If this technology blossoms this needs to be addressed as the possibility of jamming WiFi traffic certainly exists. That was my thought.  Fine if there is an open and unused channel. .. but if you live in say an apartment block where your best channel choice is the one that ''only'' has half strength signal from the guy down the hall instead of being saturated, and both of you start ''broadcasting'' noise... I imagine that would slow things down/cause dropped/lost packets or something. Of course their research was just proof of concept, but a more robust approach would be to realize that if there's only 1 free channel (such as in your example), then it can be assumed that every other channel is occupied with neighboring routers, which *should* produce lots of noise anyhow.

Naturally this is pure theory, and research like what was posted is how we test it, but the nature of this device seems to be that the more crowded the EM space is, the quicker it charges. Adding in extra noise like they did is to just replicate other routers chatting away. Why does it have to BE current Wi-Fi implementations? there could be a next generation of routers that have 802.11lakjsdklajsdjklnvckjnhfjvuvudeer (I don't know these days) that have one frequency to broadcasting noise for your devices and the other frequencies are for regular Wi-Fi? Because we only have so much unlicensed spectrum available for public use. Mainly 900mhz, 2.4ghz and 5.8ghz are the main ones used for WiFi. Designing a new standard would do little to help with trying to broadcast packages for power transmission as every wireless device on that frequency would have to be using the same standard in order to avoid collisions and interference which wouldn't happen anytime soon.  If this is completely viable it would make more sense just to designated a band of spectrum for power. Sadly we cannot. All we can do if free up old spectrum that's not being used anymore like analog TV, am/fm radio and others however there's ideal frequencies for certain uses so lower frequency spectrum like fm is pretty much useless for high bandwidth applications but can travel long distance. The idea here is to use traditional WiFi on traditional unlicensed spectrum to transmit power, doing this on anything but WiFi and without using those spectrum blocks defeats the whole purpose. 

Sure they can find the best frequencies and methods for straight wireless power transmission but that's been known and possible for decades, this is really just a proof of concept using WiFi technologies.  "Beamed" or "ambient", which is it? The fact they intentionally made the antenna always broadcast makes me want to say beamed. I'd be curious to see how often, if at all, the camera could get photos on a normal wifi signal. So it is at a higher power than normal signals. Higher total energy transfer at least, due to always transmitting. The signals strength was the same as any regular router could do though. Regarding flair isn't this more engineering than computer science? Power delivery and wifi from the hardware side is very much more engineering than it is comp Sci.  Very much this. This has little to nothing to do with computer science. The most this would have to do with computer science is if there was some new algorithm incorporated into controlling the wifi signal and even then it would be a stretch to not categorize it computer/electrical engineering.

The article seems to focus on the electrical aspects of the signal, not the computational side of the circuit controlling the signal. It is only correct to classify this as electrical engineering. Thanks for pointing it out; I'll change it straight away. I really wasn't sure what to put as a flair (and inevitably landed on computer science), so I appreciate the feedback and will put the correct one. Why does this power-beaming stuff always generate excitement?

It's been known to be possible for generations. It's stupidly inefficient (inverse-squared) and it has few practicable applications.

Reminds me of a tour of JPL. The guides mentioned that sandstorms can shut down one or both rovers (this was back in the Spirit &amp; Opportunity days), and some tourist was emphatically asking why the rover in sunlight couldn't beam power to the other rover.

This concept has a tremendous grip on the imagination. Because we can't really make micro electronics like tiny camera's or sensors because batteries are quite big. If we can beam enough energy to power these devices then this is quite cool. I talked to someone at a convention once who worked for JPL.  That same topic of sand building ip on the solar killing the rovers came up.  I asked him why they didn't just put a wiper brush on the solar panels.  I'll never forget the expression of "well because...wait....well... um...." this guy had without actually saying something.  I figured
my suggestion was either so stupid he was speechless or I just fixed NASA.  I didn't get to find out the answer though because I was interrupted by another colleague. There are several reasons, but the biggest one is that it wouldn't be worth the investment. They designed a 90 day mission and a rover that could accomplish that mission. Adding a brush is a big engineering challenge for questionable payoff.

One problem is that you would have to power up the brush motor after a night or a sandstorm. How do you plan on powering up the motor when the solar panels are covered in sand? I dunno. Dipoles fall off at 1/r, not 1/r^2 . Then, several antennae can be combined using interferometry to give the signal some directionality and improve the falloff even further. (edit: improve the coefficient, not the r-dependence) Dipoles are 1/r^3 For static fields And near fields. Far field is always inverse square, so I don't know what he was talking about.  Looks like he was talking about the dipole moment of a static electric field, rather than the intensity of dipole EM radiation. Time-varying fields fall off at 1/r in the far-field, no matter the antenna (derived from Hertzian sources), but the power falls at 1/r^2 (since you combine mag and electric field). You just can't get way from spherical propagation losses in the far-field. As you probably know, but just for clarification, Interferometry only changes your constant coefficients in front. It doesn't change the variables (wave mechanics don't change) so you aren't really changing the roll-off.

Near-field is more complicated and you can "say" 1/r for power in some cases, but that is just 13 cm for wifi. Doesn't really make sense to talk about loss over distance then. It is more about the complexity of the fields. Suppose you were to place the dipole antennas in a line across one wall of your house, spaced half a wave (about 6cm) apart for 8m or so. It seems like you'd have to go 40m or so away before the falloff seems spherical. &gt; Dipoles fall off at 1/r, not 1/r^2 . 

shouldn't it be inverse cubed, since you're basically looking at the derivative of the strength of the magnetic field with dipoles? i dunno i never took a physics course

edit: [yep, my intuition was right](http://en.wikipedia.org/wiki/Force_between_magnets#Magnetic_dipole_moment) That's for static fields Thank you. This should be understood before people debate it's validity. 
 [deleted] [deleted] Contributions as listed in the paper:

"We make the following contributions:
 We introduce PoWiFi, a novel system for power delivery using existing Wi-Fi chipsets. We do so without compro- mising the Wi-Fi networks communication performance.
 To achieve this, we co-design Wi-Fi router transmissions and the harvesting hardware circuits. Our novel multi- channel harvester hardware can efficiently harvest power from multiple 2.4 GHz Wi-Fi channels.
 Weprototypethefirstbattery-freetemperatureandcamera sensors that are powered using Wi-Fi chipsets. We also demonstrate the feasibility of recharging NiMH and Li- Ion coin-cell batteries using Wi-Fi signals.
Finally, we deploy our system in six homes in a metropoli- tan area and demonstrate its real-world practicality."

 Isn't this a more complex form of what Tesla was trying to do, by sending power over RF waves? It's not a more complex form, it's a simpler form. The waves aren't directed at the energy capturing device, so it's never going to work, unless you get a super powerful router. it may work for select applications. Reddit is always fascinated with the next big tech that effects everyone and always dismisses the numerous small tech jumps that effect a limited number of applications, forgetting that they make up the vast majority of technological advancement. Well the paper indicates that it is able to power their harvesting hardware (no information about it was designed?). Said hardware was connected to a few devices. They were able to keep a few low power devices, such as sensors and small cameras, running.

Edit: It is also worth noting that the paper never mentions the use of a "super powerful router"

Edit2: Okay, so it looks like it is not an ideal mechanism for energy transfer, but it's still perhaps misleading to say that it doesn't work without a powerful router. The applications for this technology with even a normal router are very broad. They were able to power their harvesting hardware because it consumed less than 100 microwatts. Their definition of "running" is very loose. With the camera, they take a low resolution greyscale picture with an extremely efficient camera only once every few minutes. It's pretty much the same thing that people have been doing since radio was invented. It's the same principle that a crystal radio operates on. 

The fundamental problem though is that power increases exponentially with distance. So you can either have decent power over short distances, like current induction chargers, or you can have low power over longer distances, like wi-fi or radio.  [deleted] [deleted] It would be interesting to do an overall power efficiency calculation here. Their scheme requires ambient routers to always be in transmit mode: signal or noise. When does the cost of this constant transmission overtake the benefits of intermittent energy scavenging?

Transceiver electronics are also operated in burst mode to reduce the heat load on the components. Having them always in on state may reduce component lifetime. 

It would seem that the issue they're tackling is that there isn't enough power integrated over a large time to have enough energy for the capacitor to fully charge (signal bursts are too short), so they're modifying transmitters to always be on. Why can't an on board battery in conjunction with this capacitor work? The energy bursts can be temporarily stored in a capacitor, which then quickly discharges into a rechargeable battery. Do this long enough such that "burst mode power x time = energy from continuously on state" and you'll have the same power to operate the camera.  You raise a good point about increased load resulting in excessive heat reducing the lifetime on the device. It might be pretty practical in a case of charging a battery, for example, to have whatever device you could be charging send a request to the router to enable the technology and then disable it when it's either charged or specifically instructed to stop. Is "beamed" the right word here? No it's not. [deleted] &gt; The battery-charging harvester operates down to -19.3 dBm, compared to -17.8 dBm for the battery-free harvester.

That is a super super strong signal. Typically for a first class, high performance wifi network, we target -65dBm for the coverage area. Another way of looking at it is -20dBm is about 10,000 times more power than -60dBm

-60dBm = 1e-9 W

-20dBm = 1e-5W So all those folks worried about wi-fi signals cancerizing them might not be so far off after all?! I don't really see how this is any different from pointing a flashlight at a solar panel. Why do people get so excited about this kind of thing?

Edit: I read the paper. The novel thing they did was that they put in noise between gaps in the wifi signal so there would be a constant flow of power. It's looks magical for people with lack of understanding of electromagnetic radiations. \[semi-rant\] Don't we have enough problems with noise and interference with radio communications? Now we are purposely broadcasting it?

All i see this doing is giving fodder to the "i'm allergic to wifi's .04 milliwatts of energy" crowd." Just plug your phone in when you go to bed and have a decent battery in the device. no need to waste power fighting the inverse-square law or a wall with more than 3 layers of paint. who cares what they think? Quick everyone make a Nichola Testla comment!!! Isn't this something already existing? From my basic telecommunications knowledge, this is more or less how a passive RFID tag gather its power to send information back to the reader. Is this "only" an advancement in the operation frequency and efficiency? As far as I remember, RFIDs need ad hoc modulation and frequency range of the incoming signal. Is this an attempt to harvest energy from a common electromagnetic field we're often in range? I guess it'll be cool to recharge your phone just staying in house, even if by this experiment it looks like we're still far from the power required. The amount of power is significantly higher than normal RFID from my understanding.  Alternatives to beaming any useful amount of power typically have other hazards (microwave beams for example), have higher amounts of loss from the atmosphere (which as a general rule scale according to how far away the wave type is from visible/ultraviolet), or have simply been too expensive and unwieldy.  We don't have any specific uses for this yet, but as the saying goes, "Build it and they will come." Our devices keep being made on ever lower power processes. Maybe soon it will be useful to harvest already available radio waves for some small devices. Worked for a government contractor for a few years just after that Minneapolis bridge collapse. A branch of the company made micro - sensors that could be mounted on a bridge to broadcast stresses real time. The sensors were powered in a similar way, but they depended on a lot less voltage. Impressive that they have been able to scale this up/produce a low enough power camera. I know it's not possible due to the amount of power available, but I'd love to have a wifi phone charger. This is a great solution for all those times I ran out of camera batteries and was sitting inside several ambient wifi signals.  If this catches on the 2.4ghz band is going to be almost unusable if you live in an apartment.  Isn't this kind of a huge deal? Maybe not now, but for future technologies? I do not understand why this is news.  MIT did this back in 2007 by beaming power wirelessly into a 60W light bulb : http://newsoffice.mit.edu/2007/wireless-0607
 Shh. Don't tell the people who believe they are allergic to WiFi about this.  Isn't the title wrong?  It would be "Scientists has successfully powered a small camera using ambient wifi signals"? Hooray. The vision Tesla wanted to provide to us many decades ago, finally starting to be realized. His ambitions were a little larger, but hey-- maybe we'll get there. Is this what Tesla had in mind?  This isn't actually revolutionary, you can do this yourself with RF and a few parts.                 ~~~*ambient wifi signalzzzzz*~~~ when you chill AF Tesla Teslaed Tesla Teslas ago. Woo, something tesla was doing decades ago Does it work with cellular data signals as well?  &gt;success

this is going to be great for privacy! My old college advisor is one of the authors for this study. No guarantee he'd have time do it but I could ask him to do an AMA if there is interest. He likes connecting with the public.  Please do get us that AMA. I'd be really interested if you could arrange this! [deleted] [deleted] I'm interested, please ask him. Losey or Danforth? I had Losey for IPM a year ago, he's a good guy. I just had Losey for IPM and work for Danforth, so I could ask the other Cornell researcher too!  That would be a huge solid. For reddit, and the bees. That's great, please do ask! Does he do cooperative extension stuff? Here is the peer-reviewed journal entry: http://rspb.royalsocietypublishing.org/content/282/1809/20150299 It's behind a paywall. Does anyone have access? I'm curious about what pesticides were used exactly. This recent report (May 2015) by NPR provides information on the results of neonicotinoids.
http://www.npr.org/sections/thesalt/2015/04/22/401536105/buzz-over-bee-health-new-pesticide-studies-rev-up-controversy

edit: and links to two papers that were cited within the article.

Ecology: Tasteless pesticides affect bees in the field
http://www.nature.com/nature/journal/v521/n7550/full/nature14391.html

Bees prefer foods containing neonicotinoid pesticides
http://www.nature.com/articles/nature14414.epdf?referrer_access_token=RTNAGJ0r7txW1VMgB2OwctRgN0jAjWel9jnR3ZoTv0MduC3nHCOl2J2ECXCmE9hd2x8RlCnEGmk8ywijoGHvRvePt5iOSRTu235EXr9Vo8M8UVsrY8ZiC61C-wfCSm5PlFg-e2VHR9keu0RBYfVXXCZWjcNaB0dIeD8HpIAZLYvwCs_FLBwVTHSFfhh8qQp1V2SImHnv-2MCHZRpm-LH1A%3D%3D&amp;tracking_referrer=www.nature.com The article is behind a paywall. They do make their materials and methods [available](http://rspb.royalsocietypublishing.org/content/royprsb/suppl/2015/05/30/rspb.2015.0299.DC1/rspb-2015-0299-File004.pdf).

It says that the bactericides are copper formulations, which act as both a bactericide and fungicide. The link does not list the brand names of the bactericide used. I searched for a few names online: MasterCop, Camelot O Fungicide Bactericide, Champ Dry Prill Copper Fungicide Bactericide, ChampION, Badge SC, Junction WSP Fungicide/Bactericide, Agri-Life. As far as the chemical formulations go, copper sulfate pentahydrate, copper hydroxide, copper oxychloride are listed. There are probably many more, since copper appears to be a very popular method of pest control. Is this what you meant?

Maybe someone will jump in with more information on what this data means.  Copper fungicides and pesticides are allowed in organic farming.

Edit: I think what I should have said is "Interestingly copper fungicides and pesticides are allowed in organic farming. I suspect this study however will be trotted out to show how bad Monsanto and Big Ag are."

 A lot of pesticides are allowed in organic farming.  In many cases the same chemicals are used in non-organic/organic farming because there are limited alternatives. I believe Plums were sprayed for the longest time with antibiotics because the alternative was to have your crop decimated by disease (and go bankrupt). 

If you are really into the organic scene, I suggest going to a local farmer market. It may be more expensive but I really enjoy getting to know your local farmers and ect... You must first define what a pesticide is. You're eating right now,  due to them.  Pesticides enable feeding a population on a large scale.  Using anti fungicide on plants goes back as far as ancient Greece, bees weren't wiped out or dieing.  But they are being wiped out and dying now, so what's different from Ancient Greece besides industrial chemical pesticide formulation? The difference: dealing with recurring famines, crop failures, far less productivity from an acre, and far less population to support that wasn't directly working off the land with access to alternative immediate food sources.  Because we use the products we use, in the volume we use them we don't have the problems they do, we have different problems, hurting out bees being one.

The Greeks might have been using anti-fungicides but it wasn't on the scale we use them across thousands of acres all at once in monoculture fields surrounded by other monoculture fields. If a Greek farmer used a fungicide on a specific crop his bees (and his not a commercial rented set brought in for the season) would only get a small amount of fungicide because they were pollinating not only his treated crop but his other untreated crops and those of his neighbors. In the modern day if a farmer treats for fungus its likely the bees have no other sources of intake since his fields are so large, there is nothing in them but the crop in question and the neighbors are treating at the same time with the same formulations because its done on a schedule not in reaction to an outbreak. 

It has little to do with industrial chemical pesticide formulation, it has to do with practices of use. More natural chemicals used in the same manner would be having similar effects, we just use the industrial chemical ones because they are more effective on the target, easier to produce, transport, apply and cost effective. It's true that copper fungicides are used in OG farming but what sets those copper products apart is often the co-processing agents used in the non-OMRI (Organic Materials Review Institute) copper.  Some hurdles to clear for a product to be considered for organic use include no acute mammalian toxicity and that it is a naturally occurring substance and those additional agents are often not allowed.  Can't read the paper, but I wonder if this distinction is discussed. I'm not trying to make a point for or against organic farming. The little bit of info I gleaned from the materials and methods in the study mentioned copper. What would you suggest as an alternative.? Why not use the newly developed ones?

They are good value for money and tend to be less toxic. See this about Roundup for example

http://weedcontrolfreaks.com/2014/06/salt-vinegar-and-glyphosate/ thank you Apples are a tough bag. I live in the stunning Columbia Gorge, where small organic operations are encouraged, and everywhere. Yet, the orchards are still heavily sprayed to the point where we can not have salmon hatcheries on the Hood River because the arsenic level is way to high. Not to mention the Columbia River itself, which is actually extremely gross. As the shark free capital of Kite-Boarding in the world, these athletes constantly struggle with bizarre ear, nose, and throat infections all summer long. Disappointing, Because visually, this place is the TITS. Are the small organic operations leaching arsenic into the river?

 No, that was badly worded.  In spite of all of the encouragement to grow organically there are still a lot of conventional orchards. Except arsenic is an organic pesticide. As opposed to short lived synthetics. The term "organic" in organic arsenical really has nothing to do with the term "certified organic" as it pertains to food.  This is a common point of confusion.  Synthetic compounds like organohalides are also "organic" but not allowed to be used on organic produce. To some degree. For example: 

http://www.ecfr.gov/cgi-bin/text-idx?SID=e386366704ccae8326c012543c3f53ea&amp;mc=true&amp;node=se7.3.205_1105&amp;rgn=div8

Specifies synthetic and organic substances. And I know it depends on the certifying body. I was using a dumb rule of thumb that usually lists any synthetic as "only on inclusion" and organics as "prohibited only if specified."

But yes, I made that shorthand mistake. My apologies. Alright, here is the list of prohibited organic substances for USDA certified organic. Now the question is, Is MSMA organic or non-organic? And did it count as Arsenic, or is the prohibition against only pure arsenic?

http://www.ecfr.gov/cgi-bin/text-idx?SID=e386366704ccae8326c012543c3f53ea&amp;mc=true&amp;node=se7.3.205_1602&amp;rgn=div8 http://www2.epa.gov/ingredients-used-pesticide-products/monosodium-methanearsonate-msma-organic-arsenical

This is the current EPA status of at least MSMA. I'm unsure wether this qualifies it in orchard use still. And the information from /u/BasaltFormation might be a little out of date. "In 2009, the MSMA uses on athletic fields, parks, residential lawns, forestry, non-bearing fruit and nut trees, and citrus orchards were also canceled."

Also, to be certified in Oregon is to be TILTH certified, and that is a whole different ballgame. 
 I was more concerned about 

"As a result, MSMA is still allowed for use on sod farms, golf courses, and highway rights-of-way. The amended cancellation order for organic arsenicals describes the status of these uses and existing stocks provisions in more detail."

So yes, you're correct. But up until 2009, they were organic pesticides. I am not aware of TILTH requirements, but I'll take your word for it that they're more stringing. My understanding was that it is also used in conventional orchards, and is not currently allowed in most organic farming. I understand.  

That's truly sad.  The Columbia River Gorge is magnificent, and is worthy of preservation.

Destroy hatcheries, destroy water supply, destroy habitat...

So sorry to hear it. Yeah, I see that now.  No, it is the big operations that do. With that being said, organic apple operations are few and far between.  Right, I garden using Integrated Pest Management techniques.

Peaches and Apples have been two of the most difficult to manage from year to year.

All respect to those that work out a sustainable balance. If you are curious about organic apple packing operations I'd be happy to show you the many local organic operations and I work for a large sales company that sells organic apples. I would assume that's what he meant since arsenic is organic. As an apple farmer in western New York I know that we haven't sprayed arsenic containing compounds on fruit trees in over 60 years. The problem with arsenic is that it is a heavy metal and it persists in the soil for a long time. But please don't demonize us for what our grandfathers did. We do everything that we can to balance the need of the environment we grow in and the need to grow a profitable crop. We do not want to kill all the pollinators that we rely on to grow a crop, we do care about the bees. We also employ a lot of people who rely on our ability to grow a profitable crop to feed their families. It is a tough balancing act that I don't think the majority of the population who are removed from the food production industry can really grasp.  As an ignorant city dweller I always assumed apples didn't really need pesticides, or at least not to the same scale as other squishier low hanging fruits (like blueberries).  In order to grow the pretty red apples you see on the grocery store shelves it takes a lot. Apples have long growing season so they are out in the orchard susceptible to all sorts of fungal diseases and insects. They are also such a high value crop that any disease or insect outbreaks would be disastrous if you are trying to run a sustainable business. In short we use a lot of pesticides but only when they are necessary to avoid damage to our crop.  As a home grower of apples I can attest the very much need pesticide. I have 4 trees in the best year they produce about 400 apples in total. Over half those will be destroyed before harvest time by various insects and that was in the years I sprayed. Now that I have bees in the yard I do not spray and its closer to 2/3rds are damaged if not destroyed before harvest. 

A damaged apple might be just fine for me to cut out the bad part and make into applesauce, its unacceptable for a commercial farmer because the public won't buy it with a defect. Removing random parts of the fruit for applesauce or juicing is impractical when considering the volume so its unacceptable to him for that purpose also. This is exactly what I was getting at honestly. I wasn't pointing fingers either, just stating that apples are a 'tough bag'. The few farmers that I know of who practice TILTH at their orchards do it as a labor of love, and are not profitable. Hell, fruit trees in general are a labor of love. Great pear harvest last fall though! This is the thing people don't get, organics still need pesticides. Certain GMOs are aiming to produce certain strands that may not need pesticide sprays or less of them.

In fact, I read somewhere (can't find it), that organic growers are more likely to use very harmful pesticides from the 70s out of some misguided distrust for anything new or synthetic. Even if the old stuff they use is also synthetic. It's actually amazing what some people believe. The problem is, when you just want to have a nice garden, good luck deciphering what is truth from fiction. It's incredible the amount of misinformation.  I love to skate on the old historic trail! I didn't know that the water was so bad. Quick question (I hope): what do they spray the apples to they don't turn brown once one takes a bite?  Thanks and sorry for being off topic I think you're thinking of the new Genetically Modified apples that don't brown when exposed to oxygen. That isn't related to spraying but rather GMO.  GMO apples are currently produced by one outfit in wa state and are not very much liked but most apples producers in the area do to the fact that people confuse GMO with grafted, but there are other varieties that don't brown easily that are not GMO.  I work in the apple industry in central wa.  Why aren't they much liked? Genuinely curious. Is it because GMO, taste, appearance, or something else? Lemon juice will prevent sliced apples from turning brown. Are you trying to say that arsenic is still being used in the orchards? Btw I love me some Sawtooth Pizza! Honest question, how come we read about Mosquitos populations becoming resistant to pesticides like DDT, but bees don't seem to be gaining resistance? Does it have to do with their method of reproduction? Probably because any mosquito can fuck, but only one bee can. Kinda limits how much variations can happen over time.  Bees are a completely different type of insects.  Urban beavers are also uncommon, but Raccoons are not. Don't the laws of selection still apply to bees though, regardless of the type of pesticide causing the selective pressure? As in some gene that codes for resistance will gradually become more common in the bee population over time, if there is a pressure selecting for it? I mean pesticides are still very effective against mosquitoes despite increasing resistance, so I don't really see how the two situations are different.  &gt; As in some gene that codes for resistance will gradually become more common in the bee population over time, if there is a pressure selecting for it?

That assumes that there is an existing gene to be exploited.  Sometimes a species can't adapt and becomes extinct. There are a million factors, one being that if you compare bees and mosquitoes: one entire colony would equal one male and female mosquito as far as genetic diversity, so mosquitoes would have bees outnumbered by the billions, and the greater the population the more likely adaption can happen. Bee's simply might not be able to reproduce fast enough with enough diversity to be able to adapt, and that's if it's physically possible, it might be like trying to adapt to an a-bomb. Mosquitoes are wild, they go through constant processes of selection. Bees are domestic, like tiny sheep, humans do more of the selection, but selection for resistance usually comes later and there needs to be some serious breeding programs. Most bee keepers select primarily for queen bee's intense egg laying and for production in general (bees gathering more, better) or for bees being calmer.   Democracy Now! reported on organic agriculture in cuba and interviewed a cuban beekeeper who mentioned that  bees there have not been experiencing colony collapse disorder.      Have there been any peer reviewed studies analyzing this?

( link to the DN!  transcript:  http://www.democracynow.org/2015/6/2/organic_farming_flourishes_in_cuba_but )
 [deleted] [deleted] [deleted] [deleted] [deleted] As you probably heard on that program Cuba utilizes some pretty extensive organic practices. That farmer you heard on the program, Fernando R. Funes-Monzote, is also a professor.  He's co-authored some good papers. Here's a paper relating bee populations to the type of agriculture Fernando Funes researches and implements, agroecology. 


[Plant biodiversity enhances bees and other insect
pollinators in agroecosystems. A review](http://www.researchgate.net/publication/257805443_Plant_biodiversity_enhances_bees_and_other_insect_pollinators_in_agroecosystems._A_review)
 What do organic pesticides consist of? Most naturally occurring and biological substances can be used in organic farming. For example, there has been a big push recently toward "biopesticides" which often include symbiotic bacteria/fungi and/or extracts from those organisms that protect the plant from a pest or pathogen.  Oh, like the natural bacterial toxins produced in GMO corn? :)  Don't get me started. At risk of doing just that, I'll just say the fact that organic farmers can use isolated Bt toxin but not Bt corn is one of the most nonsensical dichotomies in the world of agriculture &amp;ndash; and that's saying something since, as I'm sure you know, there are plenty. Isn't this the smoking gun that directly links the poor agricultural practice against U.S. agribusiness?  The correlation is astonishing... One organic farmer in Cuba does not mean that US practices are the blame. Data not significant. One of the main problems is American consumerism which drives how agriculture is done. Corn and its byproducts, for example, has an insane [amount of uses](http://static3.businessinsider.com/image/5005e30decad04a548000005-960/corn-infographic.png).

As a farmer, or a buyer of product, you're going the biggest bang for your buck for most of this. You're not going to spend a small fortune on a limited supply which is what organic would get you, you're going to utilize the land and use whatever generally legal means to get the most, cheapest product you can so you can turn the highest profit. This means using the cheapest pesticides to ensure your crop is the largest in can be. Same can be said with any produce really. 

If you can adjust the way Americans consume, you'd probably actually find it would help with ecosystems and saving important species like bees. all of those things that corn do would still need to be done and would be replaced with some other crop. Another crop might not have as high of a yeild which might result in more damage. You're generally on point, although many farmers are feeling some pain from low prices per bushel due to overproduction of corn. The lower prices coupled with higher cost to produce has many farmers operating at a net loss if they don't change anything soon. (And that includes any cash boost from crop insurance.) we send a lot of our corn to oher countries. And we now get from 1 acre what took 4 acres to produce just 10=20 years ago. So there are working on a lot of angles to the situation, but its not American consumerism per se,... we are the worlds bread basket in many respects. [deleted] If it's not going to be corn, it can and is potatoes, rice, soybeans, wheat, or whatever plant product we can use as a source for starches(sugars) and proteins. In the US, it happens to be corn because there's so much land in the States that's perfect for it.  Does this study link it to the increasing prevelance of colony collapse syndrome, though? After reading this paper, I have two major complaints.

1) This is a two year study, meaning that Year x Bee factors are highly suspect. Maybe 2012 was a particularly bad year. You can't establish a trend from two collection years. 

2) I am highly suspicious of the summing effect and over simplification of the Pesticide and Bee Impact scores. Taking complicated pesticide application and reducing them down to single numbers tells me nothing about what's responsible. If an orchard is suffering from weather problems, it's going to be more susceptible to pathogens, and the farmer is going to spray. Wasn't 2012 an exceptional drought year? If an orchard is heat stressed, it's going to get whacked hard by opportunists. 2012 WAS an exceptional drought year. What did the studies authors do to deal with the fact that everything was hot and nasty? No, [2012 was dry for parts of New York State](http://droughtmonitor.unl.edu/MapsAndData/DataTables.aspx?NY), but it wasn't an exceptional drought.  Good points. If you're interested in posing them to Dr. Park, her email address is provided in the paper for correspondence. It would be interesting to read her responses (or those of any of the other people involved with the paper). I live in Ithaca (I work at Cornell even, but not in the researcher's school). I am assuming that they did their research using cornell apple orchards which I am literally riding by (on the public bus) right now. We have a massive lake (Cayuga Lake) which shields the nearby parts from getting too dry. 

2012 summer was pretty hot but not overly dry. When we get extreme weather periods it affects the local wineries and pretty much everyone complains about it (this past winter was particularly annoying, for example... And Ithaca was in national news because we are snarky bastards)

I agree that a period of 2 years could have been heavily influenced by contemporary factors that may not have been controlled for, but strongly doubt that drought would be one of them.  I'm more irritated about the only one measurement of effect (essentially). If the whole point is to measure how bees respond to management regimes, then at least 3 years is necessary. I know with Crop Science (the journal), to have a two year study was often questionable and rarely made it past review.

By only measuring for two years, we've only observed really one year of effect (2012). While you can measure as many sites as you want, any large scale envirionmental effects (such as year effects) will be confounded with what you're trying to study. In this, I think the authors mixed a certain level of unluckiness in picking an aberrant year and a mite of rush to publication (probably owing to the fact that the lead author probably did this as her dissertation). I keep posting this and this is the time of year it is a splendid sight: 

Buy a pack of bee and butterfly-friendly flowers, 
find a piece of land that is not used, get rid of most of the weeds and put the seeds there.  

Currently, I have hundreds of bees in the garden on my strawberry, raspberry and blackberry plants.  
It's beautiful, NO pesticides or herbicides whatsoverrrrrrrr.  

It costs you a little bit of time, a few bucks worth of seeds and gives the bees and other harmless pollinators a safe haven to do their job in. Safe. Clean. Beautiful.

Proof (a row of my raspberries next 2 red berry bushes and surrounded with the types of flowers I talked about) : http://i.imgur.com/M5NKARR.jpg  

I'd like to think that if a few dozen people do this in a few suburban-block areas a nearby hive has a much larger chance to survive.

Edit Sorry it's sideways :D [deleted] and gets in the water suppy and other animals and bees and birds [deleted] [deleted] Quite a few pesticides are already labelled with "toxic to bees" on the back.  If it kills insects, then it shouldn't be surprising if it harms bees, even if it's non-lethal. [deleted] [deleted] [deleted] [deleted] So let's say this continues and bees go extinct.  This obviously puts a huge strain on pollinators right?  So crops dont get pollinated and we dont need pesticides anymore because the crops dont even grow in the first place?

Or is it more complicated than that? The fungicide aspect of this study is what's really interesting.

We don't need more studies to show that pesticides kill bugs.  This is common knowledge.  Every single pesticide I've looked at very clearly states that it is incredibly dangerous to bees.  They say not to apply them to blooming flowers, and the most dangerous ones say to only apply them late at night (when bees are not active).

The fungicides, though, that's news to me. [deleted] I have a very old and massive apple tree in my yard. I spent a good entire day this spring watching the hundreds of bees pollinate it. Amazing I like to imagine that from across the yard you can hear their tiny little "horray"s.  I don't care about spots on my apples, leave me the birds and the bees. Pleeeease!

On a serious note though...do you think it's possible to actually create a bee safe pesticide? Or is it just if it kills any pest, it will also kill bees? Why are they so sensitive to these chemicals? "pesticide" is a really wide term

what you're dealing with is  chemicals for different specific groups of pests:

- vs butterflies (caterpillars)
- vs flies (larva)
- vs wasps (larva)
- vs beetles (can be both adult and lava)
- vs weevils (larva)
- and many more, depending on the crop, the area, the time of year

 They "did create bee-safe pesticides" Right, but apparently they weren't actually bee safe. I'm asking if anyone thinks it's *actually* possible, or just an exercise in futility  Note the quotation marks. So much of what we do, even ecologists and scientists, does so much more damage than we imagine. My favorite is the [ecologist who killed 40,000 elephants in Africa](http://www.ted.com/talks/allan_savory_how_to_green_the_world_s_deserts_and_reverse_climate_change?language=en) thinking that it would help with desertification and actually caused the inverse, all the while destroying the elephant population.

It seems more and more we need to just stop messing with things. Sure, things make sense because we don't know better now, but who knows the lasting effects 50 years from now? I do care about spots and so do many other people. 

Surely there is something better than abandoning useful pesticides all together.  No one EVER questions the use of fungicides and its appalling! 
Fungicides will be the death of the planet because how ignorant some are about soil life. Not only that,  bees highly depend upon fungi as a main source of food. Yet, even after several major peer reviewed studies, people still support the increasing use of pesticides and fungicides that go hand- in - hand with GMOs.  Actually GMO crops use significantly *less* pesticide on average than non-GMO crops. We're able to breed plants that produce their own pesticides that specifically target the organisms we need to, which means we don't have to douse the fields in broad-spectrum pesticides.

http://reason.com/blog/2014/11/06/biotech-crops-use-less-pesticide-study-r

http://www.pbs.org/wgbh/nova/next/nature/fewer-pesticides-farming-with-gmos/ Now lets not go railing against GM food, the same processes that produce cooperative effects (Roundup+Roundup ready corn) can be used to just improve the plant's resistance to a threat. IE fungus.

It's the application that can have negative ecological impacts, not the fact that its GM 85% of all plant species are symbiotic with fungi/mycelium. Before commenting about how fungi are a threat to crops, please read up on how MONOCULTURES are the reason fungi wreak havoc. Plants that are non symbiotic with fungi are usually devastating to environments as well. For example : Yellow Mustard.   BT removes pesticides from hand, and pesticides are used in organic agriculture.  No they are not. In a smart organic 'garden', most intelligent horticulturists will grow plants that attract specific pests as a distraction next to their crops instead of using pesticides.  GMOs either come with instructions for companion planting or the companion plants come in the bag with the GMO seed. 

With GMOs, it's called refuge. 

Organic and conventional operations (non GMO) sometimes use companion planting techniques, but not like you imagine. They'll plant them as a border or perimeter trap/indicator. They'll both spray to control pests when the border crop indicates they've arrived.



 Also depends on who is spraying what on the plant. There are several well known 'organic' pesticides one may use. 
 And some of the organic approved pesticides are more toxic than those not approved for use on organic.  Dish soap mixed with water is more toxic than glyphosphate?  [deleted] [deleted] [deleted] What was the time frame of this study? [deleted] [deleted] Genetically manipulating animals is far harder than plants. We would need to find a genetic factor that already exists in bees that gives them resistance. Then it would be easier to breed that into a larger population than to directly manipulate the genome. [deleted] [deleted] [deleted] No doubt pesticides are one of the highly responsible factors in Colony Collapse Disorder. How about roundup? Roundup is an herbicide and is used to kill weeds and other plants on fields. If the weed is a source of food for the bee then there could be a indirect effect. However, that issue mainly pertains to agriculture in general. Land for crops involve replacing the previous plants/inhabitants (replacing other animal's food sources with human food sources). Roundup just prevents regrowth into the crop fields. If bee food sources are being depleted by crop fields then we would need to optimize for better land-use efficiency.  [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] So the article states "doctors grafted four of her thawed ovarian fragments onto her remaining ovary and 11 fragments onto other sites in her body."

Maybe I'm not getting something because I don't have a medical background, but what do they mean "other sites in her body"? Why not put all the fragments onto her remaining ovary? What good are the fragments that have been placed elsewhere? Is that somehow saving them for later use or are they actually contributing to her fertility? Only an undergrad so bear with me (and someone please correct me if I'm wrong!) but the actual journal article says they were grafted subcutaneously and in the peritoneal bursa (which appears to be a thick lining in the abdomen).

Due to whatever procedure she had as a child to help with her sickle-cell anemia, she never started menstruating. The ovarian tissue itself just appears to contribute to fertility by inducing production of hormones necessary for menstruation and fertility. Due to how cells signal and everything, the transplanted tissue doesn't need to physically be in the ovaries in order for the hormone signalling to occur so they were able to transplant it to other regions.  A minor correction: she never started menstruating naturally.  They artificially induced menstruation at 15.5 years old with an estrogen-progesterone treatment. Is there a reason they would induce menstruation? Health reasons I mean. I assume they induced puberty more generally, since her own hormone production would not have done so. Of the top of my head, lack of oestrogen also raises the risk of osteoporosis as well. Oestrogen ~~actually~~ also has a role in slowing and stopping bone growth.
Boys used to be castrated before puberty to preserve their singing voice (pretty messed up http://en.wikipedia.org/wiki/Castrato). They usually became very tall and continued growing in old age due to the lack of the small amounts of oestrogen that would be normally created by the testes. 
It is also one of the reason why females usually stop growing in height at around age 16-18 and males continue growing in height until age 21-23.
 Estrogen stimulates epiphyseal closure, which is what causes the growth discrepancies you've mentioned, but it also stimulates bone growth (or, more accurately, inhibits bone resorption) and so is also protective against osteoporosis. It is important for bone strength, bone microarchitecture, and mineralization. Although estrogen is not as potent as testosterone and DHT, it does have anabolic effects on bone growth, increasing levels at the onset of puberty stimulates the growth spurt seen in women. I'm not an expert, but my understanding is that an absence of menstruation can lead to potentially-harmful hormonal imbalances. I could see how this would be particularly important right around puberty, so I imagine it would help ensure she matured normally. I'm at work and can't dig up good sources fast enough, but the absence of menstruation is called amenorrhea, and from what I'm seeing, at the very least it raises the risk of developing osteoporosis.  Also the development of secondary sexual charachteristics, such as widened hips, breast development ect.  Menstruation is a sign of puberty and an active reproductive system. You want the reproductive system to be normal otherwise the oocytes (eggs) won't mature in the ovary as they should. Her body needed to be made mature, using hormone replacement therapy, so that all the traits necessary for female reproduction are present (like a mature uterus and whatnot) Once she got get ovarian tissue back, that tissue needed to mature to be able to produce all the hormones necessary for normal function of the reproductive system.

Basically, menstruation is the visible sign of a healthy reproductive system. They didn't really want to induce menstruation as much as make her reproductive system mature normally. She started menstruating naturally 5 months after the fragments were grafted on (in adulthood), is what he means I think. 

Edit: Nevermind, I see he was referring to when she was younger and didn't start menstruating naturally. I haven't had my coffee yet.  That's amazing! Thanks for the explanation. [deleted] [deleted] [deleted] Endocrine hormones, which you probably have heard of, are characterized by the fact that they are secreted into the blood and then carried to affect distal organs.

Paracrine factors, which you may not have heard of, are characterized by the opposite, acting on near by cells to guide local behavior and differentiation. Like how that one lady carried her baby *outside* of her uterus! The zygote went through the Fallopian wall, and attached itself along the outside of her uterus and her placenta was eventually found to be attached to her uterus and other organs in her abdomen. Amazing! That's called an ectopic pregnancy (https://en.wikipedia.org/wiki/Ectopic_pregnancy) and not all that uncommon.  They tend to be very dangerous for the mother because the placenta is not very picky about what it will attach to.  I'm not sure what case you're talking about, but it wouldn't have been terribly remarkable unless she was somehow able to bring the child to term. The vast majority of ectopic pregnancies are when the egg is fertilized in the fallopian tube and latches on there, which makes more sense because that's a normal place for an egg to be, at least temporarily. What /u/ifartmeat  is referring to is also called "abdominal pregnancy", which only makes up a very small subset of ectopic pregnancies, and even in that small subset only a very very low percentage can carry it to full term. So yes, it is very uncommon. That's an ectopic pregnancy. There are a handful of cases where ectopics were carried to term and born alive. In one case, it was even triplets: two in the womb and one outside.

However, that's very rare. The vast majority of ectopics either end naturally within the first few weeks or have to be aborted to save the life of the mother. I'm not an expert but this seems correct to me. It is probably even more ideal for the hormone secretion to be spread out around the body in this case. I would imagine that the implanted ovary fragments do not produce as much of the necessary hormones as a natural ovary but spreading out the fragments would help to mitigate this issue. Also with 11 fragments throughout the body if one fails to take, it will not significantly effect the overall procedure. The procedure would be more likely to fail if the entire ovary was grafted into a single location. Peer-reviewed [paper](http://humrep.oxfordjournals.org/content/early/recent) published in the journal Human Reproduction. [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] This is really quite fascinating. So the tissue transferred had to be enough that it contained ovarian tissue and eggs. Then that it still responds to the hormone changes that trigger egg maturation and release... She ovulated! I mean, She conceived naturally! 

Would her eggs, being technically younger, been more likely to be healthy, or would she have beaten some odds conceiving as she would have had fewer eggs (like in diminished ovarian reserve?). 

I'm carrying an IVF baby and find this all amazing.  What's really interesting is how this can be applied to space travel. 

One of the major concerns with long distance space travel is prolonged exposure to low level radiation could cause infertility. However if you could created a shielded capsule and then re-graft the eggs back it would seem possible to do extended multi-generational space flights to colonize.  If we manage to beat both that AND cancer, radiation might just become completely benign. There's always the option of making the passenger compartment itself shielded, once technology or desperation makes the high, wasteful energy cost of heavy vessels less of an issue. That's not how it works.  What other effects than sterility and cancer risk does low level of radiations causes? Yeah I just meant any range of radiation. Maybe very low levels wouldn't be bad but I have no idea. Apparently when we leave the Earth's protective EM field we will be open to so many ranges of radiation that we wont be able to handle much of it no matter how prepared we are. This is an excellent point, and needs to be more visible. It would be easier to shield the entire ship than to create the kind of medical facilities required. It might come down to cost and space. Inter solar or inter stellar travel is going to need a pretty good medical facility if you are going to use it for colonization. I well stocked medical facility with the option for surgery on a multi-generational voyage is pretty reasonable. Getting a massive amount of shielding might not be. Not to mention cosmic rays might penetrate and damage reproduction anyway. Having a back up plan which has an extra set of protections isn't a bad idea.  We could test ray penetration in orbit unless we are travelling closer to the sun Also, we could attempt to reverse the polarity and see if it would work. Only one of the ovaries was excised, the other was never removed. The frozen ovary fragments were then re-grafted into the patient (some near her remaining ovary and some subcutaneously). The re-introduced ovarian tissue presumably began to secrete estrogen and a few other normal hormones, which contributed to spurring her remaining ovary to ovulate.

It's an interesting "look at what we can do" case report, ~~but I have to imagine that the same result could have been achieved more cheaply and with fewer (surgical) risks to the patient through injections of hormone supplements. Don't expect this procedure to ever become anything like standard of care.~~ See below for explanation of why the ovarian grafts were necessary!

Edit: If you check out [this](http://www.reddit.com/r/science/comments/399351/worlds_first_baby_born_from_frozen_ovary/cs1rdsu) great post by /u/repro_pro you can see he corrected me on why the grafting process would have been necessary. While injections of estrogen/progesterone were able to induce menstrual cycles in the patient, the chemotherapy she underwent chemically ablated all of the oocytes (egg cells) in her ovary that was not excised. The frozen fragments of the ovary that was removed were tested and they found that they *did* still have functional oocytes, so (as far as my understanding extends), they were able to graft those fragments onto the existing ovary and reintroduce oocytes into the patient's reproductive system. That means that this procedure (or at least the ovarian grafts) was definitely necessary for the patient to conceive naturally This is not completely correct. The oocytes of the ovary that remained in her body were destroyed by the chemotherapy agents used to destroy her blood stem cells before her bone marrow transplant. It states in the article that she had a good follicular count (where the oocytes are located) in the ovarian tissue inserted at the site of her ovaries. You are correct that the ovarian tissue located in other areas of the body will produce the estrogen/progesterone etc... needed for typical cycling but the replaced ovarian tissue is the source of the oocyte that allowed her to reproduce naturally. 

Therefore, this procedure is very much needed and there was not a cheaper way for her to be able to reproduce with her own eggs. If this was not the case she could have gotten pregnant when they had her on the hormone replacement therapy from the age of 15.5. 

Source: professor who teaches reproductive biology and endocrinology.  Thanks for commenting, I'm not an expert on endocrineology by any means. I was under the impression they were able to induce menstruation in the patient at the age of 15.5 by injection of estrogen/progesterone

&gt;Menarche was induced at the age of 15.5 years (Tanner stage A2P3M3) with the use of a third-generation estrogenprogesterone preparation.

If i understand your post right, the artificial hormones allow regular mentrual cycling but without the production of an ovum because oocytes were chemoablated? But the ovarian fragments grafted to her remaining ovary had functional oocytes and because they were grafted to an existing functional ovary (albeit sans oocytes) it was able to produce a mature ovum using the remaining reproductive "infrastrcture".

That would make the necessity of the procedure make a lot more sense to me, but please correct me if anything I wrote here is wrong. &gt;Chemotherapy can destroy the ovarian function, so they removed her right ovary and froze tissue fragments. At that time, she was showing signs of puberty, but had not yet started her periods. Her remaining ovary failed at 15.

&gt;Ten years later, she decided she wanted to have a baby, so doctors grafted four of her thawed ovarian fragments onto her remaining ovary and 11 fragments onto other sites in her body.

Not a frozen ovary, but a transplanted ovary preserved from before she had chemo. She was able to conceive normally after the procedure. Slightly disappointed, but still amazing to be a part of this planet right now. How is it not a frozen ovary? If it was removed and stored externally, was it not frozen for preservation? It wasn't a functional ovary. The excised ovary was fragmented and transplanted subcutaneously (under the skin). It wasn't able to produce an egg, the fragments just secreted hormones that stimulated her remaining ovary to ovulate. Think of them like the artificial suppositories used as birth control; the fragments are just inserted under the skin and secrete hormones into the bloodstream.

Edit: If you check out [this](http://www.reddit.com/r/science/comments/399351/worlds_first_baby_born_from_frozen_ovary/cs1rdsu) great post by /u/repro_pro you can see he corrected me on why the grafting process would have been necessary. While injections of estrogen/progesterone were able to induce menstrual cycles in the patient, the chemotherapy she underwent chemically ablated all of the oocytes (egg cells) in her ovary that was not excised. The frozen fragments of the ovary that was removed were tested and they found that they *did* still have functional oocytes, so (as far as my understanding extends), they were able to graft those fragments onto the existing ovary and reintroduce oocytes into the patient's reproductive system. That means that this procedure (or at least the ovarian grafts) was definitely necessary for the patient to conceive naturally. &gt;  The excised ovary was fragmented and transplanted subcutaneously (under the skin)

It wasn't just transplanted there. The article states:

&gt;Four thawed ovarian fragments were grafted on the residual left ovary, six were grafted in the right peritoneal bursa, and five were grafted subcutaneously using a trocar incision Ahh, okay, that's a better explanation. The ovary itself was no functional as an egg reserve, so to speak, but rather served as a hormone supply to kick the remaining ovary into gear?

Still pretty cool though. Could you explain the disappointment? I thought both would be equally impressive unless you meant a donor embryo.  The transplanted ovarian fragments wouldn't have the capability to produce an egg, they simply began to secrete pretty standard levels of hormones (as normal ovaries do) after they  were reintroduced. These hormones stimulated the remaining ovary to ovulate.

~~The same effect could probably have been achieved by giving the patient regular injections of hormones to stimulate ovulation (like we already do with many women who struggle to conceive) at a much smaller expense and without the risks of a surgical procedure.~~ See edit for correction

This is more of a case study of "look at this wacky procedure we tried" than an example of a procedure that could ever become standard of care.  

Edit: If you check out [this](http://www.reddit.com/r/science/comments/399351/worlds_first_baby_born_from_frozen_ovary/cs1rdsu) great post by /u/repro_pro you can see he corrected me on why the grafting process would have been necessary. While injections of estrogen/progesterone were able to induce menstrual cycles in the patient, the chemotherapy she underwent chemically ablated all of the oocytes (egg cells) in her ovary that was not excised. The frozen fragments of the ovary that was removed were tested and they found that they *did* still have functional oocytes, so (as far as my understanding extends), they were able to graft those fragments onto the existing ovary and reintroduce oocytes into the patient's reproductive system. That means that this procedure (or at least the ovarian grafts) was definitely necessary for the patient to conceive naturally &gt; The same effect could probably have been achieved by giving the patient regular injections of hormones to stimulate ovulation (like we already do with many women who struggle to conceive)

It's not really the same, though. Many women with subfertility have ovulation induced through either something like clomifene or FSH injections, this is thought to be due to dysfunction of the pituitary-hypothalamus-ovarian axis (that is, the interaction of all these organs), for instance resulting in low GnRH which is secreted by the hypothalamus to cause FSH and LH secretion. These, however, all rely on functioning ovary in order to respond to these hormones to produce adequate levels of oestrogen and progesterone.

The patient in this case suffered from primary ovarian insufficiency (after they withdrew her previously administered exogenous oestrogen and progesterone in order to induce menstruation) which they confirmed through lab tests. This means that even with clomifene or FSH her ovarian function is not high enough to produce the levels of oestrogen and progesterone in order to allow for normal fertility. This is what has necessitated the ovarian transplantation, and so the options you listed do not apply in this case. Don't those injections come at a risk of multiples and a wide slew of side effects due to high levels used? If the ovaries were grafted subcutaneously it seems like there would be less of a risk, and you'd get a more normal/natural production of hormones. In fact, I would suspect that the presence of ovaries has more than just a fertility function, as estrogen/progesterone are needed for normal development and functioning I would bet their presence means she would no longer have to take hormone supplements on a daily basis for normal living. Some people don't respond to synthetic hormonal supplements. Due to how they are usually delivered, (pills or creams or injections cause a lot of hormonal highs and lows compared to steady stream from a functioning ovary) there can be a lot of negative side effects, not to mention expense and inconvenience. Using someone's own ovary seems like a good solution that has the potential to improve quality of life Thank you for this. Every other comment didn't quite explain it. How is it not frozen if it's "preserved" and "thawed"? Missed the last paragraph, did you?

"About 40 babies have already been born across the world using frozen ovarian tissue taken from older women."

So your disappointment, for whatever reason, is unfounded. What you proposed is a been-there-done-that of science. Nope. This girl hadn't had a period and her ovaries hadn't developed all the way. This is amazing for anyone who wants a safeguard against future infertility. Think about it. Your parents could have samples removed when you are a kid. If anything happens, you still have options. We didn't know it was possible before. ....ok... still missing what part of this you could possibly be disappointed by, then. Thanks for that. I never actually read the articles and hope that someone somewhere took the meat of it and did a tldr eli5 version. [deleted] [deleted] [deleted] [deleted] /r/science has a pretty strict comment policy that people don't tend to follow very well.

http://www.reddit.com/r/science/wiki/rules#wiki_comment_rules But she had the ovary removed at 13.  Puberty had already started.  The real question is if you do the same thing with a 5 year old will it still work?  In recent years they've been getting parents to freeze some ovarian or testicular tissue sample from their young child in hopes that some day the science may be around to give the tissue an artificial puberty and develop some sperm or eggs. According to the paper, she had shown signs of puberty in that she had breast development.  She had the ovarian tissue removed before she had undergone menstruation for the first time. At about 16 she had menstruation artificially induced with hormones. That is why they are considering her to have had immature ovarian tissue. So its not entirely immature ovarian tissue, but it definitely needed to undergo maturation after being grafted back on to her.
 There is a technique called In Vitro Maturation which can be utilised. It allows immature oocytes to be matured in vitro before fertilisation. The prospects at the moment aren't as great as IVF but it does work. There is also a great amount of research going on at the moment looking to improve it . &gt;  At that time, she was showing signs of puberty, but had not yet started her periods.

She was not able to get pregnant when she had them removed. Puberty and hormone changes start long before a girl gets her period.  The period only starts once she's able to get pregnant.  So her ovary could have already been 50% or 75% ready when they removed it.  Unlike a 4 or 5 year old who has had no hormonal changes so has had no changes to the ovaries. If you have an ovary removed, will you only get a period once every other month? Or will you only be fertile every other month despite normal period intervals? Or will the remaining ovary somehow know to pick up the slack? The paper details that they induced menarche (the first menstruation) using oestrogen-progesterone combination pills. In lieu of working ovaries that respond to secretion of FSH and LH to produce oestrogen and progesterone, this external source of progesterone and oestrogen is what kept her menstruating.

In terms of having a single working ovary remaining, the poster below who was downvoted for their sarcastic remark was correct. A single ovary is just as able to keep up production of oestrogen and progesterone for the body, much like a single kidney can still adequately filter the blood and produce urine.

edit: If my answer wasn't clear, since the levels of these hormones are what control the menstrual cycle, as long as there is adequate levels produced by the remaining ovary, the menstrual cycle will occur as normal, irrespective of there being one ovary. The idea that ovulation alternates from one ovary to the other is false, and so lack of another ovary is irrelevant. I wasn't asking about estrogen and progesterone, though. I was asking about fertility and the release of eggs. For that, the kidney is a terrible simile, as b1rd points out to OtherMemorys reply. &gt; . I was asking about fertility and the release of eggs

Right, but they are all related because they are important in regulating the menstrual cycle. Principally here it is the increasing oestrogen which causes a positive feedback on the hypothalamus, and this is responsible for the LH surge mid-cycle. Consequently this LH increase then sets of further events causing ovulation.

This is important here because it explains why the patient needed the ovarian transplants: namely her remaining ovary was not working enough to produce adequate levels of hormones, termed primary ovarian insufficiency. The transplant helped in being able to respond adequately to her pituitary hormones allowing enough hormone production from the ovary to make her fertile. It's relevant, yes, but if you were to ask me if that new app will run on windows 10, it's my presumption that you are not interested in a detailed discussion of assembly code. Well I'm sorry if I wasn't clear with my explanation from the offset, I was just trying to explain why I spoke about ostreogen and progesterone even if that wasn't what you asked, because it explains the answer to your question. It's absolutely ok. The issue is that you haven't answered my question.

Imagine if you asked your handyman friend if he could fix your roof, and his reply to you question was: "well, roof tiles are made of clay, which is clever cause they'll resist the weather and they don't lead electricity, which protects the roof from lightning." Well if you explained a bit more about where I've gone wrong then I could help, rather than giving many different analogies.

The menstrual cycle is regulated by a mix of different hormones, these transplants have allowed her ovaries to produce adequate amounts of them. So it doesn't matter if there is only one ovary, as long as the amount is sufficient, she will continue to get her periods, not withstanding the lack of any other ovaries.

The idea that ovulation alternates from one ovary to the other, necessitating your question it seems, is completely false. It is mere chance in which ovary an ovarian follicle grows large enough to suppress growth of other follicles, after which ovulation will occur. It is random and therefore if one ovary is missing, it just means that there is no other ovary where a follicle can develop and so will always occur in the same remaining ovary. As a result, the menstrual cycle will happen like normal. &gt; it doesn't matter if there is only one ovary, [..] she will continue to get her periods.

&gt; The idea that ovulation alternates from one ovary to the other, necessitating your question it seems, is completely false. It is mere chance

This is all i wanted to know. Thank you. [deleted] Why do you put "world's first baby born" when the end of the article states the following:

&gt; About 40 babies have already been born across the world using frozen ovarian tissue taken from older women. I think they're referring to the age of the patient.  The headline states that the frozen ovary was collected during the patient's childhood, whereas the 40 women who have already had babies from frozen ovaries had the tissue taken later in life, as full grown women. [deleted] It says 40 other children have been born this way, where do you see world's first? [deleted] [deleted] I mean, this is obviously good news for the patient, but what about for humanity in general?  Now her extreme sickle cell anemia genes live on, and if she has a daughter she might have to go through the exact same thing.  Do people ever consider that? Do you know how many people in Africa have sickle-cell anaemia? It remains prevalent actually because it confers a protective advantage against malaria which is endemic to that area (the patient is from the Congo). As a result passing on those genes can actually be good. So far from your outrage at 'humanity' it's actually partly beneficial for those living in malaria endemic areas to have sick-cell anaemia, and the clinical picture can vary from person to person, it's not necessarily true that her child would have to undergo the same thing.

Before you get outraged in the future, maybe consider that people are in charge of their own lives and just because they can pass on a disease doesn't mean they shouldn't reproduce, or indeed that it in any way affects the rest of humanity. I did know that.  Most of them don't have sickle cell anemia so badly that they have to undergo bone marrow transplants, but thanks to this woman reproducing, more people will.

I'm not outraged, I was just raising what I think is a fair point to consider that is often overlooked.  I don't believe anyone should have the power to say "No, you have this crazy genetic disease so we're not going to let you reproduce," or that anyone should even tell them that they shouldn't, I just think people need to at least consider the long-term ramifications of reproduction.

But hey, thanks for thinking the worst of me and writing such an extraordinarily condescending reply. &gt;Most of them don't have sickle cell anemia so badly that they have to undergo bone marrow transplants

But it's not as simple as saying "you need transplant therefore your genetics makes it that bad" or indeed that having transplant means it is somehow worse than someone who didn't receive it. The reason being that stem cell transplantation for sickle cell is an evolving therapy and hasn't always been a viable option. In addition to it being indicated in patients who are unresponsive to conventional therapy or have sickle-related organ damage, sickle diagnosis under the age of 17 is also an indication. The reason being that it is a curative treatment, and curing it early is better than having to undergo treatments for the rest of your life, treatments you may become resistant to. Indeed some studies have found that stem cell transplantation offers better morbidity and mortality statistics than those who undergo conventional therapy.

I'm sorry if I seemed overly harsh or condescending, but I do believe that your attitude is misguided. The fact that she got a transplant doesn't mean her genetics are 'worse' and that her reproducing is bad for humanity. I fundamentally do not see the difference with her getting this treatment and having children, or a woman with breast cancer positive for BRCA having children. Yes, she may pass that on to her child, but that doesn't somehow begin to harm humanity or have such drastic impacts that we need to be scared or look down on her for making such a choice. Just because we can do something, should we? I am not sure what is amazing about this. Methods to freeze tissue to maintain viability have been around for ever. The surgical procedures are not that complicated. The rest of the woman's anatomy was intact (assumed). The hormones and factors produced by the ovarian tissue would be sufficient to control ovum development and ovulation in an otherwise healthy female.

The amazing thing to me is the amount of effort put into enabling one person to have a child while there are millions of children already who need a stable home. 

The other amazing thing is that hundreds of basic scientists worked over many years to generate all the information required for this procedure. However, the final person, the physician, who likely has no idea why specific reagents or methods are required, gets the credit for a "breakthrough". Its silly.  [deleted] The ovary wasn't fully mature. Therefore, it was unknown if it could produce the hormones and eggs necessary for reproduction. Now we know it's possible.

Prior success with this procedure had only occurred with fully mature ovaries. Like someone above you pointed out, this could have applications in space travel. Apparently a big issue with colonization is that they don't yet know how a body could handle that much radiation exposure and still reproduce. So, maybe this will help them develop a way to shield reproductive tissues for long term space travel so they could later implant them.  This event might have much bigger implications than just allowing ailing young women to bear children later on in life. I know many women who would love to not menstruate, and I know many employers who hesitate to invest in a woman due to the fact that she could require maternity leave.

If this procedure becomes standardized and women obtain the ability to put reproduction on hold for a decade or two (which men naturally have), then we could see a serious shifting in the glass ceiling effect. &gt;Just because we can do something, should we?

You make it sound like she's the world's most selfish person just because she wanted to reproduce. 

In fact, just as there are millions of children who need stable homes, there are millions of people who want children. It's not just as simple as handing out needy kids to hopeful parents. The need to reproduce, to actually produce flesh from your flesh, blood from your blood, is a very, very strong biological drive. Ignoring that, or failing it, can lead to various emotional upsets, not the least of which could be major depression or a sense of being unfulfilled. 

If we are going to be the best society that we can be, then we need to recognize that the right to reproduce should be available to everyone, and if they can find a way to make it happen, then we shouldn't stop them.    I agree 100%. And something else that I haven't seen mentioned......she gets to keep the sickle-cell trait going and going. Just because you can reproduce, doesn't mean you should. There are ways she could have avoided that. She didn't, because it says she conceived naturally, but had she wanted to she could have opted for IVF and done PGS testing. The amount of limited resources spent to allow her to get pregnant are staggering.  When there are millions of orphans in world looking for homes. From what I saw from the paper, the hormones she had left after her treatment were not sufficient to control ovum development.  They had to artificially induce menstruation in her at 16. There have previously [been cases](http://www.ncbi.nlm.nih.gov/pubmed/23084082) where regrafting frozen ovary tissue has been enough to allow someone who was unable to undergo puberty naturally to undergo it. This was the first time that someone who had immature ovary tissue regrafted had children. &gt;This was the first time that someone who had it regrafted had children.

That's actually not accurate, this procedure has been performed successfully in over 40 other women who went on to have children. This is the first time the procedure has been performed on a patient who hadn't completed puberty when the ovary was removed (the patient was 13 when the ovary was excised).

Don't you love it when title editor at a pop-science magazine completely mis-represents the case report? Sorry, I did make a mistake there.  I should have included that part.  I had actually read the case study first, but I just left of the juvenile part in that sentence. I'll add it in. No need to apologize! Your post had some really useful information. [deleted] [deleted] [deleted] [deleted] [deleted] [deleted] I'm curious to learn about the viability of decades frozen ova, countered with the fact that younger ova are healthier than older ova.  Wait....collected? I didnt see anything in the article about the baby being taken. Am i misinterpreting this? Weird. I distinctly remember reading about something similar a long time ago.

EDIT: Found it. [Here is the article, apparently from 2004.](http://www.newscientist.com/article/dn6444#.VXhuyGhBvMI) This is the first time the ovary tissue was harvested when it was immature, and then implanted later.  The previous cases have all been with mature ovarian tissue.  In this case, the immature tissue was able to mature following implantation. Neat science though it seems like a big work around just to have a child of "your own". Anybody know if there are any significant differences between using an ovary from a young girl vs one of more adult age? Being we don't need more people, lets ring this up as the first and agreed upon last. [deleted] I've read a lot of articles about Toxoplasma gondii. I think it's fascinating. But I've never read an article that discussed how to get rid of it if it's in your body. The parasite has an active and cyst form. It stays in the body indefinitely in the cyst form while it's under immunological stress. It becomes active in immunocompromised individuals (AIDS, chemotherapy).  The cyst form can not be removed?
 No, the cyst protects it from drugs that work on the active form. Research into treatments are looking at the cyst wall as a target for drugs.  Yes, atovaquone penetrates the cyst well enough to kill it, although it may or may not clear all of the cysts out.

Last time I checked, it's pretty expensive though, a few thousand dollars.

It's sometimes used for AIDS patients because they can't mount an immune response.

Unless you have AIDS shouldn't worry. Makes you wonder though, if they decided to make testing and treating for toxoplasma routine and wipe it out like polio or smallpox would we see subtle, but statistically significant sociological effects similar to when we pulled tetraethyl lead from gasoline. They do routinely test for it in pregnant women.

What we need is a feline vaccine for it. *Toxoplasma gondii* is a protozoan parasite. Is it even possible to vaccinate against it? What were the effects of pulling lead off of gasoline?  The current theories are either the removal of lead from gas and/or the legalization of abortion lead to the collapse of the massive crime wave that was rampaging across the US and continuing to grow. 20 years or so after both these actions, the wave shrunk.  Specifically violent crime if I remember correctly.  Which lead poisoning is prone to cause. The most exposed are those born in the 60's.  The peak age for crime is 16-25, but that's only the stuff that gets noticed by police.  What happens when that age cohort gets old enough to hold political office?  Do we get policies of continuous war, paranioa, and violence against citizens? Average IQ rose much faster than predicted (they constantly adjust it so 'average' stays at 100) and violent crime dropped dramatically. Apparently breathing lead makes people stupid and angry. Who knew.. Scientists figured that IQs rose several points on average in the US after lead was removed from gasoline.  Many studies in the U.K. link lead removal to the falling crime rates.... I am inclined to agree. Its intermediate host is the cat, and it is transmitted by contact with their contaminated feces. That would be quite a lot of wild cats to vaccinate! [deleted] [deleted] 